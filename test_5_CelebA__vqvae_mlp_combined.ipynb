{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9627b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 1. Install and Import Dependencies\n",
    "# \n",
    "# If dependencies are not installed, run first:\n",
    "# ```\n",
    "# pip install diffusers transformers accelerate datasets\n",
    "# ```\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def get_least_used_gpu():\n",
    "    \"\"\"Dynamically select the GPU with lowest utilization\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '--query-gpu=index,utilization.gpu,memory.used', '--format=csv,noheader,nounits'],\n",
    "            capture_output=True, text=True, check=True\n",
    "        )\n",
    "        \n",
    "        gpus = []\n",
    "        for line in result.stdout.strip().split('\\n'):\n",
    "            parts = line.split(', ')\n",
    "            if len(parts) >= 3:\n",
    "                idx = int(parts[0])\n",
    "                util = int(parts[1])\n",
    "                mem = int(parts[2])\n",
    "                gpus.append((idx, util, mem))\n",
    "                print(f\"  GPU {idx}: {util}% utilization, {mem} MiB memory used\")\n",
    "        \n",
    "        if gpus:\n",
    "            # Sort by utilization first, then by memory used\n",
    "            gpus.sort(key=lambda x: (x[1], x[2]))\n",
    "            best_gpu = gpus[0][0]\n",
    "            print(f\"\\n‚úì Selected GPU {best_gpu} (lowest utilization)\")\n",
    "            return str(best_gpu)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not query GPUs: {e}\")\n",
    "    \n",
    "    return \"0\"  # Default to GPU 0\n",
    "\n",
    "print(\"=== Selecting GPU with lowest utilization ===\")\n",
    "selected_gpu = get_least_used_gpu()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = selected_gpu\n",
    "print(f\"Set CUDA_VISIBLE_DEVICES = {selected_gpu}\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project directory to sys.path to find custom modules\n",
    "project_dir = \"/workspace/kswgd\"\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "    print(f\"Added {project_dir} to sys.path\")\n",
    "\n",
    "# Check GPU\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    print(f\"    Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# ## 2. Download and Load CelebA-HQ Dataset\n",
    "# \n",
    "# We load the CelebA-HQ dataset first. This dataset serves as the **Target Distribution** for KSWGD and the **Ground Truth** for LDM's evaluation (FID). By loading it here, we ensure both methods are compared against the same data source.\n",
    "\n",
    "# Download CelebA-HQ to data/ folder\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from datasets import load_dataset\n",
    "except ImportError:\n",
    "    print(\"Installing datasets library...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"datasets\", \"-q\"])\n",
    "    from datasets import load_dataset\n",
    "\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set data directory (alongside MNIST, CIFAR-10)\n",
    "DATA_DIR = \"/workspace/kswgd/data\"\n",
    "CELEBAHQ_CACHE = os.path.join(DATA_DIR, \"CelebA-HQ\")\n",
    "os.makedirs(CELEBAHQ_CACHE, exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"CelebA-HQ cache directory: {CELEBAHQ_CACHE}\")\n",
    "\n",
    "# Download CelebA-HQ dataset to data/ folder\n",
    "# Try multiple possible dataset sources\n",
    "print(\"\\nDownloading CelebA-HQ dataset...\")\n",
    "print(\"Data will be saved to data/CelebA-HQ/ folder\")\n",
    "\n",
    "dataset_sources = [\n",
    "    \"mattymchen/celeba-hq\",           # Alternative source 1\n",
    "    \"datasets-community/CelebA-HQ\",   # Alternative source 2\n",
    "    \"xinrongzhang2022/celeba-hq\",     # Alternative source 3\n",
    "]\n",
    "\n",
    "celebahq_dataset = None\n",
    "for source in dataset_sources:\n",
    "    try:\n",
    "        print(f\"Trying to load: {source}\")\n",
    "        celebahq_dataset = load_dataset(\n",
    "            source, \n",
    "            split=\"train\",\n",
    "            cache_dir=CELEBAHQ_CACHE,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(f\"‚úì Successfully loaded: {source}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Failed to load: {e}\")\n",
    "        continue\n",
    "\n",
    "if celebahq_dataset is None:\n",
    "    raise RuntimeError(\"Unable to load CelebA-HQ dataset, please check network connection or download manually\")\n",
    "\n",
    "print(f\"\\n‚úì CelebA-HQ loaded!\")\n",
    "print(f\"  Save location: {CELEBAHQ_CACHE}\")\n",
    "print(f\"  Total available samples: {len(celebahq_dataset)}\")\n",
    "\n",
    "# ## 3. Load Pretrained Unconditional Latent Diffusion Model\n",
    "# \n",
    "# Using `CompVis/ldm-celebahq-256`, an **unconditional** Latent Diffusion Model (LDM) trained on CelebA-HQ.\n",
    "# \n",
    "# **LDM Architecture:**\n",
    "# - VAE: Image ‚Üî Latent Space\n",
    "# - UNet: Denoising in Latent Space (unconditional)\n",
    "\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "# Load Unconditional Latent Diffusion Model (LDM)\n",
    "# This is a true unconditional model working in latent space\n",
    "print(\"Loading Unconditional LDM (CompVis/ldm-celebahq-256)...\")\n",
    "print(\"This is an unconditional Latent Diffusion Model trained on CelebA-HQ\")\n",
    "\n",
    "ldm_pipe = DiffusionPipeline.from_pretrained(\"CompVis/ldm-celebahq-256\")\n",
    "ldm_pipe = ldm_pipe.to(\"cuda\")  # Use selected GPU\n",
    "\n",
    "# Fix VQ-VAE scaling factor for proper image generation\n",
    "# The default scaling_factor may cause color/brightness issues\n",
    "ldm_pipe.vqvae.config.scaling_factor = 1.0\n",
    "\n",
    "print(\"\\n‚úì Unconditional LDM loaded!\")\n",
    "print(f\"  Model: CompVis/ldm-celebahq-256\")\n",
    "print(f\"  Output size: 256√ó256\")\n",
    "print(f\"  Latent Space: Yes (VAE)\")\n",
    "print(f\"  VAE scaling factor: {ldm_pipe.vqvae.config.scaling_factor}\")\n",
    "print(f\"  Generation: Pure unconditional, no condition input required\")\n",
    "\n",
    "# ## 3.1 Extract LDM's VAE\n",
    "# \n",
    "# Directly use LDM's built-in VAE to maintain consistent latent space. The VAE is pre-trained as part of the LDM, so we demonstrate its reconstruction quality here before proceeding with image generation.\n",
    "\n",
    "# Directly use LDM's built-in VAE\n",
    "# This way KSWGD and LDM use exactly the same latent space\n",
    "\n",
    "print(\"Extracting LDM's VAE...\")\n",
    "\n",
    "# LDM pipeline contains vqvae (VQ-VAE)\n",
    "vae = ldm_pipe.vqvae\n",
    "vae_scaling = 1.0  # LDM's VQ-VAE doesn't need extra scaling\n",
    "\n",
    "print(f\"\\n‚úì VAE extracted!\")\n",
    "print(f\"  Source: LDM's built-in VQ-VAE\")\n",
    "print(f\"  Scaling factor: {vae_scaling}\")\n",
    "print(f\"  Advantage: KSWGD and LDM share the same latent space\")\n",
    "\n",
    "# VAE helper functions\n",
    "def _to_vae_range(x):\n",
    "    \"\"\"[0,1] ‚Üí [-1,1]\"\"\"\n",
    "    return (x * 2.0) - 1.0\n",
    "\n",
    "def _from_vae_range(x):\n",
    "    \"\"\"[-1,1] ‚Üí [0,1]\"\"\"\n",
    "    return torch.clamp((x + 1.0) * 0.5, 0.0, 1.0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use selected GPU\n",
    "\n",
    "# Test VAE reconstruction on real images\n",
    "# üîç VAE Quality Test: Demonstrate LDM's built-in VQ-VAE reconstruction capability\n",
    "# VAE Source: Built-in VQ-VAE from CompVis/ldm-celebahq-256 pretrained model\n",
    "\n",
    "import random\n",
    "\n",
    "# ============== Sampling Settings ==============\n",
    "n_recon = 5\n",
    "USE_RANDOM_SAMPLES = True  # True: random sampling, False: first n_recon samples\n",
    "# RANDOM_SEED = 42  # Set random seed for reproducible results (comment out for pure random)\n",
    "# ==============================================\n",
    "\n",
    "print(f\"=== VAE Reconstruction Quality Test ===\\n\")\n",
    "print(f\"Dataset size: {len(celebahq_dataset)} CelebA-HQ images\")\n",
    "print(f\"Test samples: {n_recon}\")\n",
    "print(f\"Sampling method: {'Random sampling' if USE_RANDOM_SAMPLES else 'First '+str(n_recon)+' samples'}\")\n",
    "print(f\"VAE source: LDM pretrained model built-in VQ-VAE\")\n",
    "print(f\"Test purpose: Validate VAE encode‚Üídecode reconstruction quality\\n\")\n",
    "\n",
    "# Select test samples\n",
    "if USE_RANDOM_SAMPLES:\n",
    "    # Check if RANDOM_SEED is defined, if not use pure random\n",
    "    if 'RANDOM_SEED' in locals():\n",
    "        random.seed(RANDOM_SEED)\n",
    "        print(f\"Using random seed: {RANDOM_SEED}\")\n",
    "    else:\n",
    "        print(\"Using pure random (no seed)\")\n",
    "    \n",
    "    test_indices = random.sample(range(len(celebahq_dataset)), n_recon)\n",
    "    print(f\"Randomly selected sample indices: {test_indices}\")\n",
    "else:\n",
    "    test_indices = list(range(n_recon))\n",
    "    print(f\"Fixed sample indices: {test_indices}\")\n",
    "\n",
    "fig, axes = plt.subplots(2, n_recon, figsize=(15, 6))\n",
    "\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    for plot_idx, sample_idx in enumerate(test_indices):\n",
    "        # Original\n",
    "        img = celebahq_dataset[sample_idx][\"image\"]\n",
    "        img_tensor = T.Compose([T.Resize((256, 256)), T.ToTensor()])(img).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Encode & Decode\n",
    "        latent = vae.encode(_to_vae_range(img_tensor))\n",
    "        \n",
    "        if hasattr(latent, 'latents'):\n",
    "            latent_code = latent.latents\n",
    "        elif hasattr(latent, 'latent_dist'):\n",
    "            latent_code = latent.latent_dist.mode()\n",
    "        else:\n",
    "            latent_code = latent[0] if isinstance(latent, tuple) else latent\n",
    "            \n",
    "        recon = vae.decode(latent_code).sample\n",
    "        recon_img = _from_vae_range(recon).cpu().squeeze(0).permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Plot\n",
    "        axes[0, plot_idx].imshow(np.array(img.resize((256, 256))))\n",
    "        axes[0, plot_idx].set_title(f\"Original #{sample_idx}\", fontsize=10)\n",
    "        axes[0, plot_idx].axis('off')\n",
    "        \n",
    "        axes[1, plot_idx].imshow(recon_img)\n",
    "        axes[1, plot_idx].set_title(f\"VAE Reconstructed #{sample_idx}\", fontsize=10)\n",
    "        axes[1, plot_idx].axis('off')\n",
    "\n",
    "plt.suptitle(\"LDM Built-in VQ-VAE Reconstruction Quality Test (Encode‚ÜíDecode)\", fontsize=22)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"VAE reconstruction test completed.\")\n",
    "\n",
    "# ## 4. Super-Resolution Setup (Real-ESRGAN + GFPGAN)\n",
    "# \n",
    "# Set up the upscaling pipeline. Actual LDM generation will happen **after KSWGD** to optimize memory usage.\n",
    "\n",
    "# ============== Real-ESRGAN + GFPGAN Super-Resolution Setup ==============\n",
    "import os\n",
    "import cv2\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "from realesrgan import RealESRGANer\n",
    "from gfpgan import GFPGANer\n",
    "\n",
    "# Create figures directory if not exists\n",
    "os.makedirs('/workspace/kswgd/figures', exist_ok=True)\n",
    "\n",
    "print(\"Using single GPU mode for upscaling\")\n",
    "\n",
    "# Initialize Real-ESRGAN and GFPGAN\n",
    "model_path = '/workspace/kswgd/weights/RealESRGAN_x4plus.pth'\n",
    "gfpgan_path = '/workspace/kswgd/weights/GFPGANv1.3.pth'\n",
    "\n",
    "print(\"Initializing Real-ESRGAN + GFPGAN...\")\n",
    "model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n",
    "upsampler = RealESRGANer(\n",
    "    scale=4,\n",
    "    model_path=model_path,\n",
    "    model=model,\n",
    "    tile=200,\n",
    "    tile_pad=10,\n",
    "    pre_pad=0,\n",
    "    half=False,\n",
    "    gpu_id=0  # Uses the GPU exposed by CUDA_VISIBLE_DEVICES\n",
    ")\n",
    "face_enhancer = GFPGANer(\n",
    "    model_path=gfpgan_path,\n",
    "    upscale=4,\n",
    "    arch='clean',\n",
    "    channel_multiplier=2,\n",
    "    bg_upsampler=upsampler\n",
    ")\n",
    "print(\"  ‚úì Real-ESRGAN + GFPGAN initialized\")\n",
    "\n",
    "def preprocess_image(img_bgr, use_gaussian=True, use_bilateral=True, use_color_norm=True):\n",
    "    \"\"\"Apply preprocessing before GFPGAN\"\"\"\n",
    "    processed = img_bgr.copy().astype(np.float32)\n",
    "    if use_gaussian:\n",
    "        processed = cv2.GaussianBlur(processed, (3, 3), sigmaX=0.5)\n",
    "    if use_bilateral:\n",
    "        processed = cv2.bilateralFilter(processed.astype(np.uint8), d=5, sigmaColor=30, sigmaSpace=30).astype(np.float32)\n",
    "    if use_color_norm:\n",
    "        mean_b, mean_g, mean_r = np.mean(processed[:, :, 0]), np.mean(processed[:, :, 1]), np.mean(processed[:, :, 2])\n",
    "        target_mean, alpha = 127.5, 0.15\n",
    "        processed[:, :, 0] += alpha * (target_mean - mean_b)\n",
    "        processed[:, :, 1] += alpha * (target_mean - mean_g)\n",
    "        processed[:, :, 2] += alpha * (target_mean - mean_r)\n",
    "        processed = np.clip(processed, 0, 255)\n",
    "    return processed.astype(np.uint8)\n",
    "\n",
    "def process_single_image(img, use_face_enhance=True, use_preprocess=False):\n",
    "    \"\"\"Process a single image\"\"\"\n",
    "    if isinstance(img, Image.Image):\n",
    "        img_np = np.array(img)\n",
    "    else:\n",
    "        if img.ndim == 3 and img.shape[0] == 3:\n",
    "            img_np = np.transpose(img, (1, 2, 0))\n",
    "        else:\n",
    "            img_np = img\n",
    "        if img_np.max() <= 1.0:\n",
    "            img_np = (img_np * 255).astype(np.uint8)\n",
    "\n",
    "    img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
    "    if use_preprocess:\n",
    "        img_bgr = preprocess_image(img_bgr)\n",
    "\n",
    "    if use_face_enhance:\n",
    "        _, _, output_bgr = face_enhancer.enhance(img_bgr, has_aligned=False, only_center_face=False, paste_back=True)\n",
    "    else:\n",
    "        output_bgr, _ = upsampler.enhance(img_bgr, outscale=4)\n",
    "\n",
    "    return cv2.cvtColor(output_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def upscale_images(images_list, use_face_enhance=True, use_preprocess=False, desc=\"Upscaling\"):\n",
    "    \"\"\"Upscale images using GFPGAN or Real-ESRGAN\"\"\"\n",
    "    return [process_single_image(img, use_face_enhance, use_preprocess) for img in tqdm(images_list, desc=desc)]\n",
    "\n",
    "print(\"‚úì Upscaling functions ready\")\n",
    "\n",
    "# Section 6.5: Memory Cleanup\n",
    "# To avoid OOM during KSWGD, we release the LDM UNet and other components that are no longer needed.\n",
    "import gc\n",
    "\n",
    "print(f\"GPU Memory before cleanup: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "if 'ldm_pipe' in globals():\n",
    "    # We only need the VAE for KSWGD, so we can release the UNet (the largest part)\n",
    "    # This can free up 2-4 GB of VRAM.\n",
    "    print(\"Releasing LDM UNet and other components...\")\n",
    "    if hasattr(ldm_pipe, 'unet'):\n",
    "        ldm_pipe.unet = None\n",
    "    if hasattr(ldm_pipe, 'text_encoder'):\n",
    "        ldm_pipe.text_encoder = None\n",
    "    if hasattr(ldm_pipe, 'scheduler'):\n",
    "        ldm_pipe.scheduler = None\n",
    "    \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"GPU Memory after cleanup: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "# ## 7. KSWGD Unconditional Generation (Core Experiment)\n",
    "# \n",
    "# Using KSWGD (Koopman Spectral Wasserstein Gradient Descent) to perform **unconditional** particle transport generation in latent space.\n",
    "# \n",
    "# **Unconditional Pipeline:**\n",
    "# ```\n",
    "# Training Data (CelebA-HQ) ‚Üí VAE Encoder ‚Üí Latent Z_tar (Target Distribution Samples)\n",
    "#                                               ‚Üì\n",
    "# Random Noise N(0,1) ‚Üí KSWGD Transport ‚Üí Z_gen (Unconditional Generation)\n",
    "#                                               ‚Üì\n",
    "#                                    VAE Decoder ‚Üí Generated Image\n",
    "# ```\n",
    "# \n",
    "# **Comparison of Two Unconditional Methods (Fair Comparison, Same Dataset):**\n",
    "# | | LDM (Section 2-4) | KSWGD (Section 7) |\n",
    "# |---|---|---|\n",
    "# | Dataset | CelebA-HQ | CelebA-HQ |\n",
    "# | Generation Process | Random Noise ‚Üí Denoising | Random Noise ‚Üí KSWGD |\n",
    "# | Core Component | UNet | Kernel & Koopman Matrix + Eigendecomposition |\n",
    "# | Iteration Steps | 200 | Adjustable (e.g., 300) |\n",
    "\n",
    "# Import KSWGD required libraries and custom kernel functions\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from tqdm.auto import trange\n",
    "\n",
    "# Import your kernel functions\n",
    "from grad_ker1 import grad_ker1\n",
    "from K_tar_eval import K_tar_eval\n",
    "\n",
    "# Try to import GPU version\n",
    "try:\n",
    "    import cupy as cp\n",
    "    from grad_ker1_gpu import grad_ker1 as grad_ker1_gpu\n",
    "    from K_tar_eval_gpu import K_tar_eval as K_tar_eval_gpu\n",
    "    GPU_KSWGD = True\n",
    "    print(\"‚úì GPU KSWGD backend available (CuPy)\")\n",
    "except Exception as e:\n",
    "    cp = None\n",
    "    grad_ker1_gpu = None\n",
    "    K_tar_eval_gpu = None\n",
    "    GPU_KSWGD = False\n",
    "    print(f\"‚úó GPU KSWGD backend not available: {e}\")\n",
    "    print(\"  Using CPU backend instead\")\n",
    "\n",
    "# Encode CelebA-HQ images and compress with MLP AutoEncoder\n",
    "from torchvision import transforms as T\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Image preprocessing\n",
    "transform_celebahq = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "# ============== MLP Latent Compression Settings ==============\n",
    "# Use MLP AutoEncoder to compress VAE's 1024-dim latent to lower dimension\n",
    "REDUCED_DIM = 8  # Target dimension for MLP compression (optimized for background quality)\n",
    "# =============================================================\n",
    "\n",
    "# ============== Caching Settings ==============\n",
    "LOAD_FROM_CACHE = True\n",
    "CACHE_DIR = \"/workspace/kswgd/cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "# ==============================================\n",
    "\n",
    "# ============== MLP Latent AutoEncoder ==============\n",
    "class LatentAutoEncoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Pyramidal MLP to compress VAE latent space (1024-dim) to lower dimension.\n",
    "    Architecture is defined by the encoder/decoder Sequential blocks below.\n",
    "    Use get_architecture_str() to see the current structure.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=1024, latent_dim=64):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder: Pyramidal structure (modify layers here to change architecture)\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, 1024),\n",
    "            torch.nn.LayerNorm(1024),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(1024, 512),\n",
    "            torch.nn.LayerNorm(512),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(512, 256),\n",
    "            torch.nn.LayerNorm(256),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(256, latent_dim),\n",
    "        )\n",
    "        \n",
    "        # Decoder: Symmetric structure (modify layers here to change architecture)\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(latent_dim, 256),\n",
    "            torch.nn.LayerNorm(256),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(256, 512),\n",
    "            torch.nn.LayerNorm(512),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(512, 1024),\n",
    "            torch.nn.LayerNorm(1024),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(1024, input_dim),\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        return self.decode(z), z\n",
    "    \n",
    "    def get_architecture_str(self):\n",
    "        \"\"\"Return a string describing the encoder architecture\"\"\"\n",
    "        dims = [self.input_dim]\n",
    "        for module in self.encoder:\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                dims.append(module.out_features)\n",
    "        return ' -> '.join(map(str, dims))\n",
    "    \n",
    "    def get_num_layers(self):\n",
    "        \"\"\"Return the number of Linear layers in the encoder\"\"\"\n",
    "        return sum(1 for m in self.encoder if isinstance(m, torch.nn.Linear))\n",
    "\n",
    "\n",
    "def train_latent_autoencoder(Z_flat, latent_dim=64, epochs=100, batch_size=512, lr=1e-3, use_perceptual_loss=True):\n",
    "    \"\"\"Train the MLP AutoEncoder on VAE latent codes with perceptual loss\"\"\"\n",
    "    model = LatentAutoEncoder(input_dim=Z_flat.shape[1], latent_dim=latent_dim).to(device)\n",
    "    \n",
    "    print(f\"\\n=== Training Latent AutoEncoder ===\")\n",
    "    print(f\"  Architecture: {model.get_architecture_str()}\")\n",
    "    print(f\"  Layers: {model.get_num_layers()}\")\n",
    "    print(f\"  Loss: {'LPIPS Perceptual Loss + MSE' if use_perceptual_loss else 'MSE only'}\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    # LPIPS perceptual loss (requires reshaping latent vectors to spatial format)\n",
    "    lpips_loss_fn = None\n",
    "    if use_perceptual_loss:\n",
    "        try:\n",
    "            import lpips\n",
    "            lpips_loss_fn = lpips.LPIPS(net='vgg').to(device)\n",
    "            lpips_loss_fn.eval()\n",
    "            for param in lpips_loss_fn.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"  ‚úì LPIPS loss loaded (VGG-based)\")\n",
    "        except ImportError:\n",
    "            print(\"  ‚ö† lpips not installed, falling back to MSE. Install with: pip install lpips\")\n",
    "            use_perceptual_loss = False\n",
    "    \n",
    "    # Convert to tensor\n",
    "    Z_tensor = torch.from_numpy(Z_flat).float().to(device)\n",
    "    dataset = torch.utils.data.TensorDataset(Z_tensor)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        total_mse = 0.0\n",
    "        total_lpips = 0.0\n",
    "        \n",
    "        for (batch,) in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            recon, z = model(batch)\n",
    "            \n",
    "            # MSE loss (always computed)\n",
    "            mse_loss = torch.nn.functional.mse_loss(recon, batch)\n",
    "            \n",
    "            # Perceptual loss (LPIPS on reshaped features)\n",
    "            if use_perceptual_loss and lpips_loss_fn is not None:\n",
    "                # Reshape latent vectors to spatial format for LPIPS: (B, 1024) -> (B, 4, 16, 16)\n",
    "                batch_spatial = batch.view(-1, 4, 16, 16)\n",
    "                recon_spatial = recon.view(-1, 4, 16, 16)\n",
    "                \n",
    "                # LPIPS expects 3-channel input, so we replicate channels: (B, 4, 16, 16) -> (B, 3, 16, 16)\n",
    "                # Take first 3 channels\n",
    "                batch_3ch = batch_spatial[:, :3, :, :]\n",
    "                recon_3ch = recon_spatial[:, :3, :, :]\n",
    "                \n",
    "                # Normalize to [-1, 1] range (LPIPS expects this)\n",
    "                batch_norm = 2.0 * (batch_3ch - batch_3ch.min()) / (batch_3ch.max() - batch_3ch.min() + 1e-8) - 1.0\n",
    "                recon_norm = 2.0 * (recon_3ch - recon_3ch.min()) / (recon_3ch.max() - recon_3ch.min() + 1e-8) - 1.0\n",
    "                recon_spatial = recon.view(-1, 4, 16, 16)\n",
    "                lpips_loss = lpips_loss_fn(batch_norm, recon_norm).mean()\n",
    "                \n",
    "                # Combined loss: MSE + 0.7 * LPIPS (higher weight on perceptual quality)\n",
    "                loss = mse_loss + 0.7 * lpips_loss\n",
    "                total_lpips += lpips_loss.item() * batch.size(0)\n",
    "            else:\n",
    "                loss = mse_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * batch.size(0)\n",
    "            total_mse += mse_loss.item() * batch.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(Z_tensor)\n",
    "        avg_mse = total_mse / len(Z_tensor)\n",
    "        avg_lpips = total_lpips / len(Z_tensor) if use_perceptual_loss else 0.0\n",
    "        scheduler.step()\n",
    "        \n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0 or epoch == 0:\n",
    "            if use_perceptual_loss:\n",
    "                print(f\"  Epoch {epoch+1:3d}/{epochs}: Total={avg_loss:.6f}, MSE={avg_mse:.6f}, LPIPS={avg_lpips:.6f}\")\n",
    "            else:\n",
    "                print(f\"  Epoch {epoch+1:3d}/{epochs}: Loss = {avg_loss:.6f}\")\n",
    "        scheduler.step()\n",
    "    print(f\"‚úì Training complete! Best loss: {best_loss:.6f}\")\n",
    "    \n",
    "    # Encode all data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        Z_reduced = model.encode(Z_tensor).cpu().numpy()\n",
    "    \n",
    "    return model, Z_reduced\n",
    "# ====================================================\n",
    "    \n",
    "max_samples = 28000  # Maximum number of samples to encode\n",
    "\n",
    "# Generate cache filename\n",
    "cache_path = os.path.join(CACHE_DIR, f\"vae_encoding_n{max_samples}_mlp{REDUCED_DIM}.pkl\")\n",
    "\n",
    "# Try to load from cache\n",
    "latent_ae = None\n",
    "cache_valid = False\n",
    "\n",
    "if LOAD_FROM_CACHE and os.path.exists(cache_path):\n",
    "    print(f\"Loading VAE encodings from cache: {cache_path}\")\n",
    "    with open(cache_path, 'rb') as f:\n",
    "        cache_data = pickle.load(f)\n",
    "    Z_all = cache_data['Z_all']\n",
    "    latent_dim = cache_data['latent_dim']\n",
    "    full_latent_shape = cache_data['full_latent_shape']\n",
    "    latent_ae = cache_data.get('latent_ae')\n",
    "    \n",
    "    # ====== Cache Validation: Check MLP Structure ======\n",
    "    # Create a reference model to get expected architecture\n",
    "    _ref_model = LatentAutoEncoder(input_dim=1024, latent_dim=REDUCED_DIM)\n",
    "    EXPECTED_ARCHITECTURE = _ref_model.get_architecture_str()\n",
    "    EXPECTED_NUM_LAYERS = _ref_model.get_num_layers()\n",
    "    del _ref_model\n",
    "    \n",
    "    if latent_ae is not None:\n",
    "        # Get cached model architecture\n",
    "        cached_architecture = latent_ae.get_architecture_str()\n",
    "        cached_num_layers = latent_ae.get_num_layers()\n",
    "        \n",
    "        if cached_architecture != EXPECTED_ARCHITECTURE:\n",
    "            print(f\"‚ö†Ô∏è  Cache structure mismatch detected!\")\n",
    "            print(f\"   Cached:   {cached_architecture} ({cached_num_layers} layers)\")\n",
    "            print(f\"   Expected: {EXPECTED_ARCHITECTURE} ({EXPECTED_NUM_LAYERS} layers)\")\n",
    "            print(f\"   Retraining MLP AutoEncoder with new structure...\")\n",
    "            latent_ae = None  # Invalidate cache, trigger retraining\n",
    "            cache_valid = False\n",
    "        else:\n",
    "            print(f\"‚úì Cache structure validated!\")\n",
    "            print(f\"   Architecture: {cached_architecture}\")\n",
    "            cache_valid = True\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No latent_ae found in cache, will retrain...\")\n",
    "        cache_valid = False\n",
    "    \n",
    "    if cache_valid:\n",
    "        print(f\"‚úì Loaded from cache!\")\n",
    "        print(f\"  Z_all shape: {Z_all.shape}\")\n",
    "        print(f\"  Latent dim: {latent_dim}\")\n",
    "\n",
    "if not LOAD_FROM_CACHE or not os.path.exists(cache_path) or not cache_valid:\n",
    "    print(f\"Computing VAE encodings (cache not found, disabled, or invalid)...\")\n",
    "    \n",
    "    all_latents = []\n",
    "    vae.eval()\n",
    "    print(f\"Encoding {max_samples} images to latent space...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(min(max_samples, len(celebahq_dataset))), desc=\"Encoding\"):\n",
    "            img = celebahq_dataset[i][\"image\"]\n",
    "            img_tensor = transform_celebahq(img).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Encode to latent\n",
    "            latent = vae.encode(_to_vae_range(img_tensor))\n",
    "            latent_code = latent.latents if hasattr(latent, 'latents') else (latent.latent_dist.mode() if hasattr(latent, 'latent_dist') else latent[0])\n",
    "            latent_code = latent_code * vae_scaling  # (1, 4, 16, 16)\n",
    "            \n",
    "            all_latents.append(latent_code.view(1, -1).cpu().numpy())\n",
    "\n",
    "    Z_flat = np.concatenate(all_latents, axis=0)\n",
    "    print(f\"Latent vectors collected: {Z_flat.shape}\")\n",
    "    \n",
    "    # ============== Train MLP AutoEncoder ==============\n",
    "    import time as _time\n",
    "    print(f\"\\n Training MLP AutoEncoder (1024 -> {REDUCED_DIM})...\")\n",
    "    _mlp_start = _time.time()\n",
    "    \n",
    "    # Train the autoencoder\n",
    "    latent_ae, Z_all = train_latent_autoencoder(\n",
    "        Z_flat, \n",
    "        latent_dim=REDUCED_DIM, \n",
    "        epochs=150,  # Number of training epochs\n",
    "        batch_size=512,\n",
    "        lr=1e-3\n",
    "    )\n",
    "    latent_dim = Z_all.shape[1]\n",
    "    \n",
    "    _mlp_elapsed = _time.time() - _mlp_start\n",
    "    print(f\"\\n‚úì MLP AutoEncoder complete! Time: {_mlp_elapsed:.2f}s\")\n",
    "    print(f\"  Original VAE latent: {Z_flat.shape[1]} dim (4√ó16√ó16)\")\n",
    "    print(f\"  Compressed latent:   {Z_all.shape[1]} dim\")\n",
    "    \n",
    "    # Test reconstruction quality\n",
    "    with torch.no_grad():\n",
    "        Z_test = torch.from_numpy(Z_flat[:100]).float().to(device)\n",
    "        Z_recon, _ = latent_ae(Z_test)\n",
    "        recon_loss = torch.nn.functional.mse_loss(Z_recon, Z_test).item()\n",
    "        print(f\"  Reconstruction MSE (test): {recon_loss:.6f}\")\n",
    "\n",
    "    # Record shapes for decoding\n",
    "    with torch.no_grad():\n",
    "        dummy = torch.zeros(1, 3, 256, 256, device=device)\n",
    "        dummy_latent = vae.encode(_to_vae_range(dummy))\n",
    "        dummy_code = dummy_latent.latents if hasattr(dummy_latent, 'latents') else (dummy_latent.latent_dist.mode() if hasattr(dummy_latent, 'latent_dist') else dummy_latent[0])\n",
    "        full_latent_shape = dummy_code.shape[1:]  # (4, 16, 16)\n",
    "\n",
    "    # Save to cache\n",
    "    print(f\"\\nSaving to cache: {cache_path}\")\n",
    "    cache_data = {\n",
    "        'Z_all': Z_all,\n",
    "        'latent_dim': latent_dim,\n",
    "        'latent_ae': latent_ae,\n",
    "        'full_latent_shape': full_latent_shape,\n",
    "        'REDUCED_DIM': REDUCED_DIM,\n",
    "        'max_samples': max_samples\n",
    "    }\n",
    "    with open(cache_path, 'wb') as f:\n",
    "        pickle.dump(cache_data, f)\n",
    "    print(f\"‚úì Cache saved!\")\n",
    "\n",
    "print(f\"Full Latent Shape: {full_latent_shape}\")\n",
    "\n",
    "# ### 7.1 Latent Space Compression with MLP AutoEncoder\n",
    "# \n",
    "# We use a **learnable MLP AutoEncoder** to compress the VQ-VAE latent space from 1024 (4√ó16√ó16) to `REDUCED_DIM` dimensions.\n",
    "# \n",
    "# **Architecture:**\n",
    "# - Encoder: Pyramidal structure with gradual compression\n",
    "# - Decoder: Symmetric structure for reconstruction\n",
    "# - See the `LatentAutoEncoder` class definition for exact layer dimensions\n",
    "\n",
    "# Standardize latent codes and build KSWGD kernel operator\n",
    "# Standardization\n",
    "Z_mean = np.mean(Z_all, axis=0, keepdims=True)\n",
    "Z_std = np.std(Z_all, axis=0, keepdims=True) + 1e-8\n",
    "Z_std = Z_std.astype(np.float64)\n",
    "Z_mean = Z_mean.astype(np.float64)\n",
    "X_tar = ((Z_all - Z_mean) / Z_std).astype(np.float64)\n",
    "\n",
    "print(f\"After standardization: mean={X_tar.mean():.4f}, std={X_tar.std():.4f}\")\n",
    "\n",
    "# Compute squared sum of target samples (for kernel function)\n",
    "sq_tar = np.sum(X_tar ** 2, axis=1)\n",
    "\n",
    "# Compute pairwise distances and bandwidth epsilon\n",
    "dists = pairwise_distances(X_tar, metric=\"euclidean\")\n",
    "eps_kswgd = np.median(dists**2) / (2.0 * np.log(X_tar.shape[0] + 1))\n",
    "eps_kswgd = float(max(eps_kswgd, 1e-6))\n",
    "\n",
    "print(f\"KSWGD epsilon: {eps_kswgd:.6f}\")\n",
    "print(f\"Distance stats: min={dists[dists>0].min():.4f}, median={np.median(dists):.4f}, max={dists.max():.4f}\")\n",
    "\n",
    "# Build data kernel matrix\n",
    "data_kernel = np.exp(-dists**2 / (2.0 * eps_kswgd))\n",
    "\n",
    "# Normalization\n",
    "p_x = np.sqrt(np.sum(data_kernel, axis=1))\n",
    "data_kernel_norm = data_kernel / (p_x[:, None] * p_x[None, :] + 1e-12)\n",
    "D_y = np.sum(data_kernel_norm, axis=0)\n",
    "rw_kernel = 0.5 * (data_kernel_norm / (D_y + 1e-12) + data_kernel_norm / (D_y[:, None] + 1e-12))\n",
    "rw_kernel = np.nan_to_num(rw_kernel)\n",
    "\n",
    "print(f\"Kernel matrix built, shape: {rw_kernel.shape}\")\n",
    "\n",
    "# latent_dim is already set in the previous cell (Spatial Mean Pooling)\n",
    "print(f\"KSWGD latent_dim: {latent_dim}\")\n",
    "\n",
    "# Compute eigendecomposition and KSWGD weights\n",
    "import time\n",
    "import threading\n",
    "from scipy.sparse.linalg import eigsh\n",
    "\n",
    "print(\"Computing eigendecomposition...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# ============== Truncated Eigendecomposition Settings ==============\n",
    "# k: number of largest eigenvalues to compute\n",
    "# Using truncated decomposition is much faster for large matrices (e.g., 30000x30000)\n",
    "k_eig = 1000  # Number of eigenvalues to compute\n",
    "# k_eig = rw_kernel.shape[0]  # Uncomment this line to compute ALL eigenvalues (full decomposition)\n",
    "USE_GPU_EIGSH = True  # Use GPU for truncated eigendecomposition (torch.lobpcg)\n",
    "# ===================================================================\n",
    "\n",
    "n_samples = rw_kernel.shape[0]\n",
    "use_truncated = k_eig < n_samples\n",
    "\n",
    "# Estimate time based on matrix size\n",
    "if use_truncated:\n",
    "    est_time = n_samples * k_eig / 1e7  # Rough estimate for truncated\n",
    "else:\n",
    "    est_time = (n_samples ** 2.5) / 1e9  # O(n^3) but optimized\n",
    "\n",
    "# Progress tracking with elapsed time display\n",
    "_eig_done = threading.Event()\n",
    "_eig_result = {}\n",
    "\n",
    "def _show_progress():\n",
    "    \"\"\"Show elapsed time while eigendecomposition is running\"\"\"\n",
    "    import sys\n",
    "    symbols = ['‚†ã', '‚†ô', '‚†π', '‚†∏', '‚†º', '‚†¥', '‚†¶', '‚†ß', '‚†á', '‚†è']\n",
    "    idx = 0\n",
    "    while not _eig_done.is_set():\n",
    "        elapsed = time.time() - start_time\n",
    "        sys.stdout.write(f\"\\r  {symbols[idx]} Eigendecomposition in progress... {elapsed:.1f}s elapsed\")\n",
    "        sys.stdout.flush()\n",
    "        idx = (idx + 1) % len(symbols)\n",
    "        _eig_done.wait(0.2)\n",
    "    sys.stdout.write(\"\\r\" + \" \" * 60 + \"\\r\")  # Clear line\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def _run_eigsh_cpu():\n",
    "    \"\"\"Run eigendecomposition on CPU using scipy\"\"\"\n",
    "    try:\n",
    "        eigenvalues, eigenvectors = eigsh(\n",
    "            rw_kernel, k=k_eig, which='LM', maxiter=5000, tol=1e-6\n",
    "        )\n",
    "        _eig_result['lambda'] = eigenvalues\n",
    "        _eig_result['phi'] = eigenvectors\n",
    "    except Exception as e:\n",
    "        print(f\"\\n  ‚ö† eigsh failed: {e}\")\n",
    "        _eig_result['error'] = str(e)\n",
    "    finally:\n",
    "        _eig_done.set()\n",
    "\n",
    "def _run_lobpcg_gpu():\n",
    "    \"\"\"Run eigendecomposition on GPU using torch.lobpcg\"\"\"\n",
    "    try:\n",
    "        K_tensor = torch.from_numpy(rw_kernel).float().cuda()\n",
    "        \n",
    "        # Initialize random vectors for lobpcg\n",
    "        n = K_tensor.shape[0]\n",
    "        X0 = torch.randn(n, k_eig, device='cuda', dtype=torch.float32)\n",
    "        X0, _ = torch.linalg.qr(X0)  # Orthonormalize\n",
    "        \n",
    "        # Run lobpcg\n",
    "        eigenvalues, eigenvectors = torch.lobpcg(K_tensor, k=k_eig, X=X0, niter=100, tol=1e-6)\n",
    "        \n",
    "        _eig_result['lambda'] = eigenvalues.cpu().numpy()\n",
    "        _eig_result['phi'] = eigenvectors.cpu().numpy()\n",
    "        \n",
    "        # Free GPU memory\n",
    "        del K_tensor, X0, eigenvalues, eigenvectors\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(f\"\\n  ‚ö† GPU lobpcg failed: {e}, falling back to CPU...\")\n",
    "        _eig_result['error'] = str(e)\n",
    "        _eig_done.clear()  # Reset for CPU retry\n",
    "        _run_eigsh_cpu()\n",
    "        return\n",
    "    finally:\n",
    "        _eig_done.set()\n",
    "\n",
    "if use_truncated:\n",
    "    print(f\"  Mode: Truncated (top {k_eig} eigenvalues)\")\n",
    "    \n",
    "    if USE_GPU_EIGSH and torch.cuda.is_available():\n",
    "        print(f\"  Backend: GPU (torch.lobpcg)\")\n",
    "        print(f\"  Estimated time: ~{est_time/5:.0f}-{est_time:.0f} seconds (GPU accelerated)\")\n",
    "        \n",
    "        # Start progress display thread\n",
    "        progress_thread = threading.Thread(target=_show_progress, daemon=True)\n",
    "        progress_thread.start()\n",
    "        \n",
    "        # Run GPU eigendecomposition\n",
    "        _run_lobpcg_gpu()\n",
    "        progress_thread.join(timeout=1)\n",
    "        \n",
    "        # If GPU failed, retry with CPU\n",
    "        if 'error' in _eig_result and 'lambda' not in _eig_result:\n",
    "            _eig_done.clear()\n",
    "            _eig_result.clear()\n",
    "            start_time = time.time()  # Reset timer\n",
    "            progress_thread = threading.Thread(target=_show_progress, daemon=True)\n",
    "            progress_thread.start()\n",
    "            _run_eigsh_cpu()\n",
    "            progress_thread.join(timeout=1)\n",
    "    else:\n",
    "        print(f\"  Backend: CPU (scipy.sparse.linalg.eigsh)\")\n",
    "        print(f\"  Estimated time: ~{est_time:.0f}-{est_time*2:.0f} seconds\")\n",
    "        \n",
    "        # Start progress display thread\n",
    "        progress_thread = threading.Thread(target=_show_progress, daemon=True)\n",
    "        progress_thread.start()\n",
    "        \n",
    "        # Run CPU eigendecomposition\n",
    "        _run_eigsh_cpu()\n",
    "        progress_thread.join(timeout=1)\n",
    "    \n",
    "    # Sort in descending order\n",
    "    sort_idx = np.argsort(_eig_result['lambda'])[::-1]\n",
    "    lambda_ns = _eig_result['lambda'][sort_idx]\n",
    "    phi = _eig_result['phi'][:, sort_idx]\n",
    "else:\n",
    "    print(f\"  Using FULL eigendecomposition ({n_samples} x {n_samples})...\")\n",
    "    print(f\"  Estimated time: ~{est_time:.0f}-{est_time*3:.0f} seconds\")\n",
    "    \n",
    "    # Start progress display thread\n",
    "    progress_thread = threading.Thread(target=_show_progress, daemon=True)\n",
    "    progress_thread.start()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(\"    Using GPU acceleration...\")\n",
    "        K_tensor = torch.from_numpy(rw_kernel).float().cuda()\n",
    "        eigenvalues, eigenvectors = torch.linalg.eigh(K_tensor)\n",
    "        lambda_ns = eigenvalues.cpu().numpy()[::-1]  # Descending order\n",
    "        phi = eigenvectors.cpu().numpy()[:, ::-1]\n",
    "        del K_tensor, eigenvalues, eigenvectors\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        lambda_ns, phi = np.linalg.eigh(rw_kernel)\n",
    "        lambda_ns = lambda_ns[::-1]\n",
    "        phi = phi[:, ::-1]\n",
    "    \n",
    "    _eig_done.set()\n",
    "    progress_thread.join(timeout=1)\n",
    "\n",
    "eig_time = time.time() - start_time\n",
    "print(f\"\\n‚úì Eigendecomposition complete! Time: {eig_time:.2f}s\")\n",
    "print(f\"  Eigenvalue range: [{lambda_ns[-1]:.6f}, {lambda_ns[0]:.6f}]\")\n",
    "print(f\"  Top 5 eigenvalues: {lambda_ns[:5]}\")\n",
    "\n",
    "# Spectral gap (difference between 1st and 2nd eigenvalues)\n",
    "if len(lambda_ns) > 1:\n",
    "    spectral_gap = lambda_ns[0] - lambda_ns[1]\n",
    "    print(f\"  Spectral gap: {spectral_gap:.6f}\")\n",
    "\n",
    "# Compute weights for KSWGD\n",
    "print(\"\\nComputing KSWGD weights...\")\n",
    "# Threshold very small eigenvalues to avoid numerical issues\n",
    "lambda_thresh = np.maximum(lambda_ns, 1e-10)\n",
    "weights = 1.0 / lambda_thresh\n",
    "weights = weights / weights.sum()  # Normalize\n",
    "\n",
    "print(f\"‚úì Weights computed!\")\n",
    "print(f\"  Weight range: [{weights.min():.2e}, {weights.max():.2e}]\")\n",
    "print(f\"  Effective dim (1/sum(w^2)): {1.0/np.sum(weights**2):.1f}\")\n",
    "\n",
    "# ### 6.2 EDMD Dictionary Learning for True Koopman Operator (KSWGD)\n",
    "# \n",
    "# The previous cell computed the **Diffusion Map** spectral decomposition (DMPS method).\n",
    "# Now we implement the **true KSWGD** using EDMD (Extended Dynamic Mode Decomposition):\n",
    "# \n",
    "# 1. **KDE-based drift estimation**: Estimate the score function $\\nabla \\log p(x)$ using kernel density estimation\n",
    "# 2. **Langevin evolution**: Generate time-series pairs $(X_t, X_{t+\\Delta t})$ via Langevin dynamics\n",
    "# 3. **Dictionary learning**: Learn a sparse dictionary on the latent space\n",
    "# 4. **EDMD operator**: Build the Koopman operator in dictionary space\n",
    "# \n",
    "# | Method | Basis Functions | Dynamics |\n",
    "# |--------|----------------|----------|\n",
    "# | DMPS | Diffusion map eigenvectors | Static kernel |\n",
    "# | KSWGD (EDMD) | Dictionary + Koopman eigenfunctions | Learned Langevin dynamics |\n",
    "\n",
    "# KDE-based drift estimation and Langevin evolution to build EDMD pairs\n",
    "from sklearn.decomposition import MiniBatchDictionaryLearning\n",
    "from scipy.linalg import eig\n",
    "\n",
    "print(\"=== EDMD Pipeline for True Koopman KSWGD ===\")\n",
    "\n",
    "# Step 1: KDE-based score function estimation\n",
    "dt_edmd = 0.5  # time step for EDMD Langevin evolution\n",
    "dist2_edmd = pairwise_distances(X_tar, metric=\"sqeuclidean\")  # squared distances\n",
    "h_edmd = np.sqrt(np.median(dist2_edmd) + 1e-12)  # KDE bandwidth\n",
    "W_edmd = np.exp(-dist2_edmd / (2.0 * (h_edmd ** 2)))\n",
    "sumW_edmd = np.sum(W_edmd, axis=1, keepdims=True) + 1e-12\n",
    "weighted_means_edmd = W_edmd @ X_tar / sumW_edmd\n",
    "score_edmd = (weighted_means_edmd - X_tar) / (h_edmd ** 2)  # KDE score = drift term\n",
    "\n",
    "# Step 2: Langevin step to generate (X_t, X_{t+dt}) pairs\n",
    "xi_edmd = np.random.normal(0.0, 1.0, size=X_tar.shape)\n",
    "X_tar_next = X_tar + dt_edmd * score_edmd + np.sqrt(2.0 * dt_edmd) * xi_edmd\n",
    "\n",
    "print(f\"EDMD drift bandwidth h: {h_edmd:.4f}\")\n",
    "print(f\"X_tar stats -> mean {X_tar.mean():.4f}, std {X_tar.std():.4f}\")\n",
    "print(f\"X_tar_next stats -> mean {X_tar_next.mean():.4f}, std {X_tar_next.std():.4f}\")\n",
    "\n",
    "# Step 3: Dictionary learning on latent space\n",
    "# Rule of thumb: dictionary size = 3-6x latent_dim, with N/m > 50 to avoid overfitting\n",
    "# For N=30000, d=64: recommended m = 300-500 (gives N/m = 60-100)\n",
    "n_dict_components = 300  # Dictionary atoms (total basis = n_dict_components + 1 for constant)\n",
    "dict_alpha = 1e-3  # Sparsity penalty\n",
    "dict_batch = 256\n",
    "dict_max_iter = 1000\n",
    "dict_random_state = 42\n",
    "\n",
    "print(f\"\\n=== Dictionary Learning ===\")\n",
    "print(f\"Data: N={X_tar.shape[0]} samples, d={X_tar.shape[1]} dimensions\")\n",
    "print(f\"Dictionary: {n_dict_components} atoms ‚Üí {n_dict_components + 1} total basis functions\")\n",
    "print(f\"Oversampling ratio: N/m = {X_tar.shape[0] / (n_dict_components + 1):.1f}x (recommended > 50)\")\n",
    "print(f\"Learning dictionary...\")\n",
    "\n",
    "dict_model = MiniBatchDictionaryLearning(\n",
    "    n_components=n_dict_components,\n",
    "    alpha=dict_alpha,\n",
    "    batch_size=dict_batch,\n",
    "    max_iter=dict_max_iter,\n",
    "    random_state=dict_random_state,\n",
    "    verbose=0,\n",
    "    fit_algorithm=\"lars\"\n",
    ")\n",
    "dict_model.fit(X_tar)\n",
    "\n",
    "# Transform to dictionary space and add constant term\n",
    "Phi_X = dict_model.transform(X_tar)\n",
    "Phi_Y = dict_model.transform(X_tar_next)\n",
    "Phi_X = np.hstack([np.ones((Phi_X.shape[0], 1)), Phi_X])  # Add constant basis\n",
    "Phi_Y = np.hstack([np.ones((Phi_Y.shape[0], 1)), Phi_Y])\n",
    "\n",
    "print(f\"Dictionary atoms shape: {dict_model.components_.shape}\")\n",
    "print(f\"Dictionary codes (current) Phi_X: {Phi_X.shape}\")\n",
    "print(f\"Dictionary codes (next) Phi_Y: {Phi_Y.shape}\")\n",
    "\n",
    "# Step 4: Build EDMD Koopman operator in dictionary space\n",
    "print(\"\\n=== EDMD Koopman Operator ===\")\n",
    "\n",
    "reg_edmd = 1e-3\n",
    "N_edmd, m_edmd = Phi_X.shape\n",
    "\n",
    "# EDMD: K = (Phi_X^T Phi_X + reg*I)^{-1} Phi_X^T Phi_Y\n",
    "G_edmd = (Phi_X.T @ Phi_X) / N_edmd + reg_edmd * np.eye(m_edmd)\n",
    "A_edmd = (Phi_X.T @ Phi_Y) / N_edmd\n",
    "\n",
    "# Generalized eigenvalue problem: A v = lambda G v\n",
    "eigvals_edmd, eigvecs_edmd = eig(A_edmd, G_edmd)\n",
    "idx_edmd = np.argsort(-eigvals_edmd.real)\n",
    "eigvals_edmd = eigvals_edmd[idx_edmd]\n",
    "eigvecs_edmd = eigvecs_edmd[:, idx_edmd]\n",
    "\n",
    "# Koopman eigenfunctions evaluated at data points\n",
    "efuns_edmd = Phi_X @ eigvecs_edmd\n",
    "\n",
    "print(f\"EDMD eigenvalues (first 10): {np.round(eigvals_edmd[:10].real, 4)}\")\n",
    "print(f\"EDMD eigenfunctions shape: {efuns_edmd.shape}\")\n",
    "\n",
    "# Step 5: Prepare EDMD-derived KSWGD weights (skip constant mode)\n",
    "print(\"\\n=== Prepare EDMD-KSWGD Spectral Weights ===\")\n",
    "\n",
    "lambda_ns_edmd = eigvals_edmd.real\n",
    "lambda_gen_edmd = (lambda_ns_edmd - 1.0) / dt_edmd  # Generator eigenvalues\n",
    "\n",
    "mode_skip_edmd = 1  # Skip the constant eigenfunction (eigenvalue ~ 1)\n",
    "eig_threshold_edmd = 1e-6  # Keep modes with eigenvalue > threshold\n",
    "\n",
    "valid_idx_edmd = np.arange(mode_skip_edmd, lambda_ns_edmd.shape[0])\n",
    "valid_mask_edmd = lambda_ns_edmd[mode_skip_edmd:] > eig_threshold_edmd\n",
    "valid_idx_edmd = valid_idx_edmd[valid_mask_edmd]\n",
    "\n",
    "if valid_idx_edmd.size == 0:\n",
    "    raise RuntimeError(\"No EDMD modes survived the threshold; adjust eig_threshold_edmd or dictionary size.\")\n",
    "\n",
    "# Truncated Koopman eigenfunctions for KSWGD\n",
    "phi_trunc_edmd = np.real(efuns_edmd[:, valid_idx_edmd])\n",
    "\n",
    "# Generator inverse weights\n",
    "lambda_gen_inv_edmd = np.zeros_like(lambda_gen_edmd)\n",
    "mask_nonzero_edmd = np.abs(lambda_gen_edmd) > 1e-6\n",
    "lambda_gen_inv_edmd[mask_nonzero_edmd] = 1.0 / lambda_gen_edmd[mask_nonzero_edmd]\n",
    "lambda_ns_s_ns_edmd = lambda_gen_inv_edmd[valid_idx_edmd].real\n",
    "\n",
    "print(f\"EDMD kept {valid_idx_edmd.size} Koopman modes (threshold {eig_threshold_edmd})\")\n",
    "print(f\"phi_trunc_edmd shape: {phi_trunc_edmd.shape}\")\n",
    "print(f\"lambda_ns_s_ns_edmd stats: min={lambda_ns_s_ns_edmd.min():.6f}, max={lambda_ns_s_ns_edmd.max():.6f}\")\n",
    "\n",
    "# Compare Diffusion Map (DMPS) and EDMD (KSWGD) spectra\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_show_eigs = 20\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Eigenvalue comparison\n",
    "ax1 = axes[0]\n",
    "ax1.semilogy(range(1, n_show_eigs + 1), lambda_ns[:n_show_eigs], 'o-', label=\"DMPS (Diffusion Map)\", markersize=6)\n",
    "ax1.semilogy(range(1, min(n_show_eigs, lambda_ns_edmd.size) + 1), \n",
    "             lambda_ns_edmd[:n_show_eigs], '^-', label=\"KSWGD (EDMD Koopman)\", markersize=6)\n",
    "ax1.set_xlabel(\"Eigen-index\")\n",
    "ax1.set_ylabel(\"Eigenvalue (log scale)\")\n",
    "ax1.set_title(\"Spectral Decay Comparison\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Generator eigenvalues\n",
    "ax2 = axes[1]\n",
    "lambda_gen_dmps = (lambda_ns[:n_show_eigs] - 1.0) / eps_kswgd\n",
    "ax2.plot(range(1, n_show_eigs + 1), lambda_gen_dmps, 'o-', label=\"DMPS Generator\", markersize=6)\n",
    "ax2.plot(range(1, min(n_show_eigs, lambda_gen_edmd.size) + 1), \n",
    "         lambda_gen_edmd[:n_show_eigs], '^-', label=\"EDMD Generator\", markersize=6)\n",
    "ax2.set_xlabel(\"Eigen-index\")\n",
    "ax2.set_ylabel(\"Generator Eigenvalue\")\n",
    "ax2.set_title(\"Generator Spectrum Comparison\")\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"figures/dmps_vs_kswgd_spectrum.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Method Comparison ===\")\n",
    "# print(f\"DMPS: {above_tol} diffusion map modes\")\n",
    "print(f\"KSWGD: {valid_idx_edmd.size} Koopman modes from EDMD\")\n",
    "\n",
    "# Define unified sampler supporting both DMPS and KSWGD (EDMD) methods\n",
    "def run_particle_sampler(num_particles=16, num_iters=200, step_size=0.05, rng_seed=None, method=\"dmps\"):\n",
    "    \"\"\"\n",
    "    Unified Particle Transport Sampler\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    method : str\n",
    "        \"dmps\" - Diffusion Map Particle Sampling (uses diffusion map eigenvectors)\n",
    "        \"kswgd\" - True KSWGD with EDMD Koopman operator\n",
    "    rng_seed : int or None\n",
    "        Random seed. If None, uses random seed for different results each run.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(rng_seed)  # None gives random seed each run\n",
    "    use_gpu = GPU_KSWGD and torch.cuda.is_available()\n",
    "    xp = cp if use_gpu else np\n",
    "    grad_fn = grad_ker1_gpu if use_gpu else grad_ker1\n",
    "    K_eval_fn = K_tar_eval_gpu if use_gpu else K_tar_eval\n",
    "    \n",
    "    method = method.lower()\n",
    "    if method == \"dmps\":\n",
    "        method_name = \"DMPS (Diffusion Map)\"\n",
    "        phi_use = phi_trunc\n",
    "        lambda_use = lambda_ns_s_ns\n",
    "    elif method == \"kswgd\":\n",
    "        method_name = \"KSWGD (EDMD Koopman)\"\n",
    "        phi_use = phi_trunc_edmd\n",
    "        lambda_use = lambda_ns_s_ns_edmd\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}. Use 'dmps' or 'kswgd'.\")\n",
    "    \n",
    "    print(f\"Method: {method_name}\")\n",
    "    print(f\"Backend: {'GPU (CuPy)' if use_gpu else 'CPU (NumPy)'}\")\n",
    "    print(f\"Eigenfunctions shape: {phi_use.shape}\")\n",
    "    \n",
    "    x_hist = xp.zeros((num_particles, latent_dim, num_iters), dtype=xp.float64)\n",
    "    init_particles = rng.normal(0.0, 1.0, size=(num_particles, latent_dim))\n",
    "    x_hist[:, :, 0] = xp.asarray(init_particles)\n",
    "    \n",
    "    if use_gpu:\n",
    "        X_tar_dev = cp.asarray(X_tar)\n",
    "        p_tar_dev = cp.asarray(p_tar)\n",
    "        sq_tar_dev = cp.asarray(sq_tar)\n",
    "        D_vec_dev = cp.asarray(D_vec)\n",
    "        phi_dev = cp.asarray(phi_use)\n",
    "        lambda_dev = cp.asarray(lambda_use)\n",
    "    else:\n",
    "        X_tar_dev, p_tar_dev, sq_tar_dev, D_vec_dev = X_tar, p_tar, sq_tar, D_vec\n",
    "        phi_dev, lambda_dev = phi_use, lambda_use\n",
    "    \n",
    "    iterator = trange(num_iters - 1, desc=f\"{method.upper()} Transport\", unit=\"step\")\n",
    "    for t in iterator:\n",
    "        current = x_hist[:, :, t]\n",
    "        grad_matrix = grad_fn(current, X_tar_dev, p_tar_dev, sq_tar_dev, D_vec_dev, eps_kswgd)\n",
    "        cross_matrix = K_eval_fn(X_tar_dev, current, p_tar_dev, sq_tar_dev, D_vec_dev, eps_kswgd)\n",
    "        \n",
    "        tmp = (phi_dev.T @ cross_matrix) * lambda_dev[:, None]\n",
    "        push = phi_dev @ tmp\n",
    "        \n",
    "        for dim in range(latent_dim):\n",
    "            sum_term = grad_matrix[:, :, dim] @ push\n",
    "            x_hist[:, dim, t + 1] = x_hist[:, dim, t] - (step_size / num_particles) * xp.sum(sum_term, axis=1)\n",
    "        \n",
    "        if (t + 1) % 50 == 0:\n",
    "            step_norm = x_hist[:, :, t + 1] - x_hist[:, :, t]\n",
    "            mean_disp = float(xp.mean(xp.linalg.norm(step_norm, axis=1)))\n",
    "            iterator.set_postfix({\"mean_step\": f\"{mean_disp:.3e}\"})\n",
    "            if bool(xp.any(xp.isnan(x_hist[:, :, t + 1]))):\n",
    "                print(f\"\\nWarning: NaN detected at step {t+1}\")\n",
    "                return np.asarray(xp.asnumpy(x_hist[:, :, t]) if use_gpu else x_hist[:, :, t], dtype=np.float64)\n",
    "    \n",
    "    return np.asarray(xp.asnumpy(x_hist[:, :, -1]) if use_gpu else x_hist[:, :, -1], dtype=np.float64)\n",
    "\n",
    "\n",
    "# Backward compatibility alias\n",
    "def run_kswgd_sampler(num_particles=16, num_iters=200, step_size=0.05, rng_seed=42):\n",
    "    \"\"\"Legacy wrapper - uses DMPS by default for backward compatibility\"\"\"\n",
    "    return run_particle_sampler(num_particles, num_iters, step_size, rng_seed, method=\"dmps\")\n",
    "\n",
    "\n",
    "def decode_latents_to_images(flat_latents_std):\n",
    "    \"\"\"Decode standardized latent vectors to images using MLP AutoEncoder\"\"\"\n",
    "    # 1. De-standardize\n",
    "    flat_latents = flat_latents_std * Z_std + Z_mean\n",
    "    \n",
    "    # 2. MLP AutoEncoder decode (REDUCED_DIM -> 1024-dim)\n",
    "    latent_ae.eval()\n",
    "    with torch.no_grad():\n",
    "        flat_tensor = torch.from_numpy(flat_latents).float().to(device)\n",
    "        latents_recovered = latent_ae.decode(flat_tensor).cpu().numpy()\n",
    "    \n",
    "    # 3. Reshape to (N, 4, 16, 16)\n",
    "    latents_tensor = torch.from_numpy(latents_recovered).float().view(-1, *full_latent_shape).to(device)\n",
    "    \n",
    "    # 4. VAE decode\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        decoded = vae.decode(latents_tensor / vae_scaling).sample\n",
    "        decoded_rgb = _from_vae_range(decoded)\n",
    "    \n",
    "    return decoded_rgb.cpu()\n",
    "\n",
    "print(\"Particle samplers and Decoder defined!\")\n",
    "print(f\"  Latent compression: MLP AutoEncoder (1024 -> {REDUCED_DIM})\")\n",
    "print(f\"  Available methods:\")\n",
    "print(f\"    - 'dmps': Diffusion Map Particle Sampling ({above_tol} modes)\")\n",
    "print(f\"    - 'kswgd': EDMD Koopman KSWGD ({valid_idx_edmd.size} modes)\")\n",
    "\n",
    "# ### 7.1 Run Generation: DMPS vs KSWGD Comparison\n",
    "# \n",
    "# Now we run both methods and compare results:\n",
    "# - **DMPS**: Diffusion Map Particle Sampling (faster, static kernel)\n",
    "# - **KSWGD**: EDMD-based Koopman Spectral Wasserstein Gradient Descent (learns dynamics)\n",
    "\n",
    "# ============== Spectral Memory Cleanup ==============\n",
    "import gc\n",
    "import sys\n",
    "import torch\n",
    "import psutil\n",
    "\n",
    "print(\"Starting spectral memory cleanup after eigendecomposition...\")\n",
    "\n",
    "# 1. Record memory usage before cleanup\n",
    "print(\"\\n=== Memory Usage BEFORE Cleanup ===\")\n",
    "cpu_mem_before = psutil.virtual_memory().used / 1e9\n",
    "gpu_mem_before = []\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    gpu_allocated = torch.cuda.memory_allocated(i) / 1e9\n",
    "    gpu_reserved = torch.cuda.memory_reserved(i) / 1e9\n",
    "    gpu_mem_before.append((gpu_allocated, gpu_reserved))\n",
    "    print(f\"GPU {i}: Allocated: {gpu_allocated:.2f} GB, Reserved: {gpu_reserved:.2f} GB\")\n",
    "print(f\"CPU RAM: {cpu_mem_before:.2f} GB\")\n",
    "\n",
    "# 2. Calculate and print sizes of variables to be deleted\n",
    "print(f\"\\n=== Variables to be Released (max_samples = {max_samples}) ===\")\n",
    "memory_to_release = 0.0\n",
    "variables_info = {}\n",
    "\n",
    "# Variables that can be safely deleted after eigendecomposition\n",
    "deletable_vars = [\n",
    "    'dists',           # Distance matrix: (max_samples, max_samples) - huge!  \n",
    "    'data_kernel',     # Original kernel matrix: (max_samples, max_samples)\n",
    "    'rw_kernel',       # Random walk kernel: (max_samples, max_samples)  \n",
    "    'phi',             # Full eigenvector matrix (before truncation)\n",
    "    'lambda_ns',       # Full eigenvalue array (before truncation)\n",
    "    'Z_flat',          # Original flattened latent vectors\n",
    "    'all_latents',     # List of latent vectors\n",
    "    '_eig_result',     # Temporary eigendecomposition results\n",
    "]\n",
    "\n",
    "for var in deletable_vars:\n",
    "    if var in globals():\n",
    "        obj = globals()[var]\n",
    "        size_bytes = 0\n",
    "        if hasattr(obj, 'nbytes'):\n",
    "            size_bytes = obj.nbytes\n",
    "        elif hasattr(obj, '__sizeof__'):\n",
    "            size_bytes = obj.__sizeof__()\n",
    "        elif isinstance(obj, (list, dict)):\n",
    "            size_bytes = sys.getsizeof(obj)\n",
    "            if isinstance(obj, list) and len(obj) > 0:\n",
    "                # Estimate size for list of arrays\n",
    "                if hasattr(obj[0], 'nbytes'):\n",
    "                    size_bytes += sum(item.nbytes for item in obj if hasattr(item, 'nbytes'))\n",
    "        \n",
    "        size_gb = size_bytes / 1e9\n",
    "        memory_to_release += size_gb\n",
    "        variables_info[var] = size_gb\n",
    "        print(f\"  {var}: {size_gb:.3f} GB\")\n",
    "\n",
    "print(f\"Total estimated memory to release: {memory_to_release:.2f} GB\")\n",
    "\n",
    "# 3. Ensure spectral info independence before deletion\n",
    "print(f\"\\n=== Ensuring Spectral Data Independence ===\")\n",
    "\n",
    "# Ensure phi_trunc is independent from phi\n",
    "if 'phi' in globals() and 'phi_trunc' in globals():\n",
    "    if globals()['phi'] is globals()['phi_trunc']:\n",
    "        print(\"‚ö†Ô∏è  phi and phi_trunc are the same object - creating independent copy\")\n",
    "        globals()['phi_trunc'] = globals()['phi_trunc'].copy()\n",
    "        print(\"‚úì Made phi_trunc an independent copy.\")\n",
    "    else:\n",
    "        print(\"‚úì phi_trunc is already independent from phi\")\n",
    "\n",
    "# 4. Clear Python tracebacks (which can hold references)\n",
    "sys.last_traceback = None\n",
    "sys.last_value = None\n",
    "sys.last_type = None\n",
    "\n",
    "# 5. Perform the actual deletion\n",
    "print(f\"\\n=== Deleting Large Matrices ===\")\n",
    "total_released = 0.0\n",
    "\n",
    "for var in deletable_vars:\n",
    "    if var in globals():\n",
    "        size_gb = variables_info.get(var, 0)\n",
    "        del globals()[var]\n",
    "        total_released += size_gb\n",
    "        print(f\"‚úì Deleted {var} ({size_gb:.3f} GB)\")\n",
    "\n",
    "# 6. Clean up any remaining CuPy arrays\n",
    "if 'cp' in dir() and cp is not None:\n",
    "    cp_vars_found = []\n",
    "    for name in list(globals().keys()):\n",
    "        if isinstance(globals().get(name), type(cp.ndarray(1))) if cp is not None else False:\n",
    "            cp_vars_found.append(name)\n",
    "            del globals()[name]\n",
    "    if cp_vars_found:\n",
    "        print(f\"‚úì Deleted CuPy arrays from globals: {', '.join(cp_vars_found)}\")\n",
    "\n",
    "# 7. Force memory release\n",
    "print(f\"\\n=== Forcing Memory Release ===\")\n",
    "# Python garbage collection\n",
    "for _ in range(3):\n",
    "    n_collected = gc.collect()\n",
    "    if n_collected > 0:\n",
    "        print(f\"‚úì Python GC collected {n_collected} objects\")\n",
    "\n",
    "# CuPy memory pool cleanup\n",
    "if 'cp' in dir() and cp is not None:\n",
    "    try:\n",
    "        mempool = cp.get_default_memory_pool()\n",
    "        pinned_mempool = cp.get_default_pinned_memory_pool()\n",
    "        \n",
    "        for _ in range(3):\n",
    "            mempool.free_all_blocks()\n",
    "            pinned_mempool.free_all_blocks()\n",
    "        \n",
    "        print(f\"‚úì CuPy memory pool cleared and blocks freed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó CuPy cleanup error: {e}\")\n",
    "\n",
    "# PyTorch cleanup\n",
    "torch.cuda.empty_cache()\n",
    "print(\"‚úì PyTorch CUDA cache emptied\")\n",
    "\n",
    "# 8. Record memory usage after cleanup  \n",
    "print(f\"\\n=== Memory Usage AFTER Cleanup ===\")\n",
    "cpu_mem_after = psutil.virtual_memory().used / 1e9\n",
    "gpu_mem_after = []\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    gpu_allocated = torch.cuda.memory_allocated(i) / 1e9\n",
    "    gpu_reserved = torch.cuda.memory_reserved(i) / 1e9\n",
    "    gpu_mem_after.append((gpu_allocated, gpu_reserved))\n",
    "    total = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
    "    print(f\"GPU {i}: Allocated: {gpu_allocated:.2f} GB, Reserved: {gpu_reserved:.2f} GB, Free: {total - gpu_allocated:.2f} GB\")\n",
    "print(f\"CPU RAM: {cpu_mem_after:.2f} GB\")\n",
    "\n",
    "# 9. Calculate and report memory savings\n",
    "print(f\"\\n=== Memory Cleanup Summary ===\")\n",
    "cpu_released = cpu_mem_before - cpu_mem_after\n",
    "print(f\"CPU RAM released: {cpu_released:.2f} GB\")\n",
    "\n",
    "for i in range(len(gpu_mem_before)):\n",
    "    gpu_released_allocated = gpu_mem_before[i][0] - gpu_mem_after[i][0]  \n",
    "    gpu_released_reserved = gpu_mem_before[i][1] - gpu_mem_after[i][1]\n",
    "    print(f\"GPU {i} memory released: Allocated: {gpu_released_allocated:.2f} GB, Reserved: {gpu_released_reserved:.2f} GB\")\n",
    "\n",
    "print(f\"Estimated variables released: {total_released:.2f} GB\")\n",
    "\n",
    "# 10. Verify essential spectral data is preserved\n",
    "print(f\"\\n=== Essential Spectral Data Verification ===\")\n",
    "essential_vars = [\n",
    "    'phi_trunc',        # Truncated eigenvectors (n_samples, above_tol)\n",
    "    'lambda_ns_s_ns',   # Processed eigenvalue weights (above_tol,)\n",
    "    'X_tar',            # Standardized target samples (n_samples, latent_dim)\n",
    "    'p_tar',            # Target distribution weights (n_samples,)\n",
    "    'sq_tar',           # Squared sums of target samples (n_samples,) \n",
    "    'D_vec',            # Normalization vector (n_samples,)\n",
    "    'eps_kswgd',        # Kernel bandwidth parameter\n",
    "    'Z_mean',           # Standardization mean\n",
    "    'Z_std',            # Standardization std\n",
    "]\n",
    "\n",
    "all_essential_ok = True\n",
    "for var in essential_vars:\n",
    "    if var in globals():\n",
    "        obj = globals()[var]\n",
    "        if hasattr(obj, 'shape'):\n",
    "            print(f\"‚úì {var}: shape {obj.shape}, dtype {obj.dtype}\")\n",
    "        else:\n",
    "            print(f\"‚úì {var}: {type(obj).__name__} = {obj}\")\n",
    "    else:\n",
    "        print(f\"‚úó {var}: MISSING!\")\n",
    "        all_essential_ok = False\n",
    "\n",
    "if all_essential_ok:\n",
    "    print(f\"\\n‚úÖ All essential spectral data preserved! KSWGD sampling can proceed.\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Some essential data is missing! Check before running KSWGD.\")\n",
    "\n",
    "print(f\"\\n‚úÖ Spectral memory cleanup complete!\")\n",
    "print(f\"   Total estimated release: {total_released:.2f} GB\")\n",
    "print(f\"   Kernel matrix ({max_samples}x{max_samples}) and full eigenvectors deleted\")\n",
    "print(f\"   Truncated spectral info (k_eig={k_eig}) preserved for KSWGD\")\n",
    "\n",
    "# Run KSWGD sampling to generate new latent vectors\n",
    "kswgd_config = {\n",
    "    \"num_particles\": 16,  # (n_generate) Same as LDM generation count for fair comparison\n",
    "    \"num_iters\": 1000,             # Number of iterations (increased for better convergence)\n",
    "    \"step_size\": 0.01,            # Step size (smaller for stability)\n",
    "    # \"rng_seed\": 42\n",
    "}\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"KSWGD Generation Config:\")\n",
    "for k, v in kswgd_config.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nNote: Generating {kswgd_config['num_particles']} particles for reliable FID...\")\n",
    "print(\"This will take a while...\")\n",
    "\n",
    "# Run KSWGD (progress bar is built into run_kswgd_sampler via trange)\n",
    "start_time = time.time()\n",
    "Z_kswgd_std = run_kswgd_sampler(**kswgd_config)\n",
    "kswgd_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úì KSWGD Complete!\")\n",
    "print(f\"Generated samples shape: {Z_kswgd_std.shape}\")\n",
    "print(f\"Total time: {kswgd_time:.1f} seconds ({kswgd_time/60:.1f} minutes)\")\n",
    "\n",
    "# Move models to GPU for decoding\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "decode_device = torch.device(\"cuda\")\n",
    "print(f\"Moving models to {decode_device}...\")\n",
    "vae = vae.to(decode_device)\n",
    "latent_ae = latent_ae.to(decode_device)\n",
    "print(f\"‚úì Models (VAE, MLP AutoEncoder) moved to {decode_device}\")\n",
    "\n",
    "# Redefine helper to ensure it uses the correct device\n",
    "def decode_latents_to_images_safe(flat_latents_std, target_device):\n",
    "    \"\"\"Decode standardized latent vectors to images using MLP AutoEncoder\"\"\"\n",
    "    # 1. De-standardize\n",
    "    # Ensure Z_std and Z_mean are available\n",
    "    if 'Z_std' not in globals() or 'Z_mean' not in globals():\n",
    "        raise ValueError(\"Z_std or Z_mean not found in globals. Please run the standardization cell first.\")\n",
    "        \n",
    "    flat_latents = flat_latents_std * Z_std + Z_mean\n",
    "    \n",
    "    # 2. MLP AutoEncoder decode (REDUCED_DIM -> 1024-dim)\n",
    "    latent_ae.eval()\n",
    "    with torch.no_grad():\n",
    "        flat_tensor = torch.from_numpy(flat_latents).float().to(target_device)\n",
    "        latents_recovered = latent_ae.decode(flat_tensor).cpu().numpy()\n",
    "    \n",
    "    # 3. Reshape to (N, 4, 16, 16)\n",
    "    # Ensure full_latent_shape is available\n",
    "    shape = full_latent_shape if 'full_latent_shape' in globals() else (4, 16, 16)\n",
    "    latents_tensor = torch.from_numpy(latents_recovered).float().view(-1, *shape).to(target_device)\n",
    "    \n",
    "    # 4. VAE decode\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        # Ensure vae_scaling is available\n",
    "        scaling = vae_scaling if 'vae_scaling' in globals() else 1.0\n",
    "        decoded = vae.decode(latents_tensor / scaling).sample\n",
    "        \n",
    "        # Ensure _from_vae_range is available\n",
    "        if '_from_vae_range' in globals():\n",
    "            decoded_rgb = _from_vae_range(decoded)\n",
    "        else:\n",
    "            decoded_rgb = torch.clamp((decoded + 1.0) * 0.5, 0.0, 1.0)\n",
    "    \n",
    "    return decoded_rgb.cpu()\n",
    "\n",
    "# Decode KSWGD generated latent vectors to images (in batches to avoid OOM)\n",
    "if 'Z_kswgd_std' in globals():\n",
    "    print(f\"Decoding {Z_kswgd_std.shape[0]} latent vectors to images...\")\n",
    "    \n",
    "    decode_batch_size = 128  # Increased for 80GB GPUs\n",
    "    all_kswgd_images = []\n",
    "    \n",
    "    for i in tqdm(range(0, Z_kswgd_std.shape[0], decode_batch_size), desc=\"Decoding KSWGD\"):\n",
    "        batch_latents = Z_kswgd_std[i:i+decode_batch_size]\n",
    "        \n",
    "        # Use the safe local function with explicit device\n",
    "        batch_images = decode_latents_to_images_safe(batch_latents, decode_device)\n",
    "        all_kswgd_images.append(batch_images.numpy())\n",
    "        \n",
    "        # Clear cache periodically\n",
    "        if (i // decode_batch_size + 1) % 20 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    kswgd_images_np = np.concatenate(all_kswgd_images, axis=0)\n",
    "    \n",
    "    print(f\"\\n‚úì Decoding complete!\")\n",
    "    print(f\"Generated images shape: {kswgd_images_np.shape}\")\n",
    "    print(f\"Pixel value range: [{kswgd_images_np.min():.3f}, {kswgd_images_np.max():.3f}]\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Z_kswgd_std not found. Please run KSWGD sampling first.\")\n",
    "\n",
    "# Visualize KSWGD generated images (show first 16 of 10000)\n",
    "n_show = min(16, kswgd_images_np.shape[0])\n",
    "n_cols = 4\n",
    "n_rows = (n_show + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(3 * n_cols, 3 * n_rows))\n",
    "axes = np.asarray(axes).reshape(-1)\n",
    "\n",
    "for idx in range(n_show):\n",
    "    img = np.transpose(kswgd_images_np[idx], (1, 2, 0))  # (C,H,W) ‚Üí (H,W,C)\n",
    "    axes[idx].imshow(np.clip(img, 0.0, 1.0))\n",
    "    axes[idx].set_title(f\"KSWGD #{idx+1}\")\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "for idx in range(n_show, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "# Get image size from KSWGD generated images (shape is N, C, H, W)\n",
    "_kswgd_img_size = kswgd_images_np.shape[2]\n",
    "plt.suptitle(f\"KSWGD CelebA-HQ ({_kswgd_img_size}x{_kswgd_img_size})\", fontsize=22)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nGeneration Summary:\")\n",
    "# print(f\"  - LDM: {len(ldm_generated_images)} images via UNet Denoising (200 steps)\")\n",
    "print(f\"  - KSWGD: {kswgd_images_np.shape[0]} images via KSWGD ({kswgd_config['num_iters']} steps)\")\n",
    "\n",
    "# ============== Apply GFPGAN to KSWGD Generated Images ==============\n",
    "print(\"=\" * 60)\n",
    "print(\"Upscaling KSWGD Generated Images with GFPGAN\")\n",
    "print(\"(with preprocessing: Gaussian + Bilateral + Color Norm)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Convert KSWGD images to the right format for upscaling\n",
    "kswgd_images_for_upscale = []\n",
    "for i in range(kswgd_images_np.shape[0]):\n",
    "    img = np.transpose(kswgd_images_np[i], (1, 2, 0))  # (C,H,W) ‚Üí (H,W,C)\n",
    "    img = np.clip(img, 0, 1)\n",
    "    kswgd_images_for_upscale.append(img)\n",
    "\n",
    "print(f\"\\nEnhancing {len(kswgd_images_for_upscale)} KSWGD images with preprocessing + GFPGAN...\")\n",
    "kswgd_enhanced = upscale_images(kswgd_images_for_upscale, use_face_enhance=True, use_preprocess=True, desc=\"Preprocess+GFPGAN (KSWGD)\")\n",
    "\n",
    "# Visualization (show up to 16 in a grid)\n",
    "n_test_kswgd = len(kswgd_enhanced)\n",
    "n_show_kswgd = min(16, n_test_kswgd)\n",
    "n_cols = 4\n",
    "n_rows = (n_show_kswgd + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))\n",
    "axes = np.asarray(axes).reshape(-1)\n",
    "\n",
    "for i in range(n_show_kswgd):\n",
    "    axes[i].imshow(kswgd_enhanced[i])\n",
    "    # axes[i].set_title(f\"#{i+1}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "for i in range(n_show_kswgd, len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "# Get image size from enhanced images\n",
    "_kswgd_enhanced_size = kswgd_enhanced[0].shape[1] if isinstance(kswgd_enhanced[0], np.ndarray) else kswgd_enhanced[0].size[0]\n",
    "plt.suptitle(f\"KSWGD CelebA-HQ ({_kswgd_enhanced_size}x{_kswgd_enhanced_size})\\n\", fontsize=22, y=1.02)\n",
    "plt.tight_layout()\n",
    "# plt.savefig('/workspace/kswgd/figures/kswgd_gfpgan_enhanced.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Store enhanced KSWGD images\n",
    "kswgd_upscaled = kswgd_enhanced\n",
    "\n",
    "print(f\"\\n‚úì KSWGD Enhancement complete!\")\n",
    "print(f\"  Original KSWGD: 256x256\")\n",
    "print(f\"  Enhanced: {kswgd_upscaled[0].shape[1]}x{kswgd_upscaled[0].shape[0]}\")\n",
    "print(f\"  Preprocessing applied: Gaussian blur (3x3, œÉ=0.5) + Bilateral (d=5) + Color norm (Œ±=0.3)\")\n",
    "\n",
    "# ### 7.2 KSWGD FID Evaluation & Memory Cleanup\n",
    "# \n",
    "# Compute FID scores for KSWGD generated images **immediately after generation**, then clean up to free memory for LDM.\n",
    "\n",
    "# ============== FID Utility Functions ==============\n",
    "# Install pytorch-fid if needed\n",
    "import subprocess\n",
    "import sys\n",
    "import random\n",
    "\n",
    "try:\n",
    "    from pytorch_fid import fid_score\n",
    "    from pytorch_fid.inception import InceptionV3\n",
    "except ImportError:\n",
    "    print(\"Installing pytorch-fid...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pytorch-fid\", \"-q\"])\n",
    "    from pytorch_fid import fid_score\n",
    "    from pytorch_fid.inception import InceptionV3\n",
    "\n",
    "from scipy import linalg\n",
    "from torchvision import transforms as T\n",
    "\n",
    "print(\"FID computation: Using single GPU mode\")\n",
    "\n",
    "def load_real_images_for_fid(dataset, n_samples, size=256, desc=\"Loading\"):\n",
    "    \"\"\"Load real images from dataset for FID computation\"\"\"\n",
    "    images = []\n",
    "    indices = random.sample(range(len(dataset)), min(n_samples, len(dataset)))\n",
    "    \n",
    "    transform = T.Compose([\n",
    "        T.Resize((size, size)),\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    for idx in tqdm(indices, desc=desc):\n",
    "        img = dataset[idx]['image']\n",
    "        if not isinstance(img, Image.Image):\n",
    "            img = Image.fromarray(img)\n",
    "        img = transform(img)\n",
    "        images.append(img)\n",
    "    \n",
    "    return torch.stack(images)\n",
    "\n",
    "def get_inception_features(images, batch_size=256, desc=\"Extracting features\"):\n",
    "    \"\"\"Extract Inception-v3 features\n",
    "    \n",
    "    Args:\n",
    "        images: Can be:\n",
    "            - torch.Tensor with shape (N, C, H, W) - values in [0, 1]\n",
    "            - np.ndarray with shape (N, C, H, W) - values in [0, 1]\n",
    "            - np.ndarray with shape (N, H, W, C) - values in [0, 1] or [0, 255]\n",
    "            - list of np.ndarray with shape (H, W, C)\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n",
    "    inception = InceptionV3([block_idx]).to(device)\n",
    "    inception.eval()\n",
    "\n",
    "    preprocess = T.Compose([\n",
    "        T.Resize((299, 299)),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    features = []\n",
    "    \n",
    "    # Handle list of images\n",
    "    if isinstance(images, list):\n",
    "        images = np.stack(images)\n",
    "    \n",
    "    n_batches = (len(images) + batch_size - 1) // batch_size\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(n_batches), desc=desc):\n",
    "            batch = images[i*batch_size:(i+1)*batch_size]\n",
    "            \n",
    "            # Convert to torch tensor if needed\n",
    "            if isinstance(batch, np.ndarray):\n",
    "                batch = torch.from_numpy(batch).float()\n",
    "            \n",
    "            # Check shape and permute if needed\n",
    "            # We need (N, C, H, W) format where C=3\n",
    "            if batch.dim() == 4:\n",
    "                # If shape is (N, H, W, C) where last dim is 3, permute to (N, C, H, W)\n",
    "                if batch.shape[-1] == 3 and batch.shape[1] != 3:\n",
    "                    batch = batch.permute(0, 3, 1, 2)\n",
    "                # If shape is already (N, C, H, W), no change needed\n",
    "            elif batch.dim() == 3:\n",
    "                # Single image (H, W, C) or (C, H, W)\n",
    "                if batch.shape[-1] == 3:\n",
    "                    batch = batch.permute(2, 0, 1).unsqueeze(0)\n",
    "                else:\n",
    "                    batch = batch.unsqueeze(0)\n",
    "            \n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            # Normalize to [0, 1] if values are in [0, 255]\n",
    "            if batch.max() > 1.0:\n",
    "                batch = batch / 255.0\n",
    "            \n",
    "            batch = preprocess(batch)\n",
    "            feat = inception(batch)[0]\n",
    "            feat = feat.squeeze(-1).squeeze(-1)\n",
    "            features.append(feat.cpu().numpy())\n",
    "\n",
    "    del inception\n",
    "    torch.cuda.empty_cache()\n",
    "    return np.vstack(features)\n",
    "\n",
    "def calculate_fid_from_features(feat1, feat2):\n",
    "    \"\"\"Calculate FID from pre-extracted features\"\"\"\n",
    "    mu1, sigma1 = feat1.mean(axis=0), np.cov(feat1, rowvar=False)\n",
    "    mu2, sigma2 = feat2.mean(axis=0), np.cov(feat2, rowvar=False)\n",
    "    \n",
    "    diff = mu1 - mu2\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    \n",
    "    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2*covmean)\n",
    "    return float(fid)\n",
    "\n",
    "def compute_fid(images1, images2, batch_size=256):\n",
    "    \"\"\"Compute FID between two sets of images\"\"\"\n",
    "    print(f\"Computing FID between {len(images1)} and {len(images2)} images...\")\n",
    "    \n",
    "    feat1 = get_inception_features(images1, batch_size, desc=\"Features (Set 1)\")\n",
    "    feat2 = get_inception_features(images2, batch_size, desc=\"Features (Set 2)\")\n",
    "    \n",
    "    return calculate_fid_from_features(feat1, feat2)\n",
    "\n",
    "print(\"‚úì FID utility functions loaded\")\n",
    "\n",
    "# ============== KSWGD FID Evaluation (Raw 256x256 + Enhanced 1024x1024) ==============\n",
    "import gc\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Computing FID Scores for KSWGD Generated Images...\")\n",
    "print(f\"  KSWGD raw (256): {len(kswgd_images_np)} images\")\n",
    "print(f\"  KSWGD enhanced (1024): {len(kswgd_upscaled)} images\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Load real images for FID (256x256)\n",
    "n_real_samples = min(10000, len(celebahq_dataset))\n",
    "print(f\"\\n[1/5] Loading real CelebA-HQ images (256x256)...\")\n",
    "real_images_256 = load_real_images_for_fid(celebahq_dataset, n_real_samples, size=256, desc=\"Real images (256)\")\n",
    "print(f\"  Real images loaded: {len(real_images_256)}\")\n",
    "\n",
    "# 2. Extract features for 256x256\n",
    "print(\"\\n[2/5] Extracting Inception features from real images (256)...\")\n",
    "real_features_256 = get_inception_features(real_images_256, desc=\"Real (256)\")\n",
    "\n",
    "print(\"\\n[3/5] Extracting Inception features from KSWGD raw images (256)...\")\n",
    "kswgd_features_256 = get_inception_features(kswgd_images_np, desc=\"KSWGD Raw (256)\")\n",
    "\n",
    "# 3. Calculate KSWGD raw FID (from pre-extracted features)\n",
    "fid_kswgd_raw = calculate_fid_from_features(real_features_256, kswgd_features_256)\n",
    "print(f\"\\n‚úì KSWGD Raw (256x256) FID = {fid_kswgd_raw:.2f}\")\n",
    "\n",
    "# 4. Clean up 256x256 real images to free memory\n",
    "del real_images_256\n",
    "gc.collect()\n",
    "print(\"  (256x256 real images cleaned up)\")\n",
    "\n",
    "# 5. Load real images for FID (1024x1024)\n",
    "print(f\"\\n[4/5] Loading real CelebA-HQ images (1024x1024)...\")\n",
    "real_images_1024 = load_real_images_for_fid(celebahq_dataset, n_real_samples, size=1024, desc=\"Real images (1024)\")\n",
    "print(f\"  Real images loaded: {len(real_images_1024)}\")\n",
    "\n",
    "# 6. Extract features for 1024x1024\n",
    "print(\"\\n[5/5] Extracting Inception features...\")\n",
    "real_features_1024 = get_inception_features(real_images_1024, desc=\"Real (1024)\")\n",
    "kswgd_features_1024 = get_inception_features(kswgd_upscaled, desc=\"KSWGD Enhanced (1024)\")\n",
    "\n",
    "# 7. Calculate KSWGD enhanced FID (from pre-extracted features)\n",
    "fid_kswgd_enhanced = calculate_fid_from_features(real_features_1024, kswgd_features_1024)\n",
    "print(f\"\\n‚úì KSWGD Enhanced (1024x1024) FID = {fid_kswgd_enhanced:.2f}\")\n",
    "\n",
    "# 8. Print KSWGD FID Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KSWGD FID RESULTS (Lower is Better)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Real images:          {n_real_samples}\")\n",
    "print(f\"  KSWGD (Raw 256):      {len(kswgd_images_np)} ‚Üí FID = {fid_kswgd_raw:.2f}\")\n",
    "print(f\"  KSWGD (Enhanced 1024): {len(kswgd_upscaled)} ‚Üí FID = {fid_kswgd_enhanced:.2f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============== KSWGD Memory Cleanup ==============\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"KSWGD Memory Cleanup\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Record memory before cleanup\n",
    "cpu_before = psutil.virtual_memory().used / 1e9\n",
    "gpu_before = [torch.cuda.memory_allocated(i) / 1e9 for i in range(torch.cuda.device_count())]\n",
    "\n",
    "# Variables to clean up after KSWGD FID is computed\n",
    "# Keep: real_features_1024, real_features_256 (for LDM comparison)\n",
    "# Keep: fid_kswgd_raw, fid_kswgd_enhanced (results)\n",
    "kswgd_cleanup_vars = [\n",
    "    'kswgd_images_np',       # Raw KSWGD images (750 MB)\n",
    "    'kswgd_images_for_upscale',  # Images prepared for upscale\n",
    "    'all_kswgd_images',      # Decoded images list\n",
    "    'kswgd_upscaled',        # Enhanced images (3 GB)\n",
    "    'kswgd_enhanced',        # Enhanced images copy\n",
    "    'kswgd_features_256',    # Features (can recompute if needed)\n",
    "    'kswgd_features_1024',   # Features (can recompute if needed)\n",
    "    'Z_kswgd_std',           # KSWGD latent vectors\n",
    "    'real_images_1024',      # Real images (120 GB!) - will reload for LDM\n",
    "]\n",
    "\n",
    "deleted_vars = []\n",
    "for var in kswgd_cleanup_vars:\n",
    "    if var in globals():\n",
    "        size_mb = 0\n",
    "        obj = globals()[var]\n",
    "        if hasattr(obj, 'nbytes'):\n",
    "            size_mb = obj.nbytes / 1e6\n",
    "        elif isinstance(obj, list) and len(obj) > 0:\n",
    "            if hasattr(obj[0], 'nbytes'):\n",
    "                size_mb = sum(x.nbytes for x in obj if hasattr(x, 'nbytes')) / 1e6\n",
    "            elif hasattr(obj[0], 'shape'):\n",
    "                size_mb = sum(x.nbytes if hasattr(x, 'nbytes') else 0 for x in obj) / 1e6\n",
    "        del globals()[var]\n",
    "        deleted_vars.append(f\"{var} ({size_mb:.1f} MB)\")\n",
    "\n",
    "# Force garbage collection\n",
    "for _ in range(3):\n",
    "    gc.collect()\n",
    "\n",
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# CuPy cleanup if available\n",
    "if 'cp' in dir() and cp is not None:\n",
    "    try:\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "        cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Record memory after cleanup\n",
    "cpu_after = psutil.virtual_memory().used / 1e9\n",
    "gpu_after = [torch.cuda.memory_allocated(i) / 1e9 for i in range(torch.cuda.device_count())]\n",
    "\n",
    "print(f\"\\nDeleted variables:\")\n",
    "for v in deleted_vars:\n",
    "    print(f\"  ‚úì {v}\")\n",
    "\n",
    "print(f\"\\nMemory Released:\")\n",
    "print(f\"  CPU RAM: {cpu_before - cpu_after:.2f} GB\")\n",
    "for i in range(len(gpu_before)):\n",
    "    print(f\"  GPU {i}: {gpu_before[i] - gpu_after[i]:.2f} GB\")\n",
    "\n",
    "print(f\"\\nCurrent Memory Usage:\")\n",
    "print(f\"  CPU RAM: {cpu_after:.2f} GB\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    alloc = torch.cuda.memory_allocated(i) / 1e9\n",
    "    total = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
    "    print(f\"  GPU {i}: {alloc:.2f} GB / {total:.2f} GB\")\n",
    "\n",
    "print(f\"\\n‚úÖ KSWGD cleanup complete! Ready for LDM generation.\")\n",
    "\n",
    "# ### 7.3 LDM Generation, Enhancement & FID Evaluation\n",
    "# \n",
    "# Now that KSWGD is complete and memory is freed, we generate LDM images, enhance them, compute FID, and clean up.\n",
    "\n",
    "# ============== LDM Generation (After KSWGD) ==============\n",
    "import time\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "# Configuration\n",
    "if 'kswgd_config' in globals():\n",
    "    n_generate = kswgd_config.get('num_particles', 16)\n",
    "    print(f\"Using n_generate = {n_generate} (matched with KSWGD)\")\n",
    "else:\n",
    "    n_generate = 16  # Default if KSWGD not run\n",
    "    print(f\"Using default n_generate = {n_generate}\")\n",
    "\n",
    "batch_size = 64    # Batch size for 80GB GPUs\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LDM Generation (After KSWGD)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Generating {n_generate} unconditional LDM images...\")\n",
    "\n",
    "# Check if LDM pipeline exists and is valid, reload if needed\n",
    "should_reload = False\n",
    "if 'ldm_pipe' not in globals():\n",
    "    should_reload = True\n",
    "else:\n",
    "    if not hasattr(ldm_pipe, 'unet') or ldm_pipe.unet is None:\n",
    "        print(\"‚ö†Ô∏è LDM pipeline found but appears corrupted. Reloading...\")\n",
    "        should_reload = True\n",
    "\n",
    "if should_reload:\n",
    "    print(\"Loading LDM pipeline...\")\n",
    "    ldm_pipe = DiffusionPipeline.from_pretrained(\"CompVis/ldm-celebahq-256\")\n",
    "    ldm_pipe = ldm_pipe.to(\"cuda\")\n",
    "    ldm_pipe.vqvae.config.scaling_factor = 1.0\n",
    "else:\n",
    "    print(\"Using existing LDM pipeline...\")\n",
    "    ldm_pipe = ldm_pipe.to(\"cuda\")\n",
    "    ldm_pipe.vqvae.config.scaling_factor = 1.0\n",
    "\n",
    "print(\"Using single GPU mode\")\n",
    "\n",
    "start_time = time.time()\n",
    "ldm_images = []\n",
    "n_batches = (n_generate + batch_size - 1) // batch_size\n",
    "\n",
    "for batch_idx in tqdm(range(n_batches), desc=\"LDM Generation\"):\n",
    "    current_batch_size = min(batch_size, n_generate - batch_idx * batch_size)\n",
    "    output = ldm_pipe(\n",
    "        batch_size=current_batch_size,\n",
    "        num_inference_steps=200,\n",
    "        output_type=\"pil\"\n",
    "    )\n",
    "    ldm_images.extend(output.images)\n",
    "\n",
    "ldm_time = time.time() - start_time\n",
    "\n",
    "# Create alias for consistency with later cells\n",
    "ldm_generated_images = ldm_images\n",
    "\n",
    "\n",
    "print(f\"\\n‚úì Generated {len(ldm_images)} LDM images in {ldm_time:.2f}s ({ldm_time/len(ldm_images):.2f}s/img)\")\n",
    "\n",
    "# ============== LDM Enhancement with GFPGAN ==============\n",
    "print(\"=\" * 60)\n",
    "print(\"Enhancing LDM Generated Images with GFPGAN\")\n",
    "print(\"(with preprocessing: Gaussian + Bilateral + Color Norm)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "n_test = len(ldm_generated_images)\n",
    "print(f\"\\nEnhancing {n_test} LDM images...\")\n",
    "ldm_enhanced = upscale_images(ldm_generated_images, use_face_enhance=True, use_preprocess=True, desc=\"LDM GFPGAN\")\n",
    "ldm_upscaled = ldm_enhanced\n",
    "\n",
    "# Visualization\n",
    "n_show = min(16, n_test)\n",
    "n_cols = 4\n",
    "n_rows = (n_show + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))\n",
    "axes = np.asarray(axes).reshape(-1)\n",
    "\n",
    "for i in range(n_show):\n",
    "    axes[i].imshow(ldm_enhanced[i])\n",
    "    axes[i].axis('off')\n",
    "\n",
    "for i in range(n_show, len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "_ldm_enhanced_size = ldm_enhanced[0].shape[1] if isinstance(ldm_enhanced[0], np.ndarray) else ldm_enhanced[0].size[0]\n",
    "plt.suptitle(f\"LDM Enhanced ({_ldm_enhanced_size}x{_ldm_enhanced_size})\", fontsize=22, y=0.98)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "# plt.savefig('/workspace/kswgd/figures/ldm_gfpgan_enhanced.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì LDM Enhancement complete!\")\n",
    "print(f\"  Original: 256x256 ‚Üí Enhanced: {ldm_upscaled[0].shape[1]}x{ldm_upscaled[0].shape[0]}\")\n",
    "\n",
    "# ============== LDM FID Evaluation ==============\n",
    "import gc\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Computing FID Scores for LDM Generated Images...\")\n",
    "print(f\"  LDM raw (256): {len(ldm_generated_images)} images\")\n",
    "print(f\"  LDM enhanced (1024): {len(ldm_upscaled)} images\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Load real images for FID (256x256)\n",
    "n_real_samples = min(10000, len(celebahq_dataset))\n",
    "print(f\"\\n[1/5] Loading real CelebA-HQ images (256x256)...\")\n",
    "real_images_256 = load_real_images_for_fid(celebahq_dataset, n_real_samples, size=256, desc=\"Real images (256)\")\n",
    "print(f\"  Real images loaded: {len(real_images_256)}\")\n",
    "\n",
    "# 2. Extract features for 256x256\n",
    "print(\"\\n[2/5] Extracting Inception features from real images (256)...\")\n",
    "real_features_256 = get_inception_features(real_images_256, desc=\"Real (256)\")\n",
    "\n",
    "print(\"\\n[3/5] Extracting Inception features from LDM raw images (256)...\")\n",
    "ldm_features_256 = get_inception_features(ldm_generated_images, desc=\"LDM Raw (256)\")\n",
    "\n",
    "# 3. Calculate LDM raw FID (use calculate_fid_from_features since we have features)\n",
    "fid_ldm_raw = calculate_fid_from_features(real_features_256, ldm_features_256)\n",
    "print(f\"\\n‚úì LDM Raw (256x256) FID = {fid_ldm_raw:.2f}\")\n",
    "\n",
    "# 4. Clean up 256x256 real images\n",
    "del real_images_256\n",
    "gc.collect()\n",
    "print(\"  (256x256 real images cleaned up)\")\n",
    "\n",
    "# 5. Load real images for FID (1024x1024)\n",
    "print(f\"\\n[4/5] Loading real CelebA-HQ images (1024x1024)...\")\n",
    "real_images_1024 = load_real_images_for_fid(celebahq_dataset, n_real_samples, size=1024, desc=\"Real images (1024)\")\n",
    "print(f\"  Real images loaded: {len(real_images_1024)}\")\n",
    "\n",
    "# 6. Extract features for 1024x1024\n",
    "print(\"\\n[5/5] Extracting Inception features...\")\n",
    "real_features_1024 = get_inception_features(real_images_1024, desc=\"Real (1024)\")\n",
    "ldm_features_1024 = get_inception_features(ldm_upscaled, desc=\"LDM Enhanced (1024)\")\n",
    "\n",
    "# 7. Calculate LDM enhanced FID (use calculate_fid_from_features since we have features)\n",
    "fid_ldm_enhanced = calculate_fid_from_features(real_features_1024, ldm_features_1024)\n",
    "print(f\"\\n‚úì LDM Enhanced (1024x1024) FID = {fid_ldm_enhanced:.2f}\")\n",
    "\n",
    "# 8. Print LDM FID Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LDM FID RESULTS (Lower is Better)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Real images:         {n_real_samples}\")\n",
    "print(f\"  LDM (Raw 256):      {len(ldm_generated_images)} ‚Üí FID = {fid_ldm_raw:.2f}\")\n",
    "print(f\"  LDM (Enhanced 1024): {len(ldm_upscaled)} ‚Üí FID = {fid_ldm_enhanced:.2f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============== LDM Memory Cleanup & Final Summary ==============\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LDM Memory Cleanup & Final Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Memory cleanup\n",
    "cpu_before = psutil.virtual_memory().used / 1e9\n",
    "\n",
    "ldm_cleanup_vars = [\n",
    "    'ldm_generated_images',  # Raw LDM images\n",
    "    'ldm_upscaled',          # Enhanced images\n",
    "    'ldm_enhanced',          # Enhanced images copy\n",
    "    'ldm_features_256',      # Features\n",
    "    'ldm_features_1024',     # Features\n",
    "    'real_images_1024',      # Real images (120 GB!)\n",
    "    'real_features_256',     # Features\n",
    "    'real_features_1024',    # Features\n",
    "]\n",
    "\n",
    "deleted_vars = []\n",
    "for var in ldm_cleanup_vars:\n",
    "    if var in globals():\n",
    "        del globals()[var]\n",
    "        deleted_vars.append(var)\n",
    "\n",
    "for _ in range(3):\n",
    "    gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "cpu_after = psutil.virtual_memory().used / 1e9\n",
    "\n",
    "print(f\"\\nDeleted: {', '.join(deleted_vars)}\")\n",
    "print(f\"CPU RAM released: {cpu_before - cpu_after:.2f} GB\")\n",
    "\n",
    "# ============== FINAL FID COMPARISON ==============\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"                    FINAL FID COMPARISON RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Method':<25} {'Resolution':<15} {'Samples':<10} {'FID ‚Üì':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Check if FID results exist\n",
    "if 'fid_kswgd_raw' in globals():\n",
    "    print(f\"{'KSWGD':<25} {'256x256':<15} {kswgd_config['num_particles']:<10} {fid_kswgd_raw:.2f}\")\n",
    "if 'fid_kswgd_enhanced' in globals():\n",
    "    print(f\"{'KSWGD + GFPGAN':<25} {'1024x1024':<15} {kswgd_config['num_particles']:<10} {fid_kswgd_enhanced:.2f}\")\n",
    "if 'fid_ldm_raw' in globals():\n",
    "    print(f\"{'LDM':<25} {'256x256':<15} {n_generate:<10} {fid_ldm_raw:.2f}\")\n",
    "if 'fid_ldm_enhanced' in globals():\n",
    "    print(f\"{'LDM + GFPGAN':<25} {'1024x1024':<15} {n_generate:<10} {fid_ldm_enhanced:.2f}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Determine winner\n",
    "if 'fid_kswgd_raw' in globals() and 'fid_ldm_raw' in globals():\n",
    "    print(\"\\nüìä Analysis:\")\n",
    "    print(f\"  Raw 256x256:      {'KSWGD' if fid_kswgd_raw < fid_ldm_raw else 'LDM'} wins by {abs(fid_kswgd_raw - fid_ldm_raw):.2f}\")\n",
    "if 'fid_kswgd_enhanced' in globals() and 'fid_ldm_enhanced' in globals():\n",
    "    print(f\"  Enhanced 1024x1024: {'KSWGD' if fid_kswgd_enhanced < fid_ldm_enhanced else 'LDM'} wins by {abs(fid_kswgd_enhanced - fid_ldm_enhanced):.2f}\")\n",
    "\n",
    "print(\"\\n‚úÖ All evaluations complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kswgd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
