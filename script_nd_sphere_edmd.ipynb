{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66cbf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79290547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility (optional)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Sample 500 points from a 3D Gaussian (as in the MATLAB code)\n",
    "n = 500\n",
    "d = 10\n",
    "lambda_ = 1\n",
    "u = np.random.normal(0, 1, (n, d))\n",
    "# fix variable naming consistency\n",
    "u[:, 0] = lambda_ * u[:, 0]\n",
    "u_norm = np.linalg.norm(u, axis=1, keepdims=True)\n",
    "r = np.sqrt(np.random.rand(n, 1)) * 1/100 + 99/100\n",
    "u_trans = u / u_norm\n",
    "\n",
    "# X_tar = r * u_trans (element-wise multiplication)\n",
    "X_tar = r * u_trans\n",
    "# n = X_tar.shape[0] (already n=500, but keep for consistency)\n",
    "n = X_tar.shape[0]\n",
    "\n",
    "# Form the anisotropic graph Laplacian inputs\n",
    "sq_tar = np.sum(X_tar ** 2, axis=1)\n",
    "H = sq_tar[:, None] + sq_tar[None, :] - 2 * (X_tar @ X_tar.T)\n",
    "epsilon = 0.5 * np.median(H) / np.log(n + 1)\n",
    "# Kernel function\n",
    "def ker(X):\n",
    "    sq_tar = np.sum(X ** 2, axis=1)\n",
    "    return np.exp(-(sq_tar[:, None] + sq_tar[None, :] - 2 * (X @ X.T)) / (2 * epsilon))\n",
    "\n",
    "data_kernel = ker(X_tar)\n",
    "# Degree-like quantities needed by grad_ker1 / K_tar_eval\n",
    "p_x = np.sqrt(np.sum(data_kernel, axis=1))\n",
    "p_y = p_x.copy()\n",
    "\n",
    "# Diffusion maps normalization and symmetric random-walk operator\n",
    "data_kernel_norm = data_kernel / (p_x[:, None] + 1e-12) / (p_y[None, :] + 1e-12)\n",
    "D_y = np.sum(data_kernel_norm, axis=0)\n",
    "rw_kernel = 0.5 * (data_kernel_norm / (D_y + 1e-12) + (data_kernel_norm / (D_y + 1e-12)).T)\n",
    "\n",
    "# SVD of rw_kernel and DM spectral auxiliaries\n",
    "phi_dm, s_dm, _ = svd(rw_kernel)\n",
    "lambda_ns = s_dm\n",
    "lambda_ = 1.0 - lambda_ns\n",
    "inv_lambda = np.zeros_like(lambda_)\n",
    "if inv_lambda.shape[0] > 1:\n",
    "    inv_lambda[1:] = 1.0 / (lambda_[1:] + 1e-18)\n",
    "inv_lambda = inv_lambda * epsilon\n",
    "\n",
    "# Regularization and tolerance for DM spectrum\n",
    "tol = 1e-6\n",
    "lambda_ns_mod = np.copy(lambda_ns)\n",
    "lambda_ns_mod[lambda_ns_mod < tol] = 0\n",
    "below_tol = int(np.sum(lambda_ns < tol))\n",
    "above_tol_dm = int(n - below_tol)\n",
    "reg = 1e-3\n",
    "lambda_ns_inv = np.zeros_like(lambda_ns)\n",
    "mask = lambda_ns >= tol\n",
    "lambda_ns_inv[mask] = epsilon / (lambda_ns[mask] + reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd82ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDMD with Brownian time pairs matched to kernel epsilon (thin shell)\n",
    "\n",
    "# === Time-pair generation on a thin shell (EDMD) ===\n",
    "# Mapping between kernel bandwidth and heat-kernel time: for exp(-||x-y||^2/(2*epsilon)),\n",
    "# the corresponding heat time is t = epsilon / (2D). Here we take D = 1 => Delta_t = epsilon/2,\n",
    "# and choose sigma ~ sqrt(epsilon) to generate a small Brownian step.\n",
    "D_edmd = 1.0\n",
    "Delta_t = epsilon / (2.0 * D_edmd)\n",
    "sigma = np.sqrt(epsilon)\n",
    "# Optional: small dimensionality correction so the tangent noise variance matches ambient (can be commented out)\n",
    "if d > 1:\n",
    "    sigma = sigma * np.sqrt(d / (d - 1))\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "Xi = rng.normal(0.0, 1.0, size=X_tar.shape)\n",
    "# Project noise to the tangent plane at each X_tar so the perturbation stays on the thin shell\n",
    "proj = np.sum(Xi * X_tar, axis=1, keepdims=True)\n",
    "Xi_tan = Xi - proj * X_tar\n",
    "Y_pairs = X_tar + sigma * Xi_tan\n",
    "# Renormalize back to the original radius (thin shell)\n",
    "radius = np.linalg.norm(X_tar, axis=1, keepdims=True) + 1e-12\n",
    "Y_pairs = Y_pairs / (np.linalg.norm(Y_pairs, axis=1, keepdims=True) + 1e-12) * radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72a3617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Dictionary definition and evaluations (anchors = X_tar) ===\n",
    "# Source side: use nearest anchor to map each X_tar[k] to an index (approximates one-hot dictionary evaluation Psi(X)).\n",
    "# Target side: use Gaussian responsibilities W(k,j) to softly assign Y_pairs[k] to anchors (soft dictionary evaluation Psi(Y)).\n",
    "try:\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    nn = NearestNeighbors(n_neighbors=1).fit(X_tar)\n",
    "    src_idx = nn.kneighbors(X_tar, return_distance=False).ravel()\n",
    "except Exception:\n",
    "    # Fallback: identity mapping\n",
    "    src_idx = np.arange(n)\n",
    "\n",
    "sqA = np.sum(X_tar**2, axis=1)                 # (n,)\n",
    "sqY = np.sum(Y_pairs**2, axis=1)               # (n,)\n",
    "# Responsibilities from Y to anchors (row-normalized to get a soft assignment)\n",
    "W = np.exp(-(sqY[:, None] + sqA[None, :] - 2.0 * (Y_pairs @ X_tar.T)) / (2.0 * epsilon))\n",
    "W = W / (np.sum(W, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "# === Build sample-space transition K_data, symmetrize, and eigendecompose ===\n",
    "# Accumulate transitions for each source anchor i from the responsibilities of corresponding samples k, then row-normalize\n",
    "# to obtain a Markov matrix in the anchor (dictionary) basis.\n",
    "K_count = np.zeros((n, n))\n",
    "for k in range(n):\n",
    "    i = src_idx[k]\n",
    "    K_count[i, :] += W[k, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ef2263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row-normalize to get a Markov matrix\n",
    "row_sums = np.sum(K_count, axis=1, keepdims=True) + 1e-12\n",
    "K_data = K_count / row_sums\n",
    "# Symmetrize to align with the symmetric spectral pipeline, then perform eigen-decomposition (sorted descending)\n",
    "K_sym = 0.5 * (K_data + K_data.T)\n",
    "# Numerical clipping to [0,1] to avoid tiny negatives causing eig issues\n",
    "K_sym = np.clip(K_sym, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e3f0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigen-decomposition (symmetric)\n",
    "s_edmd, phi_edmd = np.linalg.eigh(K_sym)  # s_edmd ascending\n",
    "# Sort descending to match prior convention (largest first)\n",
    "order = np.argsort(-s_edmd)\n",
    "s_edmd = s_edmd[order]\n",
    "phi_edmd = phi_edmd[:, order]\n",
    "\n",
    "# === Continuous-time generator spectrum (Koopman generator) and Green-like weights ===\n",
    "# Continuous-time eigenvalues: mu = log(s) / Delta_t. Then build inverse (with sign-regularization and capping)\n",
    "# as Green-like weights for the iteration.\n",
    "s_safe = np.clip(s_edmd, 1e-12, 1.0)\n",
    "mu_edmd = np.log(s_safe) / Delta_t\n",
    "\n",
    "# Define EDMD-based inverse spectral weights (Green-like), skipping near-zero modes\n",
    "reg_mu = 1e-3\n",
    "above_tol_mask = np.abs(mu_edmd) > 1e-10\n",
    "above_tol_edmd = int(np.sum(above_tol_mask))\n",
    "inv_mu = np.zeros_like(mu_edmd)\n",
    "inv_mu[above_tol_mask] = 1.0 / (mu_edmd[above_tol_mask] + reg_mu * np.sign(mu_edmd[above_tol_mask]))\n",
    "\n",
    "# EDMD spectral weights (Green-like) for optional use in iteration\n",
    "cap = 1e3\n",
    "lambda_ns_s_ns_edmd = np.minimum(np.abs(inv_mu[:above_tol_edmd]), cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabc7e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the translated kernel and gradient functions\n",
    "from grad_ker1 import grad_ker1\n",
    "from K_tar_eval import K_tar_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd87b11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterative scheme parameters\n",
    "iter = 1000\n",
    "h = 5\n",
    "m = 700\n",
    "\n",
    "# Initialize particles\n",
    "u = np.random.normal(0, 1, (m, d))\n",
    "u_norm = np.linalg.norm(u, axis=1, keepdims=True)\n",
    "r = np.sqrt(np.random.rand(m, 1)) * 1/100 + 99/100\n",
    "u_trans = u / u_norm\n",
    "x_init = r * u_trans\n",
    "x_init = x_init[x_init[:, 1] > 0.7, :]\n",
    "m = x_init.shape[0]\n",
    "x_t = np.zeros((m, d, iter))\n",
    "x_t[:, :, 0] = x_init\n",
    "\n",
    "# Precompute quantities\n",
    "p_tar = np.sum(data_kernel, axis=0)\n",
    "D = np.sum(data_kernel / np.sqrt(p_tar) / (np.sqrt(p_tar)[:, None] + 1e-12), axis=1)\n",
    "\n",
    "# Mixed spectral setup: build DM weights once\n",
    "# inv_K_ns_s_ns = phi_dm @ np.diag(lambda_ns_inv * inv_lambda * lambda_ns_inv) @ phi_dm.T\n",
    "# (We keep only the vectors used downstream; remove unused full-operator reconstruction)\n",
    "lambda_s_s_ns = inv_lambda * inv_lambda * lambda_ns_inv\n",
    "lambda_s_s_ns = lambda_s_s_ns[:above_tol_dm]\n",
    "lambda_ns_s_ns_dm = lambda_ns_inv * inv_lambda * lambda_ns_inv\n",
    "lambda_ns_s_ns_dm = lambda_ns_s_ns_dm[:above_tol_dm]\n",
    "\n",
    "# Toggle: choose basis and weights\n",
    "USE_EDMD_BASIS = True   # True: use EDMD eigenvectors; False: use DM eigenvectors\n",
    "USE_EDMD_WEIGHTS = True # True: use EDMD Green-like weights; False: use DM weights\n",
    "\n",
    "phi_proj = phi_edmd if (USE_EDMD_BASIS and 'phi_edmd' in globals()) else phi_dm\n",
    "if USE_EDMD_WEIGHTS and 'lambda_ns_s_ns_edmd' in globals():\n",
    "    lambda_ns_s_ns = lambda_ns_s_ns_edmd\n",
    "    above_tol = above_tol_edmd\n",
    "else:\n",
    "    lambda_ns_s_ns = lambda_ns_s_ns_dm\n",
    "    above_tol = above_tol_dm  # projector uses first above_tol modes consistent with chosen weights\n",
    "\n",
    "sum_x = np.zeros((m, d))\n",
    "for t in range(iter - 1):\n",
    "    grad_matrix = grad_ker1(x_t[:, :, t], X_tar, p_tar, sq_tar, D, epsilon)\n",
    "    cross_matrix = K_tar_eval(X_tar, x_t[:, :, t], p_tar, sq_tar, D, epsilon)\n",
    "    for i in range(d):\n",
    "        sum_x[:, i] = np.sum(grad_matrix[:, :, i] @ phi_proj[:, :above_tol] @ np.diag(lambda_ns_s_ns) @ phi_proj[:, :above_tol].T @ cross_matrix, axis=1)\n",
    "    x_t[:, :, t + 1] = x_t[:, :, t] - h / m * sum_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f4e66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation of script_nd_sphere.m lines 144-168 to Python\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting results\n",
    "if d == 2:\n",
    "    plt.figure()\n",
    "    plt.plot(X_tar[:, 0], X_tar[:, 1], 'o', label='Target')\n",
    "    plt.plot(x_t[:, 0, 0], x_t[:, 1, 0], 'o', label='Init')\n",
    "    plt.plot(x_t[:, 0, -1], x_t[:, 1, -1], 'o', label='Final')\n",
    "    plt.legend()\n",
    "    plt.title('2D Results')\n",
    "    plt.show()\n",
    "else:\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(X_tar[:, 0], X_tar[:, 1], X_tar[:, 2], label='Target')\n",
    "    ax.scatter(x_t[:, 0, 0], x_t[:, 1, 0], x_t[:, 2, 0], label='Init')\n",
    "    ax.scatter(x_t[:, 0, -1], x_t[:, 1, -1], x_t[:, 2, -1], label='Final')\n",
    "    ax.legend()\n",
    "    plt.title('3D Results')\n",
    "    plt.show()\n",
    "    # Additional plot for final state only\n",
    "    fig2 = plt.figure()\n",
    "    ax2 = fig2.add_subplot(111, projection='3d')\n",
    "    ax2.scatter(x_t[:, 0, 0], x_t[:, 1, 0], x_t[:, 2, 0], label='Init')\n",
    "    ax2.scatter(x_t[:, 0, -1], x_t[:, 1, -1], x_t[:, 2, -1], label='Final')\n",
    "    ax2.legend()\n",
    "    plt.title('3D Final State')\n",
    "    plt.show()\n",
    "\n",
    "pd.plotting.scatter_matrix(\n",
    "    pd.DataFrame(X_tar),\n",
    "    alpha=0.2,\n",
    "    figsize=(6, 6),\n",
    "    diagonal='hist',\n",
    "    hist_kwds={'edgecolor': 'black'}\n",
    ")\n",
    "plt.suptitle('Scatter Matrix of X_tar')\n",
    "plt.show()\n",
    "\n",
    "pd.plotting.scatter_matrix(\n",
    "    pd.DataFrame(x_t[:, :, -1]),\n",
    "    alpha=0.2,\n",
    "    figsize=(6, 6),\n",
    "    diagonal='hist',\n",
    "    hist_kwds={'edgecolor': 'black'}\n",
    ")\n",
    "plt.suptitle('Scatter Matrix of x_t (final)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa8ea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check: print epsilon and Delta_t used in EDMD\n",
    "print('EDMD epsilon =', float(epsilon))\n",
    "print('EDMD Delta_t =', float(Delta_t))\n",
    "print('epsilon / (2*Delta_t) =', float(epsilon/(2.0*Delta_t)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
