{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b2ad603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         filename  latent_dim  num_epochs  max_dm_samples  batch_size  dm_m  spread AE_Type\n",
      "0  test_3_MNIST_edmd_dmps_0.ipynb           6          60           20000         128    15     1.0     CNN\n",
      "1  test_3_MNIST_edmd_dmps_1.ipynb           6         100            5000         128    15     1.0     CNN\n",
      "2  test_3_MNIST_edmd_dmps_2.ipynb           8         100            5000         128    15     1.0     CNN\n",
      "3  test_3_MNIST_edmd_dmps_3.ipynb           8         100            5000         128    15     1.0     CNN\n",
      "4  test_3_MNIST_edmd_dmps_4.ipynb           6         100           20000         128    15     1.0     CNN\n",
      "5  test_3_MNIST_edmd_dmps_5.ipynb           6          60           20000         128    15     1.0     CNN\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Get files\n",
    "files = glob.glob(\"test_3_MNIST_edmd_dmps_*.ipynb\")\n",
    "try:\n",
    "    files = sorted(files, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "data = []\n",
    "\n",
    "for fpath in files:\n",
    "    fname = fpath.split('\\\\')[-1]\n",
    "    \n",
    "    latent_dim = None\n",
    "    num_epochs = None\n",
    "    max_dm_samples = None\n",
    "    autoencoder_type = None\n",
    "    batch_size = None\n",
    "    dm_m = None\n",
    "    spread_factor = None\n",
    "    \n",
    "    try:\n",
    "        with open(fpath, 'r', encoding='utf-8-sig') as f:\n",
    "            nb = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {fpath}: {e}\")\n",
    "        continue\n",
    "\n",
    "    full_text = \"\"\n",
    "    for cell in nb['cells']:\n",
    "        if cell['cell_type'] == 'code':\n",
    "            source = \"\".join(cell['source'])\n",
    "            full_text += source + \"\\n\"\n",
    "    \n",
    "    # Extract latent_dim\n",
    "    m = re.search(r'^latent_dim\\s*=\\s*(\\d+)', full_text, re.MULTILINE)\n",
    "    if m: latent_dim = int(m.group(1))\n",
    "\n",
    "    # Extract num_epochs\n",
    "    m = re.search(r'^num_epochs\\s*=\\s*(\\d+)', full_text, re.MULTILINE)\n",
    "    if m: num_epochs = int(m.group(1))\n",
    "    \n",
    "    # Extract max_dm_samples\n",
    "    m = re.search(r'^max_dm_samples\\s*=\\s*(\\d+)', full_text, re.MULTILINE)\n",
    "    if m: max_dm_samples = int(m.group(1))\n",
    "    \n",
    "    # Extract batch_size\n",
    "    m = re.search(r'^batch_size\\s*=\\s*(\\d+)', full_text, re.MULTILINE)\n",
    "    if m: batch_size = int(m.group(1))\n",
    "    \n",
    "    # Extract m (DM eigenvectors)\n",
    "    m_dm = re.search(r'^m\\s*=\\s*(\\d+)', full_text, re.MULTILINE)\n",
    "    if m_dm: dm_m = int(m_dm.group(1))\n",
    "    \n",
    "    # Extract SPREAD_FACTOR\n",
    "    m_sf = re.search(r'^SPREAD_FACTOR\\s*=\\s*([\\d\\.]+)', full_text, re.MULTILINE)\n",
    "    if m_sf: spread_factor = float(m_sf.group(1))\n",
    "\n",
    "    # Extract autoencoder type\n",
    "    # Check what is uncommented\n",
    "    if re.search(r'^autoencoder\\s*=\\s*CNNAutoencoder', full_text, re.MULTILINE):\n",
    "        autoencoder_type = \"CNN\"\n",
    "    elif re.search(r'^autoencoder\\s*=\\s*MLPAutoencoder', full_text, re.MULTILINE):\n",
    "        autoencoder_type = \"MLP\"\n",
    "        \n",
    "    entry = {\n",
    "        \"filename\": fname,\n",
    "        \"latent_dim\": latent_dim,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"max_dm_samples\": max_dm_samples,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"dm_m\": dm_m,\n",
    "        \"spread\": spread_factor,\n",
    "        \"AE_Type\": autoencoder_type\n",
    "    }\n",
    "    data.append(entry)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2d1f19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           filename  latent_dim  epochs  dm_samples  particles  step  n_dict  seed\n",
      "0    test_3_MNIST_edmd_dmps_0.ipynb           6      60       20000       64.0   0.1     100     1\n",
      "1    test_3_MNIST_edmd_dmps_1.ipynb           6     100        5000       64.0   0.2     200     1\n",
      "2    test_3_MNIST_edmd_dmps_2.ipynb           8     100        5000       64.0   0.2     200     1\n",
      "3    test_3_MNIST_edmd_dmps_3.ipynb           8      60        5000        NaN   NaN     300     1\n",
      "4    test_3_MNIST_edmd_dmps_4.ipynb           8     100        5000       64.0   0.2     200     1\n",
      "5    test_3_MNIST_edmd_dmps_5.ipynb           8      60        5000        NaN   NaN     300     1\n",
      "6    test_3_MNIST_edmd_dmps_6.ipynb           8     100        5000       64.0   0.2     200     1\n",
      "7    test_3_MNIST_edmd_dmps_7.ipynb           8     100        5000       64.0   0.2     200     1\n",
      "8    test_3_MNIST_edmd_dmps_8.ipynb           6     100       20000       64.0   0.2     200     1\n",
      "9    test_3_MNIST_edmd_dmps_9.ipynb           8     100        5000      100.0   0.1     200     1\n",
      "10  test_3_MNIST_edmd_dmps_10.ipynb           6      60       20000       64.0   0.1     100     1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "files = glob.glob(\"test_3_MNIST_edmd_dmps_*.ipynb\")\n",
    "try:\n",
    "    files = sorted(files, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "data = []\n",
    "\n",
    "for fpath in files:\n",
    "    fname = fpath.split('\\\\')[-1]\n",
    "    \n",
    "    latent_dim = None\n",
    "    num_epochs = None\n",
    "    max_dm_samples = None\n",
    "    autoencoder_type = None\n",
    "    batch_size = None\n",
    "    m_particles = None\n",
    "    step_size = None\n",
    "    n_dict = None\n",
    "    seed = None\n",
    "    \n",
    "    try:\n",
    "        with open(fpath, 'r', encoding='utf-8-sig') as f:\n",
    "            nb = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {fpath}: {e}\")\n",
    "        continue\n",
    "\n",
    "    full_text = \"\"\n",
    "    for cell in nb['cells']:\n",
    "        if cell['cell_type'] == 'code':\n",
    "            source = \"\".join(cell['source'])\n",
    "            full_text += source + \"\\n\"\n",
    "    \n",
    "    # Extract params\n",
    "    m = re.search(r'^latent_dim\\s*=\\s*(\\d+)', full_text, re.MULTILINE)\n",
    "    if m: latent_dim = int(m.group(1))\n",
    "\n",
    "    m = re.search(r'^num_epochs\\s*=\\s*(\\d+)', full_text, re.MULTILINE)\n",
    "    if m: num_epochs = int(m.group(1))\n",
    "    \n",
    "    m = re.search(r'^max_dm_samples\\s*=\\s*(\\d+)', full_text, re.MULTILINE)\n",
    "    if m: max_dm_samples = int(m.group(1))\n",
    "    \n",
    "    m = re.search(r'^batch_size\\s*=\\s*(\\d+)', full_text, re.MULTILINE)\n",
    "    if m: batch_size = int(m.group(1))\n",
    "\n",
    "    m = re.search(r'^m_particles\\s*=\\s*(\\d+)', full_text, re.MULTILINE)\n",
    "    if m: m_particles = int(m.group(1))\n",
    "\n",
    "    m = re.search(r'^step_size\\s*=\\s*([\\d\\.]+)', full_text, re.MULTILINE)\n",
    "    if m: step_size = float(m.group(1))\n",
    "\n",
    "    m = re.search(r'^n_dict_components\\s*=\\s*(\\d+)', full_text, re.MULTILINE)\n",
    "    if m: n_dict = int(m.group(1))\n",
    "    \n",
    "    # Random see\n",
    "    m = re.search(r'np\\.random\\.seed\\((\\d+)\\)', full_text)\n",
    "    if m: seed = int(m.group(1))\n",
    "\n",
    "    if re.search(r'^autoencoder\\s*=\\s*CNNAutoencoder', full_text, re.MULTILINE):\n",
    "        autoencoder_type = \"CNN\"\n",
    "    elif re.search(r'^autoencoder\\s*=\\s*MLPAutoencoder', full_text, re.MULTILINE):\n",
    "        autoencoder_type = \"MLP\"\n",
    "        \n",
    "    entry = {\n",
    "        \"filename\": fname,\n",
    "        \"latent_dim\": latent_dim,\n",
    "        \"epochs\": num_epochs,\n",
    "        \"dm_samples\": max_dm_samples,\n",
    "        \"particles\": m_particles,\n",
    "        \"step\": step_size,\n",
    "        \"n_dict\": n_dict,\n",
    "        \"seed\": seed\n",
    "    }\n",
    "    data.append(entry)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df.to_string())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
