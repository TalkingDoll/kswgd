{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2025f0dc",
   "metadata": {},
   "source": [
    "# MNIST Latent Transport: Kernel EDMD + LAWGD\n",
    "\n",
    "## Objective\n",
    "1. Load high-dimensional MNIST images and map them into a low-dimensional latent space\n",
    "2. Use PCA/SVD-style dimensionality reduction to obtain latent coordinates\n",
    "3. Define a latent potential in the reduced space and learn the Langevin generator via Kernel EDMD / SDMD\n",
    "4. Apply LAWGD to transport off-manifold particles so that they match the latent MNIST distribution\n",
    "\n",
    "## Motivation\n",
    "- Directly running LAWGD on 28×28 images is infeasible, so we compress the data\n",
    "- The latent space keeps the downstream DMPS/SDMD workflows unchanged (still 2D)\n",
    "- Decoder (inverse PCA) lets us lift transported particles back to pixel space for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a2d211a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.linalg import svd, eig\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torchvision import datasets\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bc8d09",
   "metadata": {},
   "source": [
    "## MNIST Latent Representation Pipeline\n",
    "\n",
    "We treat MNIST images as points in \\(\\mathbb{R}^{784}\\) and learn a low-dimensional chart:\n",
    "- Normalize pixels to \\([0, 1]\\)\n",
    "- Standardize flattened vectors, then run PCA (2D latent space by default)\n",
    "- Keep the encoder/decoder so that we can move between latent and pixel spaces\n",
    "- Define a quadratic latent potential using the empirical covariance (acts like an isotropic energy landscape around the data manifold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e24ed974",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTLatentPipeline:\n",
    "    \"\"\"Build a PCA-based encoder/decoder for MNIST images.\"\"\"\n",
    "\n",
    "    def __init__(self, data_root: str = \"data/MNIST\", latent_dim: int = 2):\n",
    "        self.data_root = Path(data_root)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.scaler: StandardScaler | None = None\n",
    "        self.pca: PCA | None = None\n",
    "\n",
    "    def load_data(self, n_samples: int | None = 60000, train: bool = True):\n",
    "        dataset = datasets.MNIST(root=str(self.data_root), train=train, download=True)\n",
    "        images = dataset.data.numpy().astype(np.float32) / 255.0\n",
    "        labels = dataset.targets.numpy()\n",
    "\n",
    "        if n_samples is not None and n_samples < len(images):\n",
    "            idx = np.random.choice(len(images), size=n_samples, replace=False)\n",
    "            images = images[idx]\n",
    "            labels = labels[idx]\n",
    "        return images, labels\n",
    "\n",
    "    def fit(self, images: np.ndarray):\n",
    "        flat = images.reshape(images.shape[0], -1)\n",
    "        self.scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "        scaled = self.scaler.fit_transform(flat)\n",
    "        self.pca = PCA(n_components=self.latent_dim, random_state=42)\n",
    "        latent = self.pca.fit_transform(scaled)\n",
    "        return latent\n",
    "\n",
    "    def transform(self, images: np.ndarray):\n",
    "        if self.scaler is None or self.pca is None:\n",
    "            raise RuntimeError(\"Call fit() before transform().\")\n",
    "        flat = images.reshape(images.shape[0], -1)\n",
    "        scaled = self.scaler.transform(flat)\n",
    "        return self.pca.transform(scaled)\n",
    "\n",
    "    def inverse_transform(self, latent: np.ndarray):\n",
    "        if self.scaler is None or self.pca is None:\n",
    "            raise RuntimeError(\"Call fit() before inverse_transform().\")\n",
    "        scaled = self.pca.inverse_transform(latent)\n",
    "        flat = self.scaler.inverse_transform(scaled)\n",
    "        return np.clip(flat.reshape(-1, 28, 28), 0.0, 1.0)\n",
    "\n",
    "\n",
    "class LatentQuadraticPotential:\n",
    "    \"\"\"Mahalanobis energy in latent space (acts like a Gaussian well).\"\"\"\n",
    "\n",
    "    def __init__(self, covariance: np.ndarray, temperature: float = 1.0):\n",
    "        reg = 1e-6 * np.eye(covariance.shape[0])\n",
    "        self.precision = np.linalg.inv(covariance + reg)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def _ensure_2d(self, X: np.ndarray):\n",
    "        return X if X.ndim == 2 else X.reshape(1, -1)\n",
    "\n",
    "    def V(self, X: np.ndarray):\n",
    "        X2d = self._ensure_2d(X)\n",
    "        energy = 0.5 * np.sum((X2d @ self.precision) * X2d, axis=1)\n",
    "        return energy / self.temperature\n",
    "\n",
    "    def grad_V(self, X: np.ndarray):\n",
    "        X2d = self._ensure_2d(X)\n",
    "        grad = (X2d @ self.precision) / self.temperature\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51723dc",
   "metadata": {},
   "source": [
    "## Fit PCA Encoder and Inspect Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fee4f91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MNIST latent statistics:\n",
      "============================================================\n",
      "Samples used: 50000\n",
      "Latent dimension: 32\n",
      "Explained variance ratio: [0.05705443 0.04146104 0.03798272 0.02925506 0.0254175  0.02220319\n",
      " 0.01945319 0.01762282 0.01556257 0.01419107 0.01361732 0.01218615\n",
      " 0.01130097 0.01106123 0.01047601 0.01010681 0.00945249 0.00934628\n",
      " 0.00911511 0.00885225 0.00839101 0.0081518  0.00775032 0.00754129\n",
      " 0.00728822 0.00703061 0.00698728 0.00675782 0.00640675 0.0062678\n",
      " 0.00606485 0.00596459]\n",
      "Latent mean (first 2 dims): [ 1.9847631e-07  1.0109925e-06  8.4353445e-07  4.4567108e-07\n",
      " -4.7385456e-07  1.4875793e-06  2.1265578e-06 -5.2680969e-07\n",
      " -6.1259271e-08 -4.9475074e-07  4.9034117e-07  1.6213376e-06\n",
      " -8.1505777e-07 -4.2264460e-07  2.1303331e-06  3.2160281e-07\n",
      " -1.5620851e-06 -6.6792489e-07  8.3309891e-07 -1.3632250e-06\n",
      "  9.8544126e-07 -2.9323459e-07  2.2020226e-06 -3.1128405e-07\n",
      "  1.4401471e-06  2.0357525e-06  6.3280106e-07  7.0935249e-07\n",
      "  8.0370188e-07  4.5450329e-07  5.0237475e-07 -6.0159681e-07]\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Build density grid for visualization\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m x_min, y_min \u001b[38;5;241m=\u001b[39m latent_embeddings\u001b[38;5;241m.\u001b[39mmin(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m     21\u001b[0m x_max, y_max \u001b[38;5;241m=\u001b[39m latent_embeddings\u001b[38;5;241m.\u001b[39mmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m     22\u001b[0m latent_bounds \u001b[38;5;241m=\u001b[39m ((x_min, x_max), (y_min, y_max))\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Build latent representation\n",
    "latent_dim = 32\n",
    "mnist_pipeline = MNISTLatentPipeline(data_root=\"data/MNIST\", latent_dim=latent_dim)\n",
    "mnist_images, mnist_labels = mnist_pipeline.load_data(n_samples=50000, train=True)\n",
    "latent_embeddings = mnist_pipeline.fit(mnist_images)\n",
    "\n",
    "latent_cov = np.cov(latent_embeddings, rowvar=False)\n",
    "potential = LatentQuadraticPotential(covariance=latent_cov, temperature=1.0)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MNIST latent statistics:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Samples used: {latent_embeddings.shape[0]}\")\n",
    "print(f\"Latent dimension: {latent_dim}\")\n",
    "print(f\"Explained variance ratio: {mnist_pipeline.pca.explained_variance_ratio_}\")\n",
    "print(f\"Latent mean (first 2 dims): {latent_embeddings.mean(axis=0)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Build density grid for visualization\n",
    "x_min, y_min = latent_embeddings.min(axis=0) - 1.0\n",
    "x_max, y_max = latent_embeddings.max(axis=0) + 1.0\n",
    "latent_bounds = ((x_min, x_max), (y_min, y_max))\n",
    "x_range = np.linspace(x_min, x_max, 200)\n",
    "y_range = np.linspace(y_min, y_max, 200)\n",
    "X_grid, Y_grid = np.meshgrid(x_range, y_range)\n",
    "\n",
    "kde = gaussian_kde(latent_embeddings.T)\n",
    "density_grid = kde(np.vstack([X_grid.ravel(), Y_grid.ravel()])).reshape(X_grid.shape)\n",
    "\n",
    "fig_scatter, ax = plt.subplots(figsize=(7, 6))\n",
    "ax.contourf(X_grid, Y_grid, density_grid, levels=20, cmap=\"viridis\", alpha=0.7)\n",
    "scatter = ax.scatter(\n",
    "    latent_embeddings[:, 0], latent_embeddings[:, 1],\n",
    "    c=mnist_labels[:latent_embeddings.shape[0]], cmap=\"tab20\", s=5, alpha=0.4,\n",
    ")\n",
    "ax.set_xlabel(\"Latent dim 1\")\n",
    "ax.set_ylabel(\"Latent dim 2\")\n",
    "ax.set_title(\"MNIST latent distribution (PCA)\")\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Reconstruct a few digits to verify the lift\n",
    "recon_samples = mnist_pipeline.inverse_transform(latent_embeddings[:16])\n",
    "fig_recon, axes = plt.subplots(4, 4, figsize=(6, 6))\n",
    "for idx, ax in enumerate(axes.ravel()):\n",
    "    if idx < recon_samples.shape[0]:\n",
    "        ax.imshow(recon_samples[idx], cmap=\"gray\")\n",
    "    ax.axis(\"off\")\n",
    "fig_recon.suptitle(\"Reconstructed digits via inverse PCA\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Decoder sanity check complete: reconstructed samples visualized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b664331",
   "metadata": {},
   "source": [
    "## Validate Latent ↔ Pixel Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe328e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify reconstruction error statistics\n",
    "n_eval = 2048\n",
    "idx_eval = np.random.choice(mnist_images.shape[0], size=n_eval, replace=False)\n",
    "images_eval = mnist_images[idx_eval]\n",
    "latent_eval = mnist_pipeline.transform(images_eval)\n",
    "recon_eval = mnist_pipeline.inverse_transform(latent_eval)\n",
    "\n",
    "mse = np.mean((images_eval - recon_eval) ** 2)\n",
    "max_err = np.max(np.abs(images_eval - recon_eval))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Latent reconstruction diagnostics\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Evaluation samples: {n_eval}\")\n",
    "print(f\"Mean squared error: {mse:.6f}\")\n",
    "print(f\"Max abs error: {max_err:.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig_compare, axes = plt.subplots(4, 4, figsize=(6, 6))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    original = images_eval[i]\n",
    "    recon = recon_eval[i]\n",
    "    ax.imshow(np.hstack([original, recon]), cmap=\"gray\")\n",
    "    ax.axis(\"off\")\n",
    "fig_compare.suptitle(\"Left: original | Right: reconstruction\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3cded4",
   "metadata": {},
   "source": [
    "## Generate Training Data: (X_tar, X_tar_next)\n",
    "\n",
    "Sample latent codes directly from MNIST, then evolve them with Langevin dynamics in the latent space:\n",
    "$$dZ = -\\nabla V(Z)\\,dt + \\sqrt{2}\\,dW$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e18366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 1: Sample X_tar from latent MNIST distribution\n",
    "# ============================================================\n",
    "\n",
    "def sample_latent(latent_data, labels, n_samples=2500):\n",
    "    idx = np.random.choice(latent_data.shape[0], size=n_samples, replace=False)\n",
    "    return latent_data[idx], labels[idx], idx\n",
    "\n",
    "n_samples = 2500\n",
    "print(\"=\" * 60)\n",
    "print(\"Sampling X_tar from MNIST latent embeddings ...\")\n",
    "print(\"=\" * 60)\n",
    "X_tar, X_tar_labels, X_tar_idx = sample_latent(latent_embeddings, mnist_labels, n_samples)\n",
    "print(f\"X_tar shape: {X_tar.shape}\")\n",
    "print(f\"Latent ranges: dim1 ∈ [{X_tar[:, 0].min():.2f}, {X_tar[:, 0].max():.2f}], dim2 ∈ [{X_tar[:, 1].min():.2f}, {X_tar[:, 1].max():.2f}]\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define latent anchors (previously 'wells') using k-means clusters\n",
    "n_latent_regions = 4\n",
    "kmeans = KMeans(n_clusters=n_latent_regions, random_state=42, n_init=10)\n",
    "kmeans.fit(X_tar)\n",
    "well_centers = kmeans.cluster_centers_\n",
    "center_dists = np.linalg.norm(X_tar[:, None, :] - well_centers[None, :, :], axis=2)\n",
    "min_center_dist = center_dists.min(axis=1)\n",
    "well_radius = np.percentile(min_center_dist, 40)\n",
    "\n",
    "print(\"Latent anchors (derived via KMeans):\")\n",
    "for i, center in enumerate(well_centers):\n",
    "    print(f\"  Anchor {i+1}: ({center[0]:+.3f}, {center[1]:+.3f})\")\n",
    "print(f\"Suggested anchor radius: {well_radius:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ab92e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 2: Evolve X_tar → X_tar_next using latent Langevin dynamics\n",
    "# ============================================================\n",
    "\n",
    "def langevin_step(Z, potential_model, dt, n_substeps=500):\n",
    "    \"\"\"Euler-Maruyama integration in latent space.\"\"\"\n",
    "    n = Z.shape[0]\n",
    "    dt_sub = dt / n_substeps\n",
    "    Z_curr = Z.copy()\n",
    "    for _ in range(n_substeps):\n",
    "        grad_V = potential_model.grad_V(Z_curr)\n",
    "        drift = -grad_V\n",
    "        noise = np.sqrt(2.0 * dt_sub) * np.random.randn(n, Z_curr.shape[1])\n",
    "        Z_curr = Z_curr + drift * dt_sub + noise\n",
    "    return Z_curr\n",
    "\n",
    "# Evolution parameters\n",
    "dt = 0.1\n",
    "n_substeps = 500\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Evolving X_tar → X_tar_next via latent Langevin dynamics ...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"SDE: dZ = -∇V(Z)dt + √(2)dW (latent space)\")\n",
    "print(f\"Total time step: dt = {dt}\")\n",
    "print(f\"Number of sub-steps: {n_substeps}\")\n",
    "print(f\"Number of particles: {X_tar.shape[0]}\")\n",
    "\n",
    "X_tar_next = langevin_step(X_tar, potential, dt, n_substeps=n_substeps)\n",
    "\n",
    "print(f\"X_tar_next shape: {X_tar_next.shape}\")\n",
    "print(f\"Latent ranges: dim1 ∈ [{X_tar_next[:, 0].min():.2f}, {X_tar_next[:, 0].max():.2f}], dim2 ∈ [{X_tar_next[:, 1].min():.2f}, {X_tar_next[:, 1].max():.2f}]\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff90dd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualize latent samples and their evolved counterparts\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for ax, data, title, color in [\n",
    "    (axes[0], X_tar, \"X_tar (sampled latent codes)\", \"tab:blue\"),\n",
    "    (axes[1], X_tar_next, f\"X_tar_next (after dt={dt})\", \"tab:orange\"),\n",
    "]:\n",
    "    ax.contourf(X_grid, Y_grid, density_grid, levels=20, cmap=\"viridis\", alpha=0.5)\n",
    "    ax.scatter(data[:, 0], data[:, 1], c=color, s=15, alpha=0.6, edgecolors=\"k\", linewidth=0.2)\n",
    "    ax.scatter(well_centers[:, 0], well_centers[:, 1], color=\"red\", s=100, marker=\"*\", label=\"Latent anchors\")\n",
    "    ax.set_xlabel(\"Latent dim 1\")\n",
    "    ax.set_ylabel(\"Latent dim 2\")\n",
    "    ax.set_title(title, fontsize=14, fontweight=\"bold\")\n",
    "    ax.set_xlim(latent_bounds[0])\n",
    "    ax.set_ylim(latent_bounds[1])\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Data generation complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training data pairs: {X_tar.shape[0]}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d594149",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8234ce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================================\n",
    "# # Polynomial EDMD: Extended Dynamic Mode Decomposition\n",
    "# # ============================================================\n",
    "\n",
    "# # Preconditions\n",
    "# if 'X_tar' not in globals() or 'X_tar_next' not in globals():\n",
    "#     raise RuntimeError('X_tar and X_tar_next must be computed before running EDMD (polynomial).')\n",
    "\n",
    "# n = X_tar.shape[0]\n",
    "\n",
    "# # Step 1: Build polynomial basis functions\n",
    "# degree = 4\n",
    "\n",
    "# def monomial_exponents_2d(deg: int):\n",
    "#     exps = []\n",
    "#     for total in range(deg + 1):\n",
    "#         for i in range(total + 1):\n",
    "#             exps.append((i, total - i))\n",
    "#     return exps\n",
    "\n",
    "# exps = monomial_exponents_2d(degree)\n",
    "# m_dict = len(exps)\n",
    "\n",
    "# # Optional feature scaling for numerical stability\n",
    "# Z_all = np.vstack([X_tar, X_tar_next])\n",
    "# scale = 1.0  # No scaling for this example\n",
    "# X0 = X_tar / scale\n",
    "# Y0 = X_tar_next / scale\n",
    "\n",
    "# # Feature map Φ(X)\n",
    "# def phi_poly(X: np.ndarray) -> np.ndarray:\n",
    "#     N = X.shape[0]\n",
    "#     Phi = np.empty((N, m_dict), dtype=float)\n",
    "#     x = X[:, 0]\n",
    "#     y = X[:, 1]\n",
    "#     for k, (i, j) in enumerate(exps):\n",
    "#         if i == 0 and j == 0:\n",
    "#             Phi[:, k] = 1.0\n",
    "#         elif i == 0:\n",
    "#             Phi[:, k] = y ** j\n",
    "#         elif j == 0:\n",
    "#             Phi[:, k] = x ** i\n",
    "#         else:\n",
    "#             Phi[:, k] = (x ** i) * (y ** j)\n",
    "#     return Phi\n",
    "\n",
    "# Phi = phi_poly(X0)\n",
    "# Phi_next = phi_poly(Y0)\n",
    "# N = Phi.shape[0]\n",
    "\n",
    "# # Step 2: Build EDMD matrices\n",
    "# G_edmd = (Phi.T @ Phi) / N\n",
    "# A_edmd = (Phi.T @ Phi_next) / N\n",
    "\n",
    "# # Step 3: Compute Koopman operator\n",
    "# reg = 1e-10\n",
    "# I = np.eye(G_edmd.shape[0])\n",
    "# K_edmd = np.linalg.solve(G_edmd + reg * I, A_edmd)\n",
    "\n",
    "# # Step 4: Compute eigenvalues\n",
    "# eigenvalues_edmd = np.linalg.eigvals(K_edmd)\n",
    "\n",
    "# # Step 5: Construct generator eigenvalues and inverse weights\n",
    "# # Extract real part of eigenvalues (ignore imaginary part)\n",
    "# lambda_ns_edmd = eigenvalues_edmd.real\n",
    "\n",
    "# # Construct generator eigenvalues: λ_gen = (λ_K - 1) / dt\n",
    "# lambda_gen_edmd = (lambda_ns_edmd - 1.0) / dt\n",
    "\n",
    "# # Build inverse generator weights (for LAWGD)\n",
    "# tol_edmd = 1e-6\n",
    "# lambda_ns_inv_edmd = np.zeros_like(lambda_ns_edmd)\n",
    "# mask_edmd = lambda_ns_edmd >= tol_edmd\n",
    "# lambda_ns_inv_edmd[mask_edmd] = 1.0 / (lambda_ns_edmd[mask_edmd] + 0.001)\n",
    "\n",
    "# # Store results for LAWGD\n",
    "# eigvals_K_edmd = lambda_ns_edmd.copy()\n",
    "# lambda_gen_full_edmd = lambda_gen_edmd.copy()\n",
    "\n",
    "# # ============================================================\n",
    "# # Visualization: Eigenvalues on Unit Circle\n",
    "# # ============================================================\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# # Plot unit circle\n",
    "# theta = np.linspace(0, 2*np.pi, 100)\n",
    "# ax.plot(np.cos(theta), np.sin(theta), 'k--', linewidth=1, label='Unit Circle')\n",
    "\n",
    "# # Plot eigenvalues\n",
    "# ax.scatter(eigenvalues_edmd.real, eigenvalues_edmd.imag, c='red', s=50, marker='o', \n",
    "#            edgecolors='black', linewidths=1, label='Eigenvalues', zorder=3)\n",
    "\n",
    "# # Set equal aspect ratio and labels\n",
    "# ax.set_aspect('equal')\n",
    "# ax.grid(True, alpha=0.3)\n",
    "# ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "# ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "# ax.set_xlabel('Real', fontsize=12)\n",
    "# ax.set_ylabel('Imaginary', fontsize=12)\n",
    "# ax.set_title('Polynomial EDMD: Koopman Eigenvalues on Unit Circle', fontsize=14)\n",
    "# ax.legend(fontsize=10)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# print(f\"Number of eigenvalues: {len(eigenvalues_edmd)}\")\n",
    "# print(f\"Max magnitude: {np.max(np.abs(eigenvalues_edmd)):.4f}\")\n",
    "# print(f\"Eigenvalues outside unit circle: {np.sum(np.abs(eigenvalues_edmd) > 1)}\")\n",
    "\n",
    "# # Sort eigenvalues by real part (descending order)\n",
    "# sorted_real_edmd = np.sort(eigenvalues_edmd.real)[::-1]\n",
    "\n",
    "# print(\"\\n\" + \"=\"*50)\n",
    "# print(\"Eigenvalues (Real part, sorted from large to small):\")\n",
    "# print(\"=\"*50)\n",
    "# for i, real_part in enumerate(sorted_real_edmd):\n",
    "#     print(f\"{i+1:3d}. {real_part:+.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64df2625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from deeptime.data import quadruple_well\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# h = 1e-3 # step size of the Euler-Maruyama integrator\n",
    "# n_steps = 10000 # number of steps, the lag time is thus tau = nSteps*h = 10\n",
    "# x0 = np.zeros((1, 2)) # inital condition\n",
    "# n = 10000 # number of evaluations of the  discretized dynamical system with lag time tau\n",
    "\n",
    "# f = quadruple_well(n_steps=n_steps)  # loading the model\n",
    "# traj = f.trajectory(x0, n, seed=42)\n",
    "\n",
    "# m = 2500 # number of training data points\n",
    "# X = np.random.uniform(-2, 2, size=(2500, 2)) # training data\n",
    "# # X = 4*np.random.rand(2, m)-2\n",
    "# Y = f(X, seed=42, n_jobs=1) # training data mapped forward by the dynamical system\n",
    "\n",
    "# from deeptime.kernels import GaussianKernel\n",
    "# from deeptime.decomposition import KernelEDMD\n",
    "\n",
    "# # ============================================================\n",
    "# # Kernel Definition (GaussianKernel)\n",
    "# # ============================================================\n",
    "# # Deeptime's GaussianKernel(sigma) defines:\n",
    "# #   k(x, y) = exp(-||x - y||² / (2 * sigma²))\n",
    "# # \n",
    "# # This is equivalent to RBF kernel with bandwidth epsilon = sigma²\n",
    "# # So sigma=1 means epsilon=1\n",
    "# # ============================================================\n",
    "\n",
    "# sigma = 1  # kernel bandwidth parameter\n",
    "# kernel = GaussianKernel(sigma)\n",
    "\n",
    "# # ============================================================\n",
    "# # Compute ALL eigenvalues using full eigendecomposition\n",
    "# # ============================================================\n",
    "# # Instead of only computing n_eigs=6, we'll compute ALL eigenvalues\n",
    "# # by manually constructing the Koopman operator matrix\n",
    "\n",
    "# def rbf_kernel_matrix(X1, X2, sigma):\n",
    "#     \"\"\"Compute RBF kernel matrix: k(x,y) = exp(-||x-y||²/(2*sigma²))\"\"\"\n",
    "#     # Compute squared distances\n",
    "#     X1_sq = np.sum(X1**2, axis=1, keepdims=True)  # (n1, 1)\n",
    "#     X2_sq = np.sum(X2**2, axis=1, keepdims=True)  # (n2, 1)\n",
    "#     sq_dists = X1_sq + X2_sq.T - 2 * (X1 @ X2.T)  # (n1, n2)\n",
    "    \n",
    "#     # Apply RBF kernel\n",
    "#     K = np.exp(-sq_dists / (2 * sigma**2))\n",
    "#     return K\n",
    "\n",
    "# K_XX = rbf_kernel_matrix(X, X, sigma)  # shape: (2500, 2500)\n",
    "# K_XY = rbf_kernel_matrix(X, Y, sigma)  # shape: (2500, 2500)\n",
    "\n",
    "# # Construct Koopman operator K = K_XY @ (K_XX + epsilon*I)^{-1}\n",
    "# epsilon_reg = 1e-3  # regularization parameter\n",
    "# I_mat = np.eye(K_XX.shape[0])\n",
    "# K_koopman = K_XY @ np.linalg.inv(K_XX + epsilon_reg * I_mat)\n",
    "\n",
    "# # Compute ALL eigenvalues\n",
    "# eigenvalues_all, eigenvectors_all = np.linalg.eig(K_koopman)\n",
    "\n",
    "# # ============================================================\n",
    "# # Visualization: Eigenvalues on Unit Circle\n",
    "# # ============================================================\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# # Plot unit circle\n",
    "# theta = np.linspace(0, 2*np.pi, 100)\n",
    "# ax.plot(np.cos(theta), np.sin(theta), 'k--', linewidth=1, label='Unit Circle')\n",
    "\n",
    "# # Plot eigenvalues\n",
    "# ax.scatter(eigenvalues_all.real, eigenvalues_all.imag, c='red', s=50, marker='o', \n",
    "#            edgecolors='black', linewidths=1, label='Eigenvalues', zorder=3)\n",
    "\n",
    "# # Set equal aspect ratio and labels\n",
    "# ax.set_aspect('equal')\n",
    "# ax.grid(True, alpha=0.3)\n",
    "# ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "# ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "# ax.set_xlabel('Real', fontsize=12)\n",
    "# ax.set_ylabel('Imaginary', fontsize=12)\n",
    "# ax.set_title('Deeptime Kernel EDMD: Koopman Eigenvalues on Unit Circle', fontsize=14)\n",
    "# ax.legend(fontsize=10)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# print(f\"Number of eigenvalues: {len(eigenvalues_all)}\")\n",
    "# print(f\"Max magnitude: {np.max(np.abs(eigenvalues_all)):.4f}\")\n",
    "# print(f\"Eigenvalues outside unit circle: {np.sum(np.abs(eigenvalues_all) > 1)}\")\n",
    "\n",
    "# # Sort eigenvalues by real part (descending order)\n",
    "# sorted_idx = np.argsort(np.abs(eigenvalues_all))[::-1]\n",
    "# sorted_real_deeptime = np.sort(eigenvalues_all.real)[::-1]\n",
    "\n",
    "# print(\"\\n\" + \"=\"*50)\n",
    "# print(\"Eigenvalues (Real part, sorted from large to small):\")\n",
    "# print(\"=\"*50)\n",
    "# for i, real_part in enumerate(sorted_real_deeptime):\n",
    "#     print(f\"{i+1:3d}. {real_part:+.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02b8631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================================\n",
    "# # Kernel EDMD: Extended Dynamic Mode Decomposition with RBF Kernel\n",
    "# # ============================================================\n",
    "\n",
    "# # Preconditions\n",
    "# if 'X_tar' not in globals() or 'X_tar_next' not in globals():\n",
    "#     raise RuntimeError('X_tar and X_tar_next must be computed before running Kernel EDMD.')\n",
    "\n",
    "# n = X_tar.shape[0]\n",
    "\n",
    "# # Step 1: Build Gaussian RBF kernel matrices\n",
    "# # Compute bandwidth epsilon using median heuristic\n",
    "# sq_tar = np.sum(X_tar ** 2, axis=1)\n",
    "# H_tar = sq_tar[:, None] + sq_tar[None, :] - 2 * (X_tar @ X_tar.T)\n",
    "# epsilon_kedmd = 0.5 * np.median(H_tar) / np.log(n + 1)\n",
    "\n",
    "# def rbf_kernel(X, Y, eps):\n",
    "#     \"\"\"Gaussian RBF kernel k(x,y) = exp(-||x-y||²/(2ε))\"\"\"\n",
    "#     sq_x = np.sum(X ** 2, axis=1)\n",
    "#     sq_y = np.sum(Y ** 2, axis=1)\n",
    "#     H = sq_x[:, None] + sq_y[None, :] - 2 * (X @ Y.T)\n",
    "#     return np.exp(-H / (2 * eps))\n",
    "\n",
    "# # Kernel matrices: K_xx = K(X_tar, X_tar), K_xy = K(X_tar, X_tar_next)\n",
    "# K_xx = rbf_kernel(X_tar, X_tar, 1)\n",
    "# K_xy = rbf_kernel(X_tar, X_tar_next, 1)\n",
    "\n",
    "# # Step 2: Compute Koopman operator via kernel matrices\n",
    "# # K = K_xy @ (K_xx + γI)^{-1}\n",
    "# reg_kedmd = 1e-3  # Use strong regularization like deeptime\n",
    "# I_kedmd = np.eye(n)\n",
    "# K_kedmd = K_xy @ np.linalg.inv(K_xx + reg_kedmd * I_kedmd)\n",
    "\n",
    "# # Step 3: Compute eigenvalues and eigenvectors\n",
    "# eigenvalues_kedmd, eigenvectors_kedmd = np.linalg.eig(K_kedmd)\n",
    "\n",
    "# # Step 4: Construct generator eigenvalues and inverse weights\n",
    "# # Extract real part of eigenvalues (ignore imaginary part)\n",
    "# lambda_ns_kedmd = eigenvalues_kedmd.real\n",
    "\n",
    "# # Construct generator eigenvalues: λ_gen = (λ_K - 1) / dt\n",
    "# lambda_gen_kedmd = (lambda_ns_kedmd - 1.0) / dt\n",
    "\n",
    "# # Build inverse generator weights (for LAWGD)\n",
    "# tol_kedmd = 1e-6\n",
    "# lambda_ns_inv_kedmd = np.zeros_like(lambda_ns_kedmd)\n",
    "# mask_kedmd = lambda_ns_kedmd >= tol_kedmd\n",
    "# lambda_ns_inv_kedmd[mask_kedmd] = 1.0 / (lambda_ns_kedmd[mask_kedmd] + 0.001)\n",
    "\n",
    "# # Store results for LAWGD\n",
    "# eigvals_K_kedmd = lambda_ns_kedmd.copy()\n",
    "# eigvecs_K_kedmd = eigenvectors_kedmd.copy()\n",
    "# lambda_gen_full_kedmd = lambda_gen_kedmd.copy()\n",
    "\n",
    "# # ============================================================\n",
    "# # Visualization: Eigenvalues on Unit Circle\n",
    "# # ============================================================\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# # Plot unit circle\n",
    "# theta = np.linspace(0, 2*np.pi, 100)\n",
    "# ax.plot(np.cos(theta), np.sin(theta), 'k--', linewidth=1, label='Unit Circle')\n",
    "\n",
    "# # Plot eigenvalues\n",
    "# ax.scatter(eigenvalues_kedmd.real, eigenvalues_kedmd.imag, c='red', s=50, marker='o', \n",
    "#            edgecolors='black', linewidths=1, label='Eigenvalues', zorder=3)\n",
    "\n",
    "# # Set equal aspect ratio and labels\n",
    "# ax.set_aspect('equal')\n",
    "# ax.grid(True, alpha=0.3)\n",
    "# ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "# ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "# ax.set_xlabel('Real', fontsize=12)\n",
    "# ax.set_ylabel('Imaginary', fontsize=12)\n",
    "# ax.set_title('Kernel EDMD: Koopman Eigenvalues on Unit Circle', fontsize=14)\n",
    "# ax.legend(fontsize=10)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# print(f\"Number of eigenvalues: {len(eigenvalues_kedmd)}\")\n",
    "# print(f\"Max magnitude: {np.max(np.abs(eigenvalues_kedmd)):.4f}\")\n",
    "# print(f\"Eigenvalues outside unit circle: {np.sum(np.abs(eigenvalues_kedmd) > 1)}\")\n",
    "\n",
    "# # Sort eigenvalues by real part (descending order)\n",
    "# sorted_real_kedmd = np.sort(eigenvalues_kedmd.real)[::-1]\n",
    "\n",
    "# print(\"\\n\" + \"=\"*50)\n",
    "# print(\"Eigenvalues (Real part, sorted from large to small):\")\n",
    "# print(\"=\"*50)\n",
    "# for i, real_part in enumerate(sorted_real_kedmd):\n",
    "#     print(f\"{i+1:3d}. {real_part:+.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841304ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DM Method: Diffusion Maps for Langevin Generator Construction\n",
    "# ============================================================\n",
    "\n",
    "# Preconditions\n",
    "if 'X_tar' not in globals() or 'X_tar_next' not in globals():\n",
    "    raise RuntimeError('X_tar and X_tar_next must be computed before running DM method.')\n",
    "\n",
    "n = X_tar.shape[0]\n",
    "\n",
    "# Step 1: Build Gaussian kernel\n",
    "sq_tar = np.sum(X_tar ** 2, axis=1)\n",
    "H = sq_tar[:, None] + sq_tar[None, :] - 2 * (X_tar @ X_tar.T)\n",
    "# epsilon = 0.5 * np.median(H) / np.log(n + 1)\n",
    "epsilon = 2 * dt\n",
    "\n",
    "def ker(X):\n",
    "    \"\"\"Gaussian kernel k(x,y) = exp(-||x-y||²/(2ε))\"\"\"\n",
    "    sq = np.sum(X ** 2, axis=1)\n",
    "    return np.exp(-(sq[:, None] + sq[None, :] - 2 * (X @ X.T)) / (2 * epsilon))\n",
    "\n",
    "data_kernel = ker(X_tar)\n",
    "\n",
    "# Step 2: Anisotropic normalization\n",
    "p_x = np.sqrt(np.sum(data_kernel, axis=1))\n",
    "p_y = p_x.copy()\n",
    "data_kernel_norm = data_kernel / p_x[:, None] / p_y[None, :]\n",
    "D_y = np.sum(data_kernel_norm, axis=0)\n",
    "\n",
    "# Step 3: Random-walk symmetric normalization\n",
    "# rw_kernel = 0.5 * (data_kernel_norm / D_y + data_kernel_norm / D_y[:, None])\n",
    "rw_kernel = data_kernel_norm / D_y[:, None]\n",
    "\n",
    "# Step 4: SVD to get spectrum\n",
    "phi, s, _ = svd(rw_kernel)\n",
    "lambda_ns = s  # Singular values (eigenvalues of symmetric matrix)\n",
    "\n",
    "# Step 5: Construct generator eigenvalues\n",
    "lambda_gen_dm = (lambda_ns - 1.0) / epsilon\n",
    "\n",
    "# Step 6: Build inverse generator weights (for LAWGD)\n",
    "tol = 1e-6\n",
    "lambda_ns_inv = np.zeros_like(lambda_ns)\n",
    "mask = lambda_ns >= tol\n",
    "lambda_ns_inv[mask] = epsilon / (lambda_ns[mask] + 0.001)\n",
    "\n",
    "# Store results for LAWGD\n",
    "eigvals_K_dm = lambda_ns.copy()\n",
    "eigvecs_K_dm = phi.copy()\n",
    "lambda_gen_full = lambda_gen_dm.copy()\n",
    "\n",
    "# ============================================================\n",
    "# Visualization: Eigenvalues on Unit Circle\n",
    "# ============================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# Plot unit circle\n",
    "theta = np.linspace(0, 2*np.pi, 100)\n",
    "ax.plot(np.cos(theta), np.sin(theta), 'k--', linewidth=1, label='Unit Circle')\n",
    "\n",
    "# Plot eigenvalues\n",
    "ax.scatter(eigvals_K_dm.real, eigvals_K_dm.imag, c='red', s=50, marker='o', \n",
    "           edgecolors='black', linewidths=1, label='Eigenvalues', zorder=3)\n",
    "\n",
    "# Set equal aspect ratio and labels\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "ax.set_xlabel('Real', fontsize=12)\n",
    "ax.set_ylabel('Imaginary', fontsize=12)\n",
    "ax.set_title('DM Method: Koopman Eigenvalues on Unit Circle', fontsize=14)\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Number of eigenvalues: {len(eigvals_K_dm)}\")\n",
    "print(f\"Max magnitude: {np.max(np.abs(eigvals_K_dm)):.4f}\")\n",
    "print(f\"Eigenvalues outside unit circle: {np.sum(np.abs(eigvals_K_dm) > 1)}\")\n",
    "\n",
    "# Sort eigenvalues by real part (descending order)\n",
    "sorted_real_dm = np.sort(eigvals_K_dm.real)[::-1]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Eigenvalues (Real part, sorted from large to small):\")\n",
    "print(\"=\"*50)\n",
    "for i, real_part in enumerate(sorted_real_dm):\n",
    "    print(f\"{i+1:3d}. {real_part:+.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2c77ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LAWGD: Langevin-Adjusted Wasserstein Gradient Descent (DMPS)\n",
    "# ============================================================\n",
    "\n",
    "from matplotlib.patches import Circle\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Step 1: Generate initial particles outside latent anchors ...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "n_init_particles = 500\n",
    "\n",
    "def sample_particles_outside(target, radius_scale=1.1, batch_size=2000):\n",
    "    collected = []\n",
    "    total_needed = target\n",
    "    radius = well_radius * radius_scale\n",
    "    while sum(chunk.shape[0] for chunk in collected) < total_needed:\n",
    "        batch, _, _ = sample_latent(latent_embeddings, mnist_labels, n_samples=batch_size)\n",
    "        dist = np.linalg.norm(batch[:, None, :] - well_centers[None, :, :], axis=2)\n",
    "        mask = dist.min(axis=1) > radius\n",
    "        if np.any(mask):\n",
    "            collected.append(batch[mask])\n",
    "    stacked = np.vstack(collected)\n",
    "    return stacked[:total_needed]\n",
    "\n",
    "X_lawgd_init = sample_particles_outside(n_init_particles)\n",
    "print(f\"Initial particles (outside anchors): {X_lawgd_init.shape[0]}\")\n",
    "print(f\"Anchor radius threshold: {well_radius:.3f}\")\n",
    "\n",
    "# Step 2: Prepare LAWGD using DM-based Koopman spectrum\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Step 2: Preparing LAWGD with DM Koopman spectrum...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Use DM method results (computed in previous cell)\n",
    "eigvals_K = eigvals_K_dm.copy()\n",
    "eigvecs_K = eigvecs_K_dm.copy()\n",
    "lambda_gen = lambda_gen_full.copy()\n",
    "\n",
    "# Mode selection strategy: use all valid modes\n",
    "n_skip = 1  # Skip first 1 eigenvalues (constant modes)\n",
    "eig_threshold = 0.01  # Threshold for valid eigenvalues (exclude near-zero eigenvalues)\n",
    "\n",
    "# Find valid eigenvalues: skip first n_skip, and keep those > threshold\n",
    "eigvals_after_skip = eigvals_K[n_skip:]\n",
    "valid_mask = eigvals_after_skip.real > eig_threshold\n",
    "n_valid = np.sum(valid_mask)\n",
    "\n",
    "# Use ALL valid modes\n",
    "k_modes = n_valid\n",
    "mode_start = n_skip\n",
    "mode_end = mode_start + k_modes\n",
    "\n",
    "print(f\"\\n  Mode selection strategy: use all valid modes\")\n",
    "print(f\"    - Total eigenvalues: {len(eigvals_K)}\")\n",
    "print(f\"    - Skipping first {n_skip} eigenvalues (constant modes)\")\n",
    "print(f\"    - Eigenvalue threshold: λ > {eig_threshold} (exclude near-zero)\")\n",
    "print(f\"    - Skipped due to threshold: {len(eigvals_K) - n_skip - k_modes}\")\n",
    "print(f\"    - **Using {k_modes} valid modes** (modes {mode_start+1} to {mode_end})\")\n",
    "print(f\"\\n  Selected eigenvalues for LAWGD:\")\n",
    "for i in range(mode_start, mode_end):\n",
    "    print(f\"    Mode {i+1}: λ = {eigvals_K[i].real:.6f}, λ_gen = {lambda_gen[i].real:.2f}\")\n",
    "\n",
    "# Build inverse generator weights for selected modes\n",
    "lambda_gen_selected = lambda_gen[mode_start:mode_end]\n",
    "tol_gen = 1e-6\n",
    "lambda_gen_inv_selected = np.zeros(k_modes, dtype=complex)\n",
    "mask_nonzero = np.abs(lambda_gen_selected) > tol_gen\n",
    "lambda_gen_inv_selected[mask_nonzero] = 1.0 / lambda_gen_selected[mask_nonzero]\n",
    "\n",
    "print(f\"  Generator eigenvalue range: [{lambda_gen_selected.real.min():.2f}, {lambda_gen_selected.real.max():.2f}]\")\n",
    "\n",
    "# Step 4: Helper functions for kernel evaluation and gradient\n",
    "def evaluate_kernel_at_points(X_query, X_data):\n",
    "    \"\"\"\n",
    "    Evaluate normalized kernel k(x_query, x_data) for all pairs\n",
    "    Returns: (n_query, n_data) kernel matrix\n",
    "    \"\"\"\n",
    "    sq_query = np.sum(X_query ** 2, axis=1)\n",
    "    sq_data = np.sum(X_data ** 2, axis=1)\n",
    "    H = sq_query[:, None] + sq_data[None, :] - 2 * (X_query @ X_data.T)\n",
    "    K_raw = np.exp(-H / (2 * epsilon))\n",
    "    \n",
    "    # Apply anisotropic normalization (using p_x from training data)\n",
    "    # For query points, estimate density using kernel with training data\n",
    "    p_query = np.sqrt(np.sum(K_raw, axis=1))\n",
    "    K_norm = K_raw / p_query[:, None] / p_x[None, :]\n",
    "    \n",
    "    # Apply random-walk normalization (using D_y from training)\n",
    "    # K_rw[i,j] = 0.5 * (K_norm[i,j] / D_y + K_norm[j,i] / D_y[i])\n",
    "    # Since K_norm is (n_query, n_data) and not symmetric, we simplify:\n",
    "    K_rw = K_norm / D_y[None, :]  # Divide each column by D_y\n",
    "    \n",
    "    return K_rw\n",
    "\n",
    "def compute_kernel_gradient(X_query, X_data):\n",
    "    \"\"\"\n",
    "    Compute gradient of kernel ∇_x k(x, y) w.r.t. x (query points)\n",
    "    Returns: (n_query, n_data, 2) array where [:, :, d] is ∂k/∂x_d\n",
    "    \"\"\"\n",
    "    n_query = X_query.shape[0]\n",
    "    n_data = X_data.shape[0]\n",
    "    \n",
    "    # Compute pairwise differences: X_query - X_data\n",
    "    # Shape: (n_query, n_data, 2)\n",
    "    diff = X_query[:, None, :] - X_data[None, :, :]  # Broadcasting\n",
    "    \n",
    "    # Compute base kernel\n",
    "    sq_query = np.sum(X_query ** 2, axis=1)\n",
    "    sq_data = np.sum(X_data ** 2, axis=1)\n",
    "    H = sq_query[:, None] + sq_data[None, :] - 2 * (X_query @ X_data.T)\n",
    "    K_raw = np.exp(-H / (2 * epsilon))\n",
    "    \n",
    "    # Gradient of Gaussian kernel: ∂k/∂x = -k(x,y) * (x-y) / ε\n",
    "    # Shape: (n_query, n_data, 2)\n",
    "    grad_K_raw = -K_raw[:, :, None] * diff / epsilon\n",
    "    \n",
    "    # For simplicity, apply normalization to gradient (approximate)\n",
    "    # This is a simplified version; full gradient would include normalization terms\n",
    "    p_query = np.sqrt(np.sum(K_raw, axis=1, keepdims=True))\n",
    "    grad_K_norm = grad_K_raw / p_query[:, :, None] / p_x[None, :, None]\n",
    "    \n",
    "    return grad_K_norm\n",
    "\n",
    "# Step 5: LAWGD iteration with DM kernel\n",
    "n_particles = X_lawgd_init.shape[0]\n",
    "n_iter_lawgd = 1000\n",
    "h_lawgd = 1  # Step size\n",
    "\n",
    "X_lawgd_traj = np.zeros((n_particles, 2, n_iter_lawgd))\n",
    "X_lawgd_traj[:, :, 0] = X_lawgd_init.copy()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Step 3: Running LAWGD iterations (DM kernel-based)...\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Particles: {n_particles}\")\n",
    "print(f\"Iterations: {n_iter_lawgd}\")\n",
    "print(f\"Step size: {h_lawgd}\")\n",
    "print(f\"Active modes: {k_modes}\")\n",
    "\n",
    "for t in range(n_iter_lawgd - 1):\n",
    "    X_curr = X_lawgd_traj[:, :, t]\n",
    "    \n",
    "    # Evaluate kernel between current particles and training data\n",
    "    K_curr = evaluate_kernel_at_points(X_curr, X_tar)  # (n_particles, n_data)\n",
    "    \n",
    "    # Project onto eigenmodes: c = Φ^T @ K(x, X_tar)\n",
    "    # eigvecs_K has shape (n_data, n_data), we use selected modes\n",
    "    eigvecs_selected = eigvecs_K[:, mode_start:mode_end]  # (n_data, k_modes)\n",
    "    c = eigvecs_selected.T @ K_curr.T  # (k_modes, n_particles)\n",
    "    \n",
    "    # Apply inverse generator: c_inv = Λ_gen^{-1} @ c\n",
    "    c_inv = lambda_gen_inv_selected[:, None] * c  # (k_modes, n_particles)\n",
    "    \n",
    "    # Project back to data space: f = Φ @ c_inv\n",
    "    f_inv = eigvecs_selected @ c_inv  # (n_data, n_particles)\n",
    "    \n",
    "    # Compute gradient using kernel gradient\n",
    "    grad_K = compute_kernel_gradient(X_curr, X_tar)  # (n_particles, n_data, 2)\n",
    "    \n",
    "    # Gradient update: ∇_x f = Σ_i f_inv[i] * ∇_x k(x, x_i)\n",
    "    grad_update = np.zeros((n_particles, 2))\n",
    "    for d_idx in range(2):\n",
    "        grad_update[:, d_idx] = np.sum(\n",
    "            grad_K[:, :, d_idx] * f_inv.T.real,  # Use real part\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    # Gradient descent step\n",
    "    X_lawgd_traj[:, :, t+1] = X_curr - h_lawgd * grad_update\n",
    "    \n",
    "    if (t+1) % 100 == 0 or t == 0:\n",
    "        print(f\"\\r  [DM] Iteration {t+1}/{n_iter_lawgd-1}  \", end='', flush=True)\n",
    "\n",
    "print()  # Print newline after loop\n",
    "print(\"LAWGD iteration complete!\")\n",
    "\n",
    "# Step 6: Compute final metrics\n",
    "dist_final_to_wells = np.array([np.linalg.norm(X_lawgd_traj[:, :, -1] - center, axis=1) for center in well_centers])\n",
    "min_dist_final = np.min(dist_final_to_wells, axis=0)\n",
    "in_well_final = min_dist_final <= well_radius\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"LAWGD Results:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Initial particles (outside anchors): {n_particles}\")\n",
    "print(f\"Final particles near anchors: {np.sum(in_well_final)} ({100*np.sum(in_well_final)/n_particles:.1f}%)\")\n",
    "print(f\"Final particles still outside: {n_particles - np.sum(in_well_final)} ({100*(n_particles - np.sum(in_well_final))/n_particles:.1f}%)\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ddd942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualization 1: Initial vs Final Positions (DMPS)\n",
    "# ============================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "\n",
    "ax.contourf(X_grid, Y_grid, density_grid, levels=30, cmap='Blues', alpha=0.7)\n",
    "ax.scatter(X_tar[:, 0], X_tar[:, 1], s=3, c='lightgray', alpha=0.2)\n",
    "ax.scatter(X_lawgd_traj[:, 0, 0], X_lawgd_traj[:, 1, 0],\n",
    "           s=25, c='red', marker='o', label='Initial (outside anchors)', zorder=5)\n",
    "ax.scatter(X_lawgd_traj[:, 0, -1], X_lawgd_traj[:, 1, -1],\n",
    "           s=35, facecolors='none', edgecolors='magenta', linewidth=1.5,\n",
    "           label='Final', zorder=15)\n",
    "\n",
    "# Draw anchor boundaries\n",
    "for idx, center in enumerate(well_centers):\n",
    "    circle = Circle(center, well_radius, fill=False, edgecolor='green',\n",
    "                   linewidth=2, linestyle='--')\n",
    "    ax.add_patch(circle)\n",
    "\n",
    "ax.scatter(well_centers[:, 0], well_centers[:, 1], s=100, c='green',\n",
    "           marker='*', zorder=10)\n",
    "ax.set_xlabel('Latent dim 1', fontsize=12)\n",
    "ax.set_ylabel('Latent dim 2', fontsize=12)\n",
    "ax.set_title('DMPS: Initial vs Final Positions', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim(latent_bounds[0])\n",
    "ax.set_ylim(latent_bounds[1])\n",
    "ax.set_aspect('equal')\n",
    "ax.legend(loc='upper right', fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a0ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualization 2: Trajectories (DMPS)\n",
    "# ============================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "\n",
    "ax.contourf(X_grid, Y_grid, density_grid, levels=30, cmap='Blues', alpha=0.7)\n",
    "\n",
    "n_show_traj = min(15, n_particles)\n",
    "for i in range(n_show_traj):\n",
    "    ax.plot(X_lawgd_traj[i, 0, :], X_lawgd_traj[i, 1, :],\n",
    "            alpha=0.5, linewidth=1, color='gray')\n",
    "\n",
    "ax.scatter(X_lawgd_traj[:n_show_traj, 0, 0], X_lawgd_traj[:n_show_traj, 1, 0],\n",
    "           s=40, c='red', marker='o', zorder=5, label='Start')\n",
    "ax.scatter(X_lawgd_traj[:n_show_traj, 0, -1], X_lawgd_traj[:n_show_traj, 1, -1],\n",
    "           s=50, facecolors='none', edgecolors='magenta',\n",
    "           linewidth=1.5, zorder=15, label='End')\n",
    "\n",
    "for idx, center in enumerate(well_centers):\n",
    "    circle = Circle(center, well_radius, fill=False, edgecolor='green',\n",
    "                   linewidth=2, linestyle='--')\n",
    "    ax.add_patch(circle)\n",
    "\n",
    "ax.scatter(well_centers[:, 0], well_centers[:, 1], s=100, c='green',\n",
    "           marker='*', zorder=10)\n",
    "ax.set_xlabel('Latent dim 1', fontsize=12)\n",
    "ax.set_ylabel('Latent dim 2', fontsize=12)\n",
    "ax.set_title(f'DMPS Trajectories (first {n_show_traj} particles)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xlim(latent_bounds[0])\n",
    "ax.set_ylim(latent_bounds[1])\n",
    "ax.set_aspect('equal')\n",
    "ax.legend(loc='upper right', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c151ed15",
   "metadata": {},
   "source": [
    "## Convergence Analysis: DMPS with Metrics Tracking (IID Data)\n",
    "\n",
    "Run DMPS for 1000 steps and track convergence metrics every 100 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27ce0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DMPS with Metrics Tracking (0-1000 steps, every 100 steps)\n",
    "# ============================================================\n",
    "\n",
    "from matplotlib.patches import Circle\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DMPS: Running 1000 iterations with metrics tracking...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Setup parameters\n",
    "n_particles = X_lawgd_init.shape[0]\n",
    "n_iter_lawgd = 1000\n",
    "h_lawgd = 1  # Step size\n",
    "record_interval = 100  # Record metrics every 100 steps\n",
    "n_records = n_iter_lawgd // record_interval + 1  # Include step 0\n",
    "\n",
    "# Initialize trajectory storage\n",
    "X_lawgd_traj_dm = np.zeros((n_particles, 2, n_iter_lawgd + 1))\n",
    "X_lawgd_traj_dm[:, :, 0] = X_lawgd_init.copy()\n",
    "\n",
    "# Initialize metrics storage\n",
    "metrics_dm = {\n",
    "    'steps': [],\n",
    "    'well_coverage': [],  # Percentage of particles in wells\n",
    "    'avg_potential': [],  # Average potential energy\n",
    "    'movement_rate': []   # Average displacement per step\n",
    "}\n",
    "\n",
    "print(f\"Particles: {n_particles}\")\n",
    "print(f\"Iterations: {n_iter_lawgd}\")\n",
    "print(f\"Recording interval: {record_interval} steps\")\n",
    "print(f\"Total records: {n_records}\")\n",
    "\n",
    "# Helper function to compute metrics\n",
    "def compute_metrics(X_current, X_previous=None):\n",
    "    \"\"\"Compute convergence metrics for current particle positions\"\"\"\n",
    "    # 1. Well coverage: percentage in wells\n",
    "    dist_to_wells = np.array([np.linalg.norm(X_current - center, axis=1) \n",
    "                              for center in well_centers])\n",
    "    min_dist = np.min(dist_to_wells, axis=0)\n",
    "    in_well = min_dist <= well_radius\n",
    "    coverage = 100.0 * np.sum(in_well) / len(X_current)\n",
    "    \n",
    "    # 2. Average potential energy\n",
    "    avg_V = np.mean(potential.V(X_current))\n",
    "    \n",
    "    # 3. Movement rate (only if previous positions available)\n",
    "    if X_previous is not None:\n",
    "        displacement = np.linalg.norm(X_current - X_previous, axis=1)\n",
    "        movement = np.mean(displacement)\n",
    "    else:\n",
    "        movement = 0.0\n",
    "    \n",
    "    return coverage, avg_V, movement\n",
    "\n",
    "# Record initial state (step 0)\n",
    "coverage_0, avg_V_0, _ = compute_metrics(X_lawgd_traj_dm[:, :, 0])\n",
    "metrics_dm['steps'].append(0)\n",
    "metrics_dm['well_coverage'].append(coverage_0)\n",
    "metrics_dm['avg_potential'].append(avg_V_0)\n",
    "metrics_dm['movement_rate'].append(0.0)\n",
    "\n",
    "print(f\"\\nInitial state (step 0):\")\n",
    "print(f\"  Well coverage: {coverage_0:.2f}%\")\n",
    "print(f\"  Avg potential: {avg_V_0:.4f}\")\n",
    "\n",
    "# Step 2: Run LAWGD iterations with DM kernel\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Running DMPS iterations...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for t in range(n_iter_lawgd):\n",
    "    X_curr = X_lawgd_traj_dm[:, :, t]\n",
    "    \n",
    "    # Evaluate kernel between current particles and training data\n",
    "    K_curr = evaluate_kernel_at_points(X_curr, X_tar)  # (n_particles, n_data)\n",
    "    \n",
    "    # Project onto eigenmodes: c = Φ^T @ K(x, X_tar)\n",
    "    eigvecs_selected = eigvecs_K[:, mode_start:mode_end]  # (n_data, k_modes)\n",
    "    c = eigvecs_selected.T @ K_curr.T  # (k_modes, n_particles)\n",
    "    \n",
    "    # Apply inverse generator: c_inv = Λ_gen^{-1} @ c\n",
    "    c_inv = lambda_gen_inv_selected[:, None] * c  # (k_modes, n_particles)\n",
    "    \n",
    "    # Project back to data space: f = Φ @ c_inv\n",
    "    f_inv = eigvecs_selected @ c_inv  # (n_data, n_particles)\n",
    "    \n",
    "    # Compute gradient using kernel gradient\n",
    "    grad_K = compute_kernel_gradient(X_curr, X_tar)  # (n_particles, n_data, 2)\n",
    "    \n",
    "    # Gradient update: ∇_x f = Σ_i f_inv[i] * ∇_x k(x, x_i)\n",
    "    grad_update = np.zeros((n_particles, 2))\n",
    "    for d_idx in range(2):\n",
    "        grad_update[:, d_idx] = np.sum(\n",
    "            grad_K[:, :, d_idx] * f_inv.T.real,  # Use real part\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    # Gradient descent step\n",
    "    X_lawgd_traj_dm[:, :, t+1] = X_curr - h_lawgd * grad_update\n",
    "    \n",
    "    # Record metrics every record_interval steps\n",
    "    if (t + 1) % record_interval == 0:\n",
    "        X_prev = X_lawgd_traj_dm[:, :, t]\n",
    "        X_next = X_lawgd_traj_dm[:, :, t+1]\n",
    "        coverage, avg_V, movement = compute_metrics(X_next, X_prev)\n",
    "        \n",
    "        metrics_dm['steps'].append(t + 1)\n",
    "        metrics_dm['well_coverage'].append(coverage)\n",
    "        metrics_dm['avg_potential'].append(avg_V)\n",
    "        metrics_dm['movement_rate'].append(movement)\n",
    "    \n",
    "    # Progress indicator\n",
    "    if (t+1) % 200 == 0 or t == 0:\n",
    "        print(f\"\\r  [DM] Iteration {t+1}/{n_iter_lawgd}  \", end='', flush=True)\n",
    "\n",
    "print()  # Newline after loop\n",
    "print(\"DMPS iteration complete!\")\n",
    "\n",
    "# Step 3: Final summary\n",
    "final_coverage = metrics_dm['well_coverage'][-1]\n",
    "final_potential = metrics_dm['avg_potential'][-1]\n",
    "final_movement = metrics_dm['movement_rate'][-1]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DMPS Final Results:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Final well coverage: {final_coverage:.2f}%\")\n",
    "print(f\"Final avg potential: {final_potential:.4f}\")\n",
    "print(f\"Final movement rate: {final_movement:.6f}\")\n",
    "print(f\"Total records: {len(metrics_dm['steps'])}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Convert metrics to numpy arrays for easier plotting\n",
    "for key in ['steps', 'well_coverage', 'avg_potential', 'movement_rate']:\n",
    "    metrics_dm[key] = np.array(metrics_dm[key])\n",
    "\n",
    "print(\"\\n✓ DMPS metrics saved in 'metrics_dm' dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73681cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SDMD: Stochastic Dynamic Mode Decomposition with Dictionary Learning\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "# import torch.nn.functional as F\n",
    "# from numpy import linalg as la\n",
    "# from numpy import *\n",
    "from solver_sdmd_torch_gpu import KoopmanNNTorch, KoopmanSolverTorch\n",
    "\n",
    "\n",
    "print (torch.__version__, torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print (torch.cuda.get_device_name())\n",
    "# device= 'cpu'\n",
    "device= 'cuda'\n",
    "\n",
    "# Reshape data_X and data_Y into a single column\n",
    "X = X_tar  # 2D latent features\n",
    "Y = X_tar_next  # 2D latent targets\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of Y: {Y.shape}\")\n",
    "\n",
    "# Separate data into two parts: train and validation\n",
    "len_all = X.shape[0]\n",
    "data_x_train = X[:int(0.7*len_all)]\n",
    "data_x_valid = X[int(0.7*len_all)+1:]\n",
    "\n",
    "data_y_train = Y[:int(0.7*len_all)]\n",
    "data_y_valid = Y[int(0.7*len_all)+1:]\n",
    "\n",
    "data_train = [data_x_train, data_y_train]\n",
    "data_valid = [data_x_valid, data_y_valid]\n",
    "\n",
    "print(data_x_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "#### SDMD Test ####\n",
    "checkpoint_file= 'well2d_example_ckpt001.torch'\n",
    "\n",
    "basis_function = KoopmanNNTorch(input_size= 2, layer_sizes=[10], n_psi_train=7).to(device)  # basis number would be 20\n",
    "\n",
    "\n",
    "solver = KoopmanSolverTorch(dic=basis_function, # Replace 'koopman_nn' by 'dic' if you use the original solver_edmdvar\n",
    "                       target_dim=np.shape(data_x_train)[-1],\n",
    "                                                   reg=0.1,  checkpoint_file= checkpoint_file, fnn_checkpoint_file= 'example_fnn001.torch', \n",
    "                            a_b_file= 'a_b_example_3ple_well.jbl', \n",
    "                        generator_batch_size= 2, fnn_batch_size= 32, delta_t= dt)\n",
    "\n",
    "solver.build_with_generator(\n",
    "    data_train=data_train,\n",
    "    data_valid=data_valid,\n",
    "    epochs=6,\n",
    "    batch_size=256,\n",
    "    lr=1e-5,\n",
    "    log_interval=10,\n",
    "    lr_decay_factor=.8\n",
    "    )\n",
    "\n",
    "# Results from solver_edmd/solver_resdmd\n",
    "evalues_sdmd = solver.eigenvalues.T\n",
    "efuns_sdmd = solver.eigenfunctions(X)\n",
    "evectors_sdmd = solver.eigenvectors.T\n",
    "# kpm_modes = solver.compute_mode().T\n",
    "N_dict_sdmd = np.shape(evalues_sdmd)[0]\n",
    "Psi_X_sdmd = solver.get_Psi_X()\n",
    "Psi_Y_sdmd = solver.get_Psi_Y()\n",
    "Koopman_matrix_K_sdmd = solver.K\n",
    "\n",
    "outputs_sdmd = {\n",
    "    'efuns': efuns_sdmd,\n",
    "    'evalues': evalues_sdmd,\n",
    "    'evectors': evectors_sdmd,\n",
    "    # 'kpm_modes': kpm_modes,\n",
    "    'N_dict': N_dict_sdmd,\n",
    "    'K': Koopman_matrix_K_sdmd,\n",
    "    # 'Psi_X': Psi_X_sdmd,\n",
    "    # 'Psi_Y': Psi_Y_sdmd,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Visualization: Eigenvalues on Unit Circle\n",
    "# ============================================================\n",
    "\n",
    "# Assuming 'efuns_sdmd' is a 2D numpy array with shape (n_samples, n_eigenfunctions)\n",
    "# and 'X' is a 2D numpy array with shape (n_samples, 2) representing your input data\n",
    "\n",
    "# Assuming evalues_sdmd is a numpy array of complex numbers\n",
    "print(\"SDMD eigenvalues shape\", evalues_sdmd.shape)\n",
    "print(\"SDMD eigenvalues\", evalues_sdmd)\n",
    "print(\"SDMD eigenvectors shape\", evectors_sdmd.shape)\n",
    "print(\"SDMD eigenvectors\", evectors_sdmd)\n",
    "\n",
    "# Plot eigenvalues on unit circle\n",
    "real_parts = evalues_sdmd.real\n",
    "imag_parts = evalues_sdmd.imag\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(real_parts, imag_parts, color='blue', label='Eigenvalues')\n",
    "\n",
    "# Draw a unit circle for reference\n",
    "theta = np.linspace(0, 2 * np.pi, 100)\n",
    "plt.plot(np.cos(theta), np.sin(theta), linestyle='--', color='grey', label='Unit Circle')\n",
    "\n",
    "plt.title('SDMD')\n",
    "plt.xlabel('Real Part')\n",
    "plt.ylabel('Imaginary Part')\n",
    "plt.axis('equal')  # Ensure the aspect ratio is equal to make the unit circle round\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot eigenfunctions\n",
    "fig, axs = plt.subplots(1, 4, figsize=(9, 2))\n",
    "\n",
    "# Plot for the 1st eigenfunction\n",
    "# scatter1 = axs[0].scatter(*X.T, c=np.real(efuns_sdmd)[:, 0], cmap='coolwarm')\n",
    "scatter1 = axs[0].scatter(*X.T, c=np.real(efuns_sdmd)[:, 0], cmap='coolwarm', vmin=1.091, vmax=1.093)\n",
    "axs[0].set_title('1st eigenfunction')\n",
    "axs[0].set_xlim(latent_bounds[0])\n",
    "axs[0].set_ylim(latent_bounds[1])\n",
    "cbar1 = fig.colorbar(scatter1, ax=axs[0], shrink=0.7, aspect=20)\n",
    "\n",
    "# Plot for the 2nd eigenfunction\n",
    "scatter2 = axs[1].scatter(*X.T, c=np.real(efuns_sdmd)[:, 1], cmap='coolwarm')\n",
    "axs[1].set_title('2nd eigenfunction')\n",
    "axs[1].set_xlim(latent_bounds[0])\n",
    "axs[1].set_ylim(latent_bounds[1])\n",
    "cbar2 = fig.colorbar(scatter2, ax=axs[1])\n",
    "\n",
    "# Plot for the 3rd eigenfunction\n",
    "scatter3 = axs[2].scatter(*X.T, c=np.real(efuns_sdmd)[:, 2], cmap='coolwarm')\n",
    "axs[2].set_title('3rd eigenfunction')\n",
    "axs[2].set_xlim(latent_bounds[0])\n",
    "axs[2].set_ylim(latent_bounds[1])\n",
    "cbar3 = fig.colorbar(scatter3, ax=axs[2])\n",
    "\n",
    "# Plot for the 4th eigenfunction\n",
    "scatter4 = axs[3].scatter(*X.T, c=np.real(efuns_sdmd)[:, 3], cmap='coolwarm')\n",
    "axs[3].set_title('4th eigenfunction')\n",
    "axs[3].set_xlim(latent_bounds[0])\n",
    "axs[3].set_ylim(latent_bounds[1])\n",
    "cbar4 = fig.colorbar(scatter4, ax=axs[3])\n",
    "\n",
    "fig.suptitle('SDMD', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# Construct Generator Eigenvalues and Inverse Weights (for LAWGD)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Constructing Generator Inverse for LAWGD...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extract real part of eigenvalues (SDMD eigenvalues are already computed)\n",
    "lambda_ns_sdmd = evalues_sdmd.real\n",
    "\n",
    "# Construct generator eigenvalues: λ_gen = (λ_K - 1) / dt\n",
    "lambda_gen_sdmd = (lambda_ns_sdmd - 1.0) / dt\n",
    "\n",
    "# Build inverse generator weights (for LAWGD)\n",
    "tol_sdmd = 1e-6\n",
    "lambda_ns_inv_sdmd = np.zeros_like(lambda_ns_sdmd)\n",
    "mask_sdmd = lambda_ns_sdmd >= tol_sdmd\n",
    "lambda_ns_inv_sdmd[mask_sdmd] = dt / (lambda_ns_sdmd[mask_sdmd] + 0.001)\n",
    "\n",
    "# Store results for LAWGD (following DM method naming convention)\n",
    "eigvals_K_sdmd = lambda_ns_sdmd.copy()\n",
    "eigvecs_K_sdmd = efuns_sdmd.copy()  # ✓ Use eigenfunctions (values on data points)!\n",
    "lambda_gen_full_sdmd = lambda_gen_sdmd.copy()\n",
    "\n",
    "print(f\"  - eigvals_K_sdmd shape: {eigvals_K_sdmd.shape}\")\n",
    "print(f\"  - eigvecs_K_sdmd shape: {eigvecs_K_sdmd.shape}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e7fb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LAWGD: Langevin-Adjusted Wasserstein Gradient Descent (SDMD)\n",
    "# ============================================================\n",
    "\n",
    "from matplotlib.patches import Circle\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LAWGD with SDMD Koopman Spectrum\")\n",
    "print(\"=\"*60)\n",
    "print(\"Strategy: Use SDMD eigenvalues/eigenvectors + DM kernel/normalization\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Reuse initial particles from DMPS (or generate new ones)\n",
    "if 'X_lawgd_init' not in globals():\n",
    "    print(\"\\nGenerating initial particles outside anchors ...\")\n",
    "    n_init_particles = 500\n",
    "    X_lawgd_init = sample_particles_outside(n_init_particles)\n",
    "    print(f\"Particles outside anchors: {X_lawgd_init.shape[0]}\")\n",
    "else:\n",
    "    print(f\"\\nReusing existing initial particles: {X_lawgd_init.shape[0]}\")\n",
    "\n",
    "# Step 2: Prepare LAWGD using SDMD-based Koopman spectrum\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Step 2: Preparing LAWGD with SDMD Koopman spectrum...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Use SDMD method results (computed in Cell 13)\n",
    "eigvals_K = eigvals_K_sdmd.copy()\n",
    "eigvecs_K = eigvecs_K_sdmd.copy()\n",
    "lambda_gen = lambda_gen_full_sdmd.copy()\n",
    "\n",
    "print(f\"SDMD eigenvalues shape: {eigvals_K.shape}\")\n",
    "print(f\"SDMD eigenvectors shape: {eigvecs_K.shape}\")\n",
    "\n",
    "# Mode selection strategy: use all valid modes\n",
    "n_skip = 1  # Skip first 1 eigenvalues (constant modes)\n",
    "eig_threshold = 0.01  # Threshold for valid eigenvalues (exclude near-zero eigenvalues)\n",
    "\n",
    "# Find valid eigenvalues: skip first n_skip, and keep those > threshold\n",
    "eigvals_after_skip = eigvals_K[n_skip:]\n",
    "valid_mask = eigvals_after_skip.real > eig_threshold\n",
    "n_valid = np.sum(valid_mask)\n",
    "\n",
    "# Use ALL valid modes\n",
    "k_modes = n_valid\n",
    "mode_start = n_skip\n",
    "mode_end = mode_start + k_modes\n",
    "\n",
    "print(f\"\\n  Mode selection strategy: use all valid modes\")\n",
    "print(f\"    - Total eigenvalues: {len(eigvals_K)}\")\n",
    "print(f\"    - Skipping first {n_skip} eigenvalues (constant modes)\")\n",
    "print(f\"    - Eigenvalue threshold: λ > {eig_threshold} (exclude near-zero)\")\n",
    "print(f\"    - Skipped due to threshold: {len(eigvals_K) - n_skip - k_modes}\")\n",
    "print(f\"    - **Using {k_modes} valid modes** (modes {mode_start+1} to {mode_end})\")\n",
    "print(f\"\\n  Selected eigenvalues for LAWGD:\")\n",
    "for i in range(mode_start, min(mode_end, mode_start + 10)):\n",
    "    print(f\"    Mode {i+1}: λ = {eigvals_K[i].real:.6f}, λ_gen = {lambda_gen[i].real:.2f}\")\n",
    "if mode_end > mode_start + 10:\n",
    "    print(f\"    ... ({mode_end - mode_start - 10} more modes)\")\n",
    "\n",
    "# Build inverse generator weights for selected modes\n",
    "lambda_gen_selected = lambda_gen[mode_start:mode_end]\n",
    "tol_gen = 1e-6\n",
    "lambda_gen_inv_selected = np.zeros(k_modes, dtype=complex)\n",
    "mask_nonzero = np.abs(lambda_gen_selected) > tol_gen\n",
    "lambda_gen_inv_selected[mask_nonzero] = 1.0 / lambda_gen_selected[mask_nonzero]\n",
    "\n",
    "print(f\"\\n  Generator eigenvalue range: [{lambda_gen_selected.real.min():.2f}, {lambda_gen_selected.real.max():.2f}]\")\n",
    "\n",
    "# Step 3: Reuse DM's kernel functions and normalization parameters\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Step 3: Reusing DM kernel functions and normalization...\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  - Kernel bandwidth (epsilon): {epsilon:.6f}\")\n",
    "print(f\"  - Normalization parameters: p_x, D_y (from DM method)\")\n",
    "print(f\"  - Kernel functions: evaluate_kernel_at_points(), compute_kernel_gradient()\")\n",
    "\n",
    "if 'epsilon' not in globals() or 'p_x' not in globals() or 'D_y' not in globals():\n",
    "    raise RuntimeError(\"DM normalization parameters not found! Please run Cell 18 (DM method) first.\")\n",
    "\n",
    "# Step 4: LAWGD iteration with SDMD spectrum + DM kernel\n",
    "n_particles = X_lawgd_init.shape[0]\n",
    "n_iter_lawgd = 1000\n",
    "h_lawgd = 1  # Step size\n",
    "\n",
    "X_lawgd_traj_sdmd = np.zeros((n_particles, 2, n_iter_lawgd))\n",
    "X_lawgd_traj_sdmd[:, :, 0] = X_lawgd_init.copy()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Step 4: Running LAWGD iterations (SDMD spectrum + DM kernel)...\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Particles: {n_particles}\")\n",
    "print(f\"Iterations: {n_iter_lawgd}\")\n",
    "print(f\"Step size: {h_lawgd}\")\n",
    "print(f\"Active modes: {k_modes}\")\n",
    "\n",
    "for t in range(n_iter_lawgd - 1):\n",
    "    X_curr = X_lawgd_traj_sdmd[:, :, t]\n",
    "    \n",
    "    # Evaluate kernel between current particles and training data (DM kernel)\n",
    "    K_curr = evaluate_kernel_at_points(X_curr, X_tar)  # (n_particles, n_data)\n",
    "    \n",
    "    # Project onto SDMD eigenmodes: c = Φ_SDMD^T @ K(x, X_tar)\n",
    "    eigvecs_selected = eigvecs_K[:, mode_start:mode_end]  # (n_data, k_modes)\n",
    "    c = eigvecs_selected.T @ K_curr.T  # (k_modes, n_particles)\n",
    "    \n",
    "    # Apply inverse generator: c_inv = Λ_gen^{-1} @ c\n",
    "    c_inv = lambda_gen_inv_selected[:, None] * c  # (k_modes, n_particles)\n",
    "    \n",
    "    # Project back to data space: f = Φ_SDMD @ c_inv\n",
    "    f_inv = eigvecs_selected @ c_inv  # (n_data, n_particles)\n",
    "    \n",
    "    # Compute gradient using DM kernel gradient\n",
    "    grad_K = compute_kernel_gradient(X_curr, X_tar)  # (n_particles, n_data, 2)\n",
    "    \n",
    "    # Gradient update: ∇_x f = Σ_i f_inv[i] * ∇_x k(x, x_i)\n",
    "    grad_update = np.zeros((n_particles, 2))\n",
    "    for d_idx in range(2):\n",
    "        grad_update[:, d_idx] = np.sum(\n",
    "            grad_K[:, :, d_idx] * f_inv.T.real,  # Use real part\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    # Gradient descent step\n",
    "    X_lawgd_traj_sdmd[:, :, t+1] = X_curr - h_lawgd * grad_update\n",
    "    \n",
    "    if (t+1) % 100 == 0 or t == 0:\n",
    "        print(f\"\\r  [SDMD] Iteration {t+1}/{n_iter_lawgd-1}  \", end='', flush=True)\n",
    "\n",
    "print()  # Print newline after loop\n",
    "print(\"LAWGD iteration complete!\")\n",
    "\n",
    "# Step 5: Compute final metrics\n",
    "dist_final_to_wells = np.array([np.linalg.norm(X_lawgd_traj_sdmd[:, :, -1] - center, axis=1) for center in well_centers])\n",
    "min_dist_final = np.min(dist_final_to_wells, axis=0)\n",
    "in_well_final = min_dist_final <= well_radius\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"LAWGD Results (SDMD Spectrum):\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Initial particles (outside anchors): {n_particles}\")\n",
    "print(f\"Final particles near anchors: {np.sum(in_well_final)} ({100*np.sum(in_well_final)/n_particles:.1f}%)\")\n",
    "print(f\"Final particles still outside: {n_particles - np.sum(in_well_final)} ({100*(n_particles - np.sum(in_well_final))/n_particles:.1f}%)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Summary:\")\n",
    "print(\"=\"*60)\n",
    "print(\"This LAWGD implementation uses:\")\n",
    "print(\"  ✓ SDMD spectrum (eigenvalues + eigenvectors from Cell 13)\")\n",
    "print(\"  ✓ DM kernel functions (evaluate_kernel_at_points, compute_kernel_gradient)\")\n",
    "print(\"  ✓ DM normalization parameters (epsilon, p_x, D_y from Cell 18)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2789878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualization 1: Initial vs Final Positions (Koopman(SDMD))\n",
    "# ============================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "\n",
    "ax.contourf(X_grid, Y_grid, density_grid, levels=30, cmap='Blues', alpha=0.7)\n",
    "ax.scatter(X_tar[:, 0], X_tar[:, 1], s=3, c='lightgray', alpha=0.2, label='Training data')\n",
    "ax.scatter(X_lawgd_traj_sdmd[:, 0, 0], X_lawgd_traj_sdmd[:, 1, 0],\n",
    "           s=25, c='red', marker='o', label='Initial (outside anchors)', zorder=5)\n",
    "ax.scatter(X_lawgd_traj_sdmd[:, 0, -1], X_lawgd_traj_sdmd[:, 1, -1],\n",
    "           s=35, facecolors='none', edgecolors='magenta', linewidth=1.5,\n",
    "           label='Final', zorder=15)\n",
    "\n",
    "for idx, center in enumerate(well_centers):\n",
    "    circle = Circle(center, well_radius, fill=False, edgecolor='green',\n",
    "                   linewidth=2, linestyle='--')\n",
    "    ax.add_patch(circle)\n",
    "\n",
    "ax.scatter(well_centers[:, 0], well_centers[:, 1], s=100, c='green',\n",
    "           marker='*', zorder=10)\n",
    "ax.set_xlabel('Latent dim 1', fontsize=12)\n",
    "ax.set_ylabel('Latent dim 2', fontsize=12)\n",
    "ax.set_title('KSWGD: Initial vs Final Positions', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim(latent_bounds[0])\n",
    "ax.set_ylim(latent_bounds[1])\n",
    "ax.set_aspect('equal')\n",
    "ax.legend(loc='upper right', fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd516f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualization 2: Trajectories (Koopman(SDMD))\n",
    "# ============================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "\n",
    "ax.contourf(X_grid, Y_grid, density_grid, levels=30, cmap='Blues', alpha=0.7)\n",
    "\n",
    "n_show_traj = min(15, n_particles)\n",
    "for i in range(n_show_traj):\n",
    "    ax.plot(X_lawgd_traj_sdmd[i, 0, :], X_lawgd_traj_sdmd[i, 1, :],\n",
    "            alpha=0.5, linewidth=1, color='gray')\n",
    "\n",
    "ax.scatter(X_lawgd_traj_sdmd[:n_show_traj, 0, 0], X_lawgd_traj_sdmd[:n_show_traj, 1, 0],\n",
    "           s=40, c='red', marker='o', zorder=5, label='Start')\n",
    "ax.scatter(X_lawgd_traj_sdmd[:n_show_traj, 0, -1], X_lawgd_traj_sdmd[:n_show_traj, 1, -1],\n",
    "           s=50, facecolors='none', edgecolors='magenta',\n",
    "           linewidth=1.5, zorder=15, label='End')\n",
    "\n",
    "for idx, center in enumerate(well_centers):\n",
    "    circle = Circle(center, well_radius, fill=False, edgecolor='green',\n",
    "                   linewidth=2, linestyle='--')\n",
    "    ax.add_patch(circle)\n",
    "\n",
    "ax.scatter(well_centers[:, 0], well_centers[:, 1], s=100, c='green',\n",
    "           marker='*', zorder=10)\n",
    "ax.set_xlabel('Latent dim 1', fontsize=12)\n",
    "ax.set_ylabel('Latent dim 2', fontsize=12)\n",
    "ax.set_title(f'KSWGD Trajectories (first {n_show_traj} particles)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xlim(latent_bounds[0])\n",
    "ax.set_ylim(latent_bounds[1])\n",
    "ax.set_aspect('equal')\n",
    "ax.legend(loc='upper right', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49764d28",
   "metadata": {},
   "source": [
    "## Convergence Analysis: Koopman(SDMD) with Metrics Tracking (IID Data)\n",
    "\n",
    "Run Koopman(SDMD) for 1000 steps and track convergence metrics every 100 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e74da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Koopman(SDMD) with Metrics Tracking (0-1000 steps, every 100 steps)\n",
    "# ============================================================\n",
    "\n",
    "from matplotlib.patches import Circle\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Koopman(SDMD): Running 1000 iterations with metrics tracking...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Prepare SDMD Koopman spectrum (reuse from previous cell)\n",
    "eigvals_K_sdmd_analysis = eigvals_K_sdmd.copy()\n",
    "eigvecs_K_sdmd_analysis = eigvecs_K_sdmd.copy()\n",
    "lambda_gen_sdmd_analysis = lambda_gen_full_sdmd.copy()\n",
    "\n",
    "# Mode selection (same as before)\n",
    "n_skip = 1\n",
    "eig_threshold = 0.01\n",
    "eigvals_after_skip = eigvals_K_sdmd_analysis[n_skip:]\n",
    "valid_mask = eigvals_after_skip.real > eig_threshold\n",
    "n_valid = np.sum(valid_mask)\n",
    "\n",
    "k_modes_sdmd = n_valid\n",
    "mode_start_sdmd = n_skip\n",
    "mode_end_sdmd = mode_start_sdmd + k_modes_sdmd\n",
    "\n",
    "print(f\"SDMD modes: {k_modes_sdmd} (from {mode_start_sdmd+1} to {mode_end_sdmd})\")\n",
    "\n",
    "# Build inverse generator weights\n",
    "lambda_gen_selected_sdmd = lambda_gen_sdmd_analysis[mode_start_sdmd:mode_end_sdmd]\n",
    "tol_gen_sdmd = 1e-6\n",
    "lambda_gen_inv_selected_sdmd = np.zeros(k_modes_sdmd, dtype=complex)\n",
    "mask_nonzero_sdmd = np.abs(lambda_gen_selected_sdmd) > tol_gen_sdmd\n",
    "lambda_gen_inv_selected_sdmd[mask_nonzero_sdmd] = 1.0 / lambda_gen_selected_sdmd[mask_nonzero_sdmd]\n",
    "\n",
    "# Step 2: Setup parameters\n",
    "n_particles = X_lawgd_init.shape[0]\n",
    "n_iter_lawgd = 1000\n",
    "h_lawgd = 1  # Step size\n",
    "record_interval = 100  # Record metrics every 100 steps\n",
    "n_records = n_iter_lawgd // record_interval + 1  # Include step 0\n",
    "\n",
    "# Initialize trajectory storage\n",
    "X_lawgd_traj_sdmd = np.zeros((n_particles, 2, n_iter_lawgd + 1))\n",
    "X_lawgd_traj_sdmd[:, :, 0] = X_lawgd_init.copy()\n",
    "\n",
    "# Initialize metrics storage\n",
    "metrics_sdmd = {\n",
    "    'steps': [],\n",
    "    'well_coverage': [],  # Percentage of particles in wells\n",
    "    'avg_potential': [],  # Average potential energy\n",
    "    'movement_rate': []   # Average displacement per step\n",
    "}\n",
    "\n",
    "print(f\"Particles: {n_particles}\")\n",
    "print(f\"Iterations: {n_iter_lawgd}\")\n",
    "print(f\"Recording interval: {record_interval} steps\")\n",
    "print(f\"Total records: {n_records}\")\n",
    "\n",
    "# Record initial state (step 0)\n",
    "coverage_0, avg_V_0, _ = compute_metrics(X_lawgd_traj_sdmd[:, :, 0])\n",
    "metrics_sdmd['steps'].append(0)\n",
    "metrics_sdmd['well_coverage'].append(coverage_0)\n",
    "metrics_sdmd['avg_potential'].append(avg_V_0)\n",
    "metrics_sdmd['movement_rate'].append(0.0)\n",
    "\n",
    "print(f\"\\nInitial state (step 0):\")\n",
    "print(f\"  Well coverage: {coverage_0:.2f}%\")\n",
    "print(f\"  Avg potential: {avg_V_0:.4f}\")\n",
    "\n",
    "# Step 3: Run LAWGD iterations with SDMD spectrum\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Running Koopman(SDMD) iterations...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for t in range(n_iter_lawgd):\n",
    "    X_curr = X_lawgd_traj_sdmd[:, :, t]\n",
    "    \n",
    "    # Evaluate kernel between current particles and training data (DM kernel)\n",
    "    K_curr = evaluate_kernel_at_points(X_curr, X_tar)  # (n_particles, n_data)\n",
    "    \n",
    "    # Project onto SDMD eigenmodes: c = Φ_SDMD^T @ K(x, X_tar)\n",
    "    eigvecs_selected_sdmd = eigvecs_K_sdmd_analysis[:, mode_start_sdmd:mode_end_sdmd]  # (n_data, k_modes)\n",
    "    c = eigvecs_selected_sdmd.T @ K_curr.T  # (k_modes, n_particles)\n",
    "    \n",
    "    # Apply inverse generator: c_inv = Λ_gen^{-1} @ c\n",
    "    c_inv = lambda_gen_inv_selected_sdmd[:, None] * c  # (k_modes, n_particles)\n",
    "    \n",
    "    # Project back to data space: f = Φ_SDMD @ c_inv\n",
    "    f_inv = eigvecs_selected_sdmd @ c_inv  # (n_data, n_particles)\n",
    "    \n",
    "    # Compute gradient using DM kernel gradient\n",
    "    grad_K = compute_kernel_gradient(X_curr, X_tar)  # (n_particles, n_data, 2)\n",
    "    \n",
    "    # Gradient update: ∇_x f = Σ_i f_inv[i] * ∇_x k(x, x_i)\n",
    "    grad_update = np.zeros((n_particles, 2))\n",
    "    for d_idx in range(2):\n",
    "        grad_update[:, d_idx] = np.sum(\n",
    "            grad_K[:, :, d_idx] * f_inv.T.real,  # Use real part\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    # Gradient descent step\n",
    "    X_lawgd_traj_sdmd[:, :, t+1] = X_curr - h_lawgd * grad_update\n",
    "    \n",
    "    # Record metrics every record_interval steps\n",
    "    if (t + 1) % record_interval == 0:\n",
    "        X_prev = X_lawgd_traj_sdmd[:, :, t]\n",
    "        X_next = X_lawgd_traj_sdmd[:, :, t+1]\n",
    "        coverage, avg_V, movement = compute_metrics(X_next, X_prev)\n",
    "        \n",
    "        metrics_sdmd['steps'].append(t + 1)\n",
    "        metrics_sdmd['well_coverage'].append(coverage)\n",
    "        metrics_sdmd['avg_potential'].append(avg_V)\n",
    "        metrics_sdmd['movement_rate'].append(movement)\n",
    "    \n",
    "    # Progress indicator\n",
    "    if (t+1) % 200 == 0 or t == 0:\n",
    "        print(f\"\\r  [SDMD] Iteration {t+1}/{n_iter_lawgd}  \", end='', flush=True)\n",
    "\n",
    "print()  # Newline after loop\n",
    "print(\"Koopman(SDMD) iteration complete!\")\n",
    "\n",
    "# Step 4: Final summary\n",
    "final_coverage = metrics_sdmd['well_coverage'][-1]\n",
    "final_potential = metrics_sdmd['avg_potential'][-1]\n",
    "final_movement = metrics_sdmd['movement_rate'][-1]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Koopman(SDMD) Final Results:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Final well coverage: {final_coverage:.2f}%\")\n",
    "print(f\"Final avg potential: {final_potential:.4f}\")\n",
    "print(f\"Final movement rate: {final_movement:.6f}\")\n",
    "print(f\"Total records: {len(metrics_sdmd['steps'])}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Convert metrics to numpy arrays for easier plotting\n",
    "for key in ['steps', 'well_coverage', 'avg_potential', 'movement_rate']:\n",
    "    metrics_sdmd[key] = np.array(metrics_sdmd[key])\n",
    "\n",
    "print(\"\\n✓ Koopman(SDMD) metrics saved in 'metrics_sdmd' dictionary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5afd8b4",
   "metadata": {},
   "source": [
    "## Visual Inspection of Generated Digits\n",
    "\n",
    "Instead of potential-well coverage curves, we decode the final LATENT particles from DMPS and Koopman(SDMD) back to 28×28 pixel space and check whether the synthesized digits resemble the MNIST targets. Each column shows a generated digit (top) and its nearest neighbor among the latent training samples (bottom)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3646ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Generated Digit Grid: DMPS final particles\n",
    "# ============================================================\n",
    "\n",
    "def _decode_latents(latent_batch):\n",
    "    if latent_batch.ndim == 1:\n",
    "        latent_batch = latent_batch[None, :]\n",
    "    return mnist_pipeline.inverse_transform(latent_batch)\n",
    "\n",
    "def _nearest_latent_neighbors(latent_batch, reference_latent):\n",
    "    diff = latent_batch[:, None, :] - reference_latent[None, :, :]\n",
    "    dist = np.linalg.norm(diff, axis=2)\n",
    "    nearest_idx = np.argmin(dist, axis=1)\n",
    "    return reference_latent[nearest_idx]\n",
    "\n",
    "def plot_generated_vs_reference(latent_batch, title, reference_latent=X_tar, n_cols=8):\n",
    "    if latent_batch.ndim == 3:\n",
    "        latent_final = latent_batch[:, :, -1]\n",
    "    else:\n",
    "        latent_final = latent_batch\n",
    "\n",
    "    n_cols = min(n_cols, latent_final.shape[0])\n",
    "    if n_cols == 0:\n",
    "        raise ValueError(\"No latent samples available for visualization.\")\n",
    "\n",
    "    sample_idx = np.random.choice(latent_final.shape[0], size=n_cols, replace=False)\n",
    "    latent_samples = latent_final[sample_idx]\n",
    "\n",
    "    generated_imgs = _decode_latents(latent_samples)\n",
    "    reference_latent_batch = _nearest_latent_neighbors(latent_samples, reference_latent)\n",
    "    reference_imgs = _decode_latents(reference_latent_batch)\n",
    "\n",
    "    fig, axes = plt.subplots(2, n_cols, figsize=(1.6 * n_cols, 3.6))\n",
    "    for col in range(n_cols):\n",
    "        axes[0, col].imshow(generated_imgs[col], cmap='gray')\n",
    "        axes[0, col].set_title(f'gen #{col+1}', fontsize=9)\n",
    "        axes[0, col].axis('off')\n",
    "        axes[1, col].imshow(reference_imgs[col], cmap='gray')\n",
    "        axes[1, col].set_title('ref', fontsize=8, pad=2)\n",
    "        axes[1, col].axis('off')\n",
    "\n",
    "    fig.suptitle(title + \"\\nTop: generated | Bottom: nearest training digit\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_generated_vs_reference(X_lawgd_traj_dm[:, :, -1], \"DMPS final decoded digits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3532c52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Generated Digit Grid: Koopman(SDMD) final particles\n",
    "# ============================================================\n",
    "\n",
    "plot_generated_vs_reference(\n",
    "    X_lawgd_traj_sdmd[:, :, -1],\n",
    "    \"Koopman(SDMD) final decoded digits\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e07089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Pixel-space quality diagnostics\n",
    "# ============================================================\n",
    "\n",
    "def compute_pixel_stats(latent_batch, label, reference_latent=X_tar, n_eval=128):\n",
    "    if latent_batch.ndim == 3:\n",
    "        latent_final = latent_batch[:, :, -1]\n",
    "    else:\n",
    "        latent_final = latent_batch\n",
    "\n",
    "    n_eval = min(n_eval, latent_final.shape[0])\n",
    "    idx = np.random.choice(latent_final.shape[0], size=n_eval, replace=False)\n",
    "    samples = latent_final[idx]\n",
    "\n",
    "    generated_imgs = _decode_latents(samples)\n",
    "    reference_latent_batch = _nearest_latent_neighbors(samples, reference_latent)\n",
    "    reference_imgs = _decode_latents(reference_latent_batch)\n",
    "\n",
    "    mse = np.mean((generated_imgs - reference_imgs) ** 2)\n",
    "    mae = np.mean(np.abs(generated_imgs - reference_imgs))\n",
    "\n",
    "    return {\n",
    "        'method': label,\n",
    "        'n_eval': n_eval,\n",
    "        'mse': mse,\n",
    "        'mae': mae\n",
    "    }\n",
    "\n",
    "stats_dm = compute_pixel_stats(X_lawgd_traj_dm, 'DMPS')\n",
    "stats_sdmd = compute_pixel_stats(X_lawgd_traj_sdmd, 'Koopman(SDMD)')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PIXEL-SPACE QUALITY SUMMARY (lower is better)\")\n",
    "print(\"=\"*70)\n",
    "for stats in [stats_dm, stats_sdmd]:\n",
    "    print(f\"{stats['method']}: n_eval={stats['n_eval']}\")\n",
    "    print(f\"  • Mean squared error:     {stats['mse']:.6f}\")\n",
    "    print(f\"  • Mean absolute error:   {stats['mae']:.6f}\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
