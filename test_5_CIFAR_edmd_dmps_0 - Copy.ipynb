{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "967f0030",
   "metadata": {},
   "source": [
    "# Diffusion Maps + KSWGD Generative Modelling on CIFAR-10\n",
    "\n",
    "This notebook implements an autoencoder + diffusion-maps latent analysis, then mirrors the KSWGD scheme from Tests 1 & 2 to sample new CIFAR-10 images directly in latent space.\n",
    "\n",
    "**Dataset: CIFAR-10**\n",
    "- 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck\n",
    "- 32×32 RGB color images (3 channels)\n",
    "- 50,000 training / 10,000 test images\n",
    "\n",
    "**Pipeline:**\n",
    "\n",
    "1. Load CIFAR-10 and normalize images.\n",
    "2. Train an autoencoder (either MLP-based or CNN-based) to learn a low-dimensional latent space.\n",
    "3. Map training images to latent codes.\n",
    "4. Apply Diffusion Maps on latent codes to obtain diffusion coordinates for diagnostics.\n",
    "5. Build the KSWGD kernel operators on the latent samples (same normalization as Tests 1 & 2).\n",
    "6. Run KSWGD particle transport to draw new latent vectors.\n",
    "7. Decode KSWGD latent vectors back to images and visually inspect generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22b8d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure core libraries, plotting defaults, and compute device\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.decomposition import MiniBatchDictionaryLearning\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from scipy.sparse.linalg import eigs\n",
    "from scipy.linalg import eig\n",
    "from tqdm.auto import trange\n",
    "import diffusers\n",
    "from diffusers import AutoencoderKL, UNet2DModel, DDPMScheduler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "try:\n",
    "    from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "    TORCHMETRICS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    FrechetInceptionDistance = None\n",
    "    TORCHMETRICS_AVAILABLE = False\n",
    "\n",
    "from grad_ker1 import grad_ker1\n",
    "from K_tar_eval import K_tar_eval\n",
    "\n",
    "try:\n",
    "    import cupy as cp\n",
    "    from grad_ker1_gpu import grad_ker1 as grad_ker1_gpu\n",
    "    from K_tar_eval_gpu import K_tar_eval as K_tar_eval_gpu\n",
    "    GPU_AVAILABLE = True\n",
    "except Exception:\n",
    "    cp = None\n",
    "    grad_ker1_gpu = None\n",
    "    K_tar_eval_gpu = None\n",
    "    GPU_AVAILABLE = False\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_gpu_kswgd = bool(GPU_AVAILABLE and torch.cuda.is_available())\n",
    "\n",
    "print(f\"Diffusers version: {diffusers.__version__}\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"GPU Info\")\n",
    "print(\"=\" * 50)\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"cuDNN version: {torch.backends.cudnn.version()}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"GPU {i}: {props.name}, Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"No CUDA GPU available\")\n",
    "\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "print(f\"KSWGD GPU backend: {'enabled' if use_gpu_kswgd else 'disabled'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a060825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 datasets and build training/test data loaders\n",
    "batch_size = 128\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # [0, 1], shape: 3x32x32\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset  = datasets.CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(\"Train size:\", len(train_dataset), \"Test size:\", len(test_dataset))\n",
    "print(\"Number of classes:\", len(train_dataset.classes))\n",
    "print(\"Classes:\", train_dataset.classes)\n",
    "print(\"Image shape: 3x32x32 (RGB color)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a837b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained Stable Diffusion VAE and wrap it for CIFAR-10 usage\n",
    "vae_repo_id = \"stabilityai/sd-vae-ft-mse\"\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    vae_repo_id,\n",
    "    torch_dtype=torch.float32,\n",
    " )\n",
    "vae = vae.to(device)\n",
    "vae_scaling = float(getattr(vae.config, \"scaling_factor\", 0.18215))\n",
    "\n",
    "# Freeze the encoder except for the last down block (optional fine-tuning target)\n",
    "for param in vae.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in vae.encoder.down_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(\"Loaded VAE:\", vae_repo_id)\n",
    "print(\"Latent channels:\", vae.config.latent_channels)\n",
    "print(\"Scaling factor:\", vae_scaling)\n",
    "\n",
    "\n",
    "def _to_vae_range(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Map [0, 1] RGB tensors to [-1, 1] for the SD VAE.\"\"\"\n",
    "    return (x * 2.0) - 1.0\n",
    "\n",
    "\n",
    "def _from_vae_range(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Map [-1, 1] outputs back to [0, 1].\"\"\"\n",
    "    return torch.clamp((x + 1.0) * 0.5, 0.0, 1.0)\n",
    "\n",
    "\n",
    "class DiffusersAutoencoderWrapper(nn.Module):\n",
    "    \"\"\"Thin wrapper so downstream cells can keep using the `autoencoder` API.\"\"\"\n",
    "\n",
    "    def __init__(self, vae: AutoencoderKL, device: torch.device, image_size: int = 32):\n",
    "        super().__init__()\n",
    "        self.vae = vae\n",
    "        self.device = device\n",
    "        self.scaling_factor = float(getattr(vae.config, \"scaling_factor\", 0.18215))\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 3, image_size, image_size, device=device)\n",
    "            latent = self.vae.encode(_to_vae_range(dummy)).latent_dist.mode()\n",
    "        self.latent_shape = latent.shape[1:]\n",
    "        self.latent_dim = int(np.prod(self.latent_shape))\n",
    "\n",
    "    def encode_latents(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        posterior = self.vae.encode(_to_vae_range(images))\n",
    "        latents = posterior.latent_dist.mode() * self.scaling_factor\n",
    "        return latents\n",
    "\n",
    "    def decode_latents(self, latents: torch.Tensor) -> torch.Tensor:\n",
    "        decoded = self.vae.decode(latents / self.scaling_factor).sample\n",
    "        return _from_vae_range(decoded)\n",
    "\n",
    "    def encode(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        latents = self.encode_latents(images)\n",
    "        return latents.view(latents.size(0), -1)\n",
    "\n",
    "    def decode(self, flat_latents: torch.Tensor) -> torch.Tensor:\n",
    "        latents = flat_latents.view(-1, *self.latent_shape)\n",
    "        return self.decode_latents(latents)\n",
    "\n",
    "    def forward(self, images: torch.Tensor):\n",
    "        latents_flat = self.encode(images)\n",
    "        recon = self.decode(latents_flat)\n",
    "        return recon, latents_flat\n",
    "\n",
    "\n",
    "autoencoder = DiffusersAutoencoderWrapper(vae, device=device, image_size=32).to(device)\n",
    "autoencoder.eval()\n",
    "latent_dim = autoencoder.latent_dim\n",
    "print(f\"Autoencoder wrapper latent dim: {autoencoder.latent_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44edc742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional fine-tuning utilities for adapting the pretrained VAE to CIFAR-10\n",
    "\n",
    "def finetune_vae(\n",
    "    model,\n",
    "    dataloader,\n",
    "    epochs=5,\n",
    "    lr=1e-5,\n",
    "    max_batches=None,\n",
    "):\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    if not trainable_params:\n",
    "        print(\"No trainable parameters detected; skipping fine-tuning.\")\n",
    "        return\n",
    "\n",
    "    optimizer = torch.optim.Adam(trainable_params, lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        steps = 0\n",
    "        for step, (imgs, _) in enumerate(dataloader):\n",
    "            if max_batches is not None and step >= max_batches:\n",
    "                break\n",
    "            imgs = imgs.to(device)\n",
    "            latents = model.encode(_to_vae_range(imgs)).latent_dist.sample()\n",
    "            recon = model.decode(latents).sample\n",
    "            recon_rgb = _from_vae_range(recon)\n",
    "            loss = loss_fn(recon_rgb, imgs)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            steps += 1\n",
    "        avg_loss = running_loss / max(1, steps)\n",
    "        print(f\"[VAE FT] epoch {epoch + 1}/{epochs} | loss={avg_loss:.5f}\")\n",
    "    model.eval()\n",
    "    print(\"Finished VAE fine-tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5ceefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute (optional) VAE fine-tuning before reconstruction diagnostics\n",
    "do_finetune = False\n",
    "finetune_epochs = 5\n",
    "finetune_lr = 1e-5\n",
    "finetune_max_batches = None  # e.g., set to 200 for a quick pass\n",
    "\n",
    "if do_finetune:\n",
    "    print(f\"Fine-tuning VAE for {finetune_epochs} epochs at lr={finetune_lr}\")\n",
    "    finetune_vae(\n",
    "        vae,\n",
    "        train_loader,\n",
    "        epochs=finetune_epochs,\n",
    "        lr=finetune_lr,\n",
    "        max_batches=finetune_max_batches,\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping VAE fine-tuning (set do_finetune=True to enable).\")\n",
    "\n",
    "vae.eval()\n",
    "autoencoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc30f619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction diagnostics utilities for the pretrained VAE\n",
    "\n",
    "def evaluate_vae_reconstruction(\n",
    "    model: AutoencoderKL,\n",
    "    dataloader: DataLoader,\n",
    "    device: torch.device,\n",
    "    num_batches: int = 1,\n",
    "):\n",
    "    model.eval()\n",
    "    mse_loss = nn.MSELoss(reduction=\"mean\")\n",
    "    mse_values = []\n",
    "    originals = []\n",
    "    reconstructions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (imgs, _) in enumerate(dataloader):\n",
    "            if batch_idx >= num_batches:\n",
    "                break\n",
    "            imgs = imgs.to(device)\n",
    "            latents = model.encode(_to_vae_range(imgs)).latent_dist.mode() * vae_scaling\n",
    "            recon = model.decode(latents / vae_scaling).sample\n",
    "            recon_rgb = _from_vae_range(recon)\n",
    "            loss = mse_loss(recon_rgb, imgs)\n",
    "\n",
    "            mse_values.append(loss.item())\n",
    "            originals.append(imgs.cpu())\n",
    "            reconstructions.append(recon_rgb.cpu())\n",
    "\n",
    "    if not mse_values:\n",
    "        raise RuntimeError(\"No batches were evaluated; increase num_batches.\")\n",
    "\n",
    "    mean_mse = float(np.mean(mse_values))\n",
    "    psnr = -10.0 * np.log10(mean_mse + 1e-8)\n",
    "\n",
    "    originals = torch.cat(originals, dim=0)\n",
    "    reconstructions = torch.cat(reconstructions, dim=0)\n",
    "\n",
    "    return {\n",
    "        \"mse\": mean_mse,\n",
    "        \"psnr\": psnr,\n",
    "        \"originals\": originals,\n",
    "        \"reconstructions\": reconstructions,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9e564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate reconstructions (Cell 8) and visualize originals vs. reconstructions\n",
    "num_eval_batches = 2\n",
    "recon_report = evaluate_vae_reconstruction(\n",
    "    vae,\n",
    "    test_loader,\n",
    "    device=device,\n",
    "    num_batches=num_eval_batches,\n",
    ")\n",
    "\n",
    "orig_samples = recon_report[\"originals\"][:16]\n",
    "recon_samples = recon_report[\"reconstructions\"][:16]\n",
    "comparison = torch.cat([orig_samples, recon_samples], dim=0)\n",
    "\n",
    "grid = make_grid(comparison, nrow=16)\n",
    "plt.figure(figsize=(16, 2.5))\n",
    "plt.imshow(np.clip(grid.permute(1, 2, 0).numpy(), 0.0, 1.0))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Top: original CIFAR-10 | Bottom: VAE reconstruction\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "mse = recon_report[\"mse\"]\n",
    "psnr = recon_report[\"psnr\"]\n",
    "print(f\"Reconstruction MSE={mse:.6f} | PSNR={psnr:.2f} dB\")\n",
    "quality_flag = \"PASS\" if mse < 3e-3 else \"TUNE\"\n",
    "print(f\"Status: {quality_flag} (threshold=0.003)\")\n",
    "\n",
    "if TORCHMETRICS_AVAILABLE and FrechetInceptionDistance is not None:\n",
    "    fid_metric = FrechetInceptionDistance(feature=2048, normalize=True).to(device)\n",
    "    real_batch = recon_report[\"originals\"].to(device)\n",
    "    fake_batch = recon_report[\"reconstructions\"].to(device)\n",
    "    fid_metric.update(real_batch, real=True)\n",
    "    fid_metric.update(fake_batch, real=False)\n",
    "    fid_score = float(fid_metric.compute().cpu())\n",
    "    print(f\"Frechet Inception Distance (orig vs recon) = {fid_score:.3f}\")\n",
    "else:\n",
    "    print(\"Frechet Inception Distance unavailable (install torchmetrics>=1.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c66079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the full training set to collect latent vectors and labels for DM\n",
    "autoencoder.eval()\n",
    "\n",
    "all_latents = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        _, z = autoencoder(images)\n",
    "        all_latents.append(z.cpu().numpy())\n",
    "        all_labels.append(labels.numpy())\n",
    "\n",
    "Z = np.concatenate(all_latents, axis=0)   # shape (N, latent_dim)\n",
    "y_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "print(\"Latent codes shape:\", Z.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc211ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the latent sample subset and kernel matrix required for diffusion maps\n",
    "# Subsample for Diffusion Maps (e.g., 5000 points) with stratified sampling to keep label balance\n",
    "max_dm_samples = 20000\n",
    "N_total = Z.shape[0]\n",
    "\n",
    "if N_total > max_dm_samples:\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, train_size=max_dm_samples, random_state=42)\n",
    "    idx_dm, _ = next(splitter.split(Z, y_labels))\n",
    "else:\n",
    "    idx_dm = np.arange(N_total)\n",
    "\n",
    "Z_dm_raw = Z[idx_dm]\n",
    "labels_dm = y_labels[idx_dm]\n",
    "\n",
    "# ========== STANDARDIZE latent codes for DM ==========\n",
    "# This ensures all dimensions have similar scale for proper kernel computation\n",
    "Z_dm_mean = np.mean(Z_dm_raw, axis=0, keepdims=True)\n",
    "Z_dm_std = np.std(Z_dm_raw, axis=0, keepdims=True) + 1e-8\n",
    "Z_dm = (Z_dm_raw - Z_dm_mean) / Z_dm_std  # Standardized latent codes\n",
    "print(f\"Standardized Z_dm: mean~{Z_dm.mean():.4f}, std~{Z_dm.std():.4f}\")\n",
    "# =====================================================\n",
    "\n",
    "unique_labels, label_counts = np.unique(labels_dm, return_counts=True)\n",
    "label_hist = {int(lbl): int(cnt) for lbl, cnt in zip(unique_labels, label_counts)}\n",
    "print(\"Latent points used for DM:\", Z_dm.shape)\n",
    "print(\"Label histogram in DM subset:\", label_hist)\n",
    "\n",
    "# Pairwise distances (now on standardized data)\n",
    "dists = pairwise_distances(Z_dm, metric=\"euclidean\")\n",
    "\n",
    "# Epsilon via median heuristic (can adjust multiplier for better separation)\n",
    "med_sq = np.median(dists**2)\n",
    "eps = med_sq / (2.0 * np.log(Z_dm.shape[0]))\n",
    "# Alternative: use a smaller epsilon for tighter clustering\n",
    "# eps = np.percentile(dists**2, 10)  # 10th percentile for local structure\n",
    "print(\"Chosen epsilon:\", eps)\n",
    "print(f\"Distance stats: min={dists[dists>0].min():.4f}, median={np.median(dists):.4f}, max={dists.max():.4f}\")\n",
    "\n",
    "# Gaussian kernel\n",
    "K = np.exp(-dists**2 / (2.0 * eps))\n",
    "\n",
    "# Row-normalize -> Markov matrix P\n",
    "row_sums = K.sum(axis=1, keepdims=True)\n",
    "P = K / row_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b22280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the leading diffusion-map eigenpairs to obtain coordinates and spectra\n",
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "m = 32  # number of eigenvectors (increase for richer Koopman payload)\n",
    "t_diffusion = 1  # diffusion time for scaling coordinates\n",
    "\n",
    "# Create progress bar widget\n",
    "progress_bar = widgets.FloatProgress(\n",
    "    value=0, min=0, max=100, \n",
    "    description='Computing:', \n",
    "    bar_style='info',\n",
    "    style={'bar_color': '#00a0dc', 'description_width': '80px'},\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "status_label = widgets.HTML(value=\"<b>Starting eigenvalue decomposition...</b>\")\n",
    "time_label = widgets.HTML(value=\"\")\n",
    "display(widgets.VBox([progress_bar, status_label, time_label]))\n",
    "\n",
    "# Estimate computation based on matrix size\n",
    "n_samples = P.shape[0]\n",
    "print(f\"Matrix size: {n_samples} x {n_samples}, computing {m} eigenvectors...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Update progress - phase 1: preparation\n",
    "progress_bar.value = 10\n",
    "status_label.value = \"<b>Phase 1/3:</b> Preparing matrix transpose...\"\n",
    "time.sleep(0.1)\n",
    "\n",
    "# Compute eigenvalues with progress updates\n",
    "progress_bar.value = 20\n",
    "status_label.value = \"<b>Phase 2/3:</b> Computing eigenvalue decomposition (this may take a while)...\"\n",
    "\n",
    "# The actual computation\n",
    "vals, vecs = eigs(P.T, k=m, which=\"LR\")\n",
    "\n",
    "# Update progress - phase 3: post-processing\n",
    "progress_bar.value = 80\n",
    "elapsed = time.time() - start_time\n",
    "status_label.value = \"<b>Phase 3/3:</b> Sorting and scaling eigenvectors...\"\n",
    "time_label.value = f\"<i>Eigenvalue computation took: {elapsed:.1f} seconds</i>\"\n",
    "\n",
    "idx_sort = np.argsort(-np.abs(vals))\n",
    "vals = vals[idx_sort]\n",
    "vecs = vecs[:, idx_sort]\n",
    "\n",
    "lam = np.real(vals)\n",
    "phi_raw = np.real(vecs)\n",
    "\n",
    "# Scale diffusion coordinates by eigenvalues^t (standard Diffusion Maps embedding)\n",
    "# This amplifies the coordinates to show structure at different scales\n",
    "phi = phi_raw * (lam[np.newaxis, :] ** t_diffusion)\n",
    "\n",
    "# Finalize progress\n",
    "progress_bar.value = 100\n",
    "progress_bar.bar_style = 'success'\n",
    "total_time = time.time() - start_time\n",
    "status_label.value = \"<b style='color: green;'>✓ Completed!</b>\"\n",
    "time_label.value = f\"<i>Total time: {total_time:.1f} seconds</i>\"\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Leading eigenvalues:\", lam)\n",
    "print(f\"Diffusion coords range: [{phi[:, 1:].min():.4f}, {phi[:, 1:].max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffe9af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot several diffusion-coordinate pairs with GMM outlier filtering (keep ~90% of points)\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# GMM outlier filtering parameters\n",
    "n_gmm_components_2d = 10  # number of GMM components\n",
    "outlier_percentile = 15   # remove bottom 10% by GMM log-likelihood (keep 90%)\n",
    "\n",
    "pairs = [(1, 2), (1, 3), (2, 3)]\n",
    "for i, j in pairs:\n",
    "    dc_i = phi[:, i]\n",
    "    dc_j = phi[:, j]\n",
    "    coords_2d = np.column_stack([dc_i, dc_j])\n",
    "    \n",
    "    # Fit GMM and compute log-likelihood scores\n",
    "    gmm_2d = GaussianMixture(n_components=n_gmm_components_2d, covariance_type='full', random_state=42)\n",
    "    gmm_2d.fit(coords_2d)\n",
    "    log_probs_2d = gmm_2d.score_samples(coords_2d)\n",
    "    \n",
    "    # Keep top 90% by log-likelihood (filter out bottom 10% outliers)\n",
    "    threshold_2d = np.percentile(log_probs_2d, outlier_percentile)\n",
    "    inlier_mask_2d = log_probs_2d >= threshold_2d\n",
    "    \n",
    "    n_total = len(dc_i)\n",
    "    n_kept = np.sum(inlier_mask_2d)\n",
    "    \n",
    "    plt.figure(figsize=(6, 5))\n",
    "    scatter = plt.scatter(dc_i[inlier_mask_2d], dc_j[inlier_mask_2d], \n",
    "                          c=labels_dm[inlier_mask_2d], s=5, alpha=0.6, cmap=\"tab10\")\n",
    "    plt.xlabel(f\"Diffusion Map coord {i}\")\n",
    "    plt.ylabel(f\"Diffusion Map coord {j}\")\n",
    "    plt.title(f\"DC{i} vs DC{j} (GMM filtered: {n_kept}/{n_total}, {100*n_kept/n_total:.1f}%)\")\n",
    "    plt.colorbar(scatter, label=\"CIFAR-10 class\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0143306c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D diffusion-coordinate visualization with GMM outlier filtering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# ============== RESCALE PARAMETER ==============\n",
    "# Increase this value to spread clusters further apart\n",
    "# Typical values: 1.0 (no change), 2.0-5.0 (moderate spread), 5.0-10.0 (large spread)\n",
    "SPREAD_FACTOR = 1.5  # Slightly spread CIFAR clusters\n",
    "# ===============================================\n",
    "\n",
    "# Use first 3 nontrivial diffusion coordinates for 3D plot\n",
    "# Scale by eigenvalues to get proper diffusion distances (standard practice)\n",
    "dc_1_raw = phi[:, 1] * lam[1]\n",
    "dc_2_raw = phi[:, 2] * lam[2]\n",
    "dc_3_raw = phi[:, 3] * lam[3]\n",
    "coords_3d_raw = np.column_stack([dc_1_raw, dc_2_raw, dc_3_raw])\n",
    "\n",
    "print(f\"Raw coordinate ranges (before filtering):\")\n",
    "print(f\"  DC1: [{dc_1_raw.min():.4f}, {dc_1_raw.max():.4f}]\")\n",
    "print(f\"  DC2: [{dc_2_raw.min():.4f}, {dc_2_raw.max():.4f}]\")\n",
    "print(f\"  DC3: [{dc_3_raw.min():.4f}, {dc_3_raw.max():.4f}]\")\n",
    "\n",
    "# Step 1: First pass - use IQR to remove extreme outliers before GMM\n",
    "def iqr_filter(data, factor=3.0):\n",
    "    \"\"\"Remove points outside factor*IQR from Q1/Q3\"\"\"\n",
    "    q1 = np.percentile(data, 25, axis=0)\n",
    "    q3 = np.percentile(data, 75, axis=0)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - factor * iqr\n",
    "    upper = q3 + factor * iqr\n",
    "    mask = np.all((data >= lower) & (data <= upper), axis=1)\n",
    "    return mask\n",
    "\n",
    "iqr_mask = iqr_filter(coords_3d_raw, factor=2.5)\n",
    "print(f\"\\nIQR pre-filter: removed {np.sum(~iqr_mask)} extreme points ({100*np.sum(~iqr_mask)/len(iqr_mask):.1f}%)\")\n",
    "\n",
    "# Step 2: Fit GMM on IQR-filtered data for finer outlier detection\n",
    "coords_filtered = coords_3d_raw[iqr_mask]\n",
    "n_gmm_components = 12\n",
    "gmm = GaussianMixture(n_components=n_gmm_components, covariance_type='full', random_state=42)\n",
    "gmm.fit(coords_filtered)\n",
    "\n",
    "# Score ALL points with the GMM trained on filtered data\n",
    "log_probs_all = gmm.score_samples(coords_3d_raw)\n",
    "# Use a more aggressive threshold: bottom 10% of GMM scores\n",
    "threshold = np.percentile(log_probs_all[iqr_mask], 10)\n",
    "gmm_mask = log_probs_all >= threshold\n",
    "\n",
    "# Combine both filters\n",
    "inlier_mask = iqr_mask & gmm_mask\n",
    "\n",
    "print(f\"GMM filter: removed additional {np.sum(iqr_mask & ~gmm_mask)} points\")\n",
    "print(f\"Total outliers removed: {np.sum(~inlier_mask)} ({100*np.sum(~inlier_mask)/len(inlier_mask):.1f}%)\")\n",
    "print(f\"Inliers kept: {np.sum(inlier_mask)}\")\n",
    "\n",
    "# Step 3: Standardize inliers, then apply SPREAD_FACTOR to increase separation\n",
    "scaler = RobustScaler()  # More robust to remaining outliers\n",
    "coords_inliers = coords_3d_raw[inlier_mask]\n",
    "coords_scaled = scaler.fit_transform(coords_inliers)\n",
    "\n",
    "# Apply spread factor: multiply by SPREAD_FACTOR to increase inter-cluster distances\n",
    "coords_spread = coords_scaled * SPREAD_FACTOR\n",
    "\n",
    "dc_1_plot, dc_2_plot, dc_3_plot = coords_spread[:, 0], coords_spread[:, 1], coords_spread[:, 2]\n",
    "\n",
    "print(f\"\\nFiltered & scaled coordinate ranges (SPREAD_FACTOR={SPREAD_FACTOR}):\")\n",
    "print(f\"  DC1: [{dc_1_plot.min():.3f}, {dc_1_plot.max():.3f}]\")\n",
    "print(f\"  DC2: [{dc_2_plot.min():.3f}, {dc_2_plot.max():.3f}]\")\n",
    "print(f\"  DC3: [{dc_3_plot.min():.3f}, {dc_3_plot.max():.3f}]\")\n",
    "\n",
    "# Plot 3D with inliers only (properly filtered and spread)\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    dc_1_plot, dc_2_plot, dc_3_plot,\n",
    "    c=labels_dm[inlier_mask], s=8, alpha=0.6, cmap=\"tab10\"\n",
    ")\n",
    "ax.set_xlabel(\"DC 1 (scaled)\")\n",
    "ax.set_ylabel(\"DC 2 (scaled)\")\n",
    "ax.set_zlabel(\"DC 3 (scaled)\")\n",
    "ax.set_title(f\"3D Diffusion Map (spread={SPREAD_FACTOR}x, {np.sum(inlier_mask)}/{len(inlier_mask)} points)\")\n",
    "fig.colorbar(scatter, ax=ax, label=\"CIFAR-10 class\", shrink=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Also show where outliers were (using same scaling)\n",
    "scaler_all = RobustScaler().fit(coords_3d_raw[inlier_mask])\n",
    "coords_all_scaled = scaler_all.transform(coords_3d_raw) * SPREAD_FACTOR\n",
    "dc_1_all, dc_2_all, dc_3_all = coords_all_scaled[:, 0], coords_all_scaled[:, 1], coords_all_scaled[:, 2]\n",
    "\n",
    "fig2 = plt.figure(figsize=(10, 8))\n",
    "ax2 = fig2.add_subplot(111, projection='3d')\n",
    "ax2.scatter(\n",
    "    dc_1_all[inlier_mask], dc_2_all[inlier_mask], dc_3_all[inlier_mask],\n",
    "    c=labels_dm[inlier_mask], s=8, alpha=0.5, cmap=\"tab10\", label=\"Inliers\"\n",
    ")\n",
    "ax2.scatter(\n",
    "    dc_1_all[~inlier_mask], dc_2_all[~inlier_mask], dc_3_all[~inlier_mask],\n",
    "    c='red', s=25, alpha=0.9, marker='x', label=f\"Outliers ({np.sum(~inlier_mask)})\"\n",
    ")\n",
    "ax2.set_xlabel(\"DC 1\")\n",
    "ax2.set_ylabel(\"DC 2\")\n",
    "ax2.set_zlabel(\"DC 3\")\n",
    "ax2.set_title(\"3D Diffusion Map (outliers shown in red)\")\n",
    "ax2.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbdc150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute diffusion-space mean and covariance for Gaussian diagnostics\n",
    "num_dc = 16  # number of nontrivial diffusion coords to use (<= m-1)\n",
    "Y_dm = phi[:, 1:1 + num_dc]\n",
    "print(\"Diffusion-space data shape:\", Y_dm.shape)\n",
    "\n",
    "mu_Y = np.mean(Y_dm, axis=0)\n",
    "cov_Y = np.cov(Y_dm.T)\n",
    "\n",
    "print(\"Mean in diffusion space:\", mu_Y)\n",
    "print(\"Covariance shape:\", cov_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6ac4c0",
   "metadata": {},
   "source": [
    "## KSWGD-based latent generation\n",
    "Following the KSWGD workflow from Tests 1 & 2, we now treat the autoencoder latents as the target samples and run Wasserstein gradient dynamics directly in latent space. Diffusion maps remain available above for visualization, but the sampling path below no longer uses the Gaussian model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921f3cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize DM latents and build the KSWGD kernel operators\n",
    "# NOTE: Z_dm is already standardized in Cell 11, so we use it directly\n",
    "X_tar = Z_dm.astype(np.float64)  # target samples for KSWGD (already standardized)\n",
    "sq_tar = np.sum(X_tar ** 2, axis=1)\n",
    "\n",
    "kswgd_dists = pairwise_distances(X_tar, metric=\"euclidean\")\n",
    "eps_kswgd = np.median(kswgd_dists**2) / (2.0 * np.log(X_tar.shape[0] + 1))\n",
    "eps_kswgd = float(max(eps_kswgd, 1e-6))\n",
    "data_kernel = np.exp(-kswgd_dists**2 / (2.0 * eps_kswgd))\n",
    "\n",
    "p_x = np.sqrt(np.sum(data_kernel, axis=1))\n",
    "data_kernel_norm = data_kernel / (p_x[:, None] * p_x[None, :] + 1e-12)\n",
    "D_y = np.sum(data_kernel_norm, axis=0)\n",
    "rw_kernel = 0.5 * (data_kernel_norm / (D_y + 1e-12) + data_kernel_norm / (D_y[:, None] + 1e-12))\n",
    "# rw_kernel = data_kernel_norm / (D_y[:, None] + 1e-12)\n",
    "rw_kernel = np.nan_to_num(rw_kernel)\n",
    "\n",
    "print(\"KSWGD target shape:\", X_tar.shape)\n",
    "print(\"KSWGD epsilon:\", eps_kswgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f941f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build spectral quantities and KSWGD weights\n",
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Create progress bar widget\n",
    "progress_bar = widgets.FloatProgress(\n",
    "    value=0, min=0, max=100, \n",
    "    description='Computing:', \n",
    "    bar_style='info',\n",
    "    style={'bar_color': '#00a0dc', 'description_width': '80px'},\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "status_label = widgets.HTML(value=\"<b>Starting KSWGD computation...</b>\")\n",
    "time_label = widgets.HTML(value=\"\")\n",
    "display(widgets.VBox([progress_bar, status_label, time_label]))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Phase 1: Eigenvalue decomposition (GPU accelerated if available)\n",
    "progress_bar.value = 10\n",
    "use_gpu_eigh = torch.cuda.is_available()\n",
    "if use_gpu_eigh:\n",
    "    status_label.value = \"<b>Phase 1/5:</b> Computing eigenvalue decomposition (GPU)...\"\n",
    "    rw_kernel_torch = torch.from_numpy(rw_kernel).to(device)\n",
    "    lambda_ns_torch, phi_torch = torch.linalg.eigh(rw_kernel_torch)\n",
    "    # Transfer back to CPU/numpy and reverse order (eigh returns ascending order)\n",
    "    lambda_ns = lambda_ns_torch.cpu().numpy()[::-1].copy()\n",
    "    phi = phi_torch.cpu().numpy()[:, ::-1].copy()\n",
    "    del rw_kernel_torch, lambda_ns_torch, phi_torch\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    status_label.value = \"<b>Phase 1/5:</b> Computing eigenvalue decomposition (CPU)...\"\n",
    "    lambda_ns, phi = np.linalg.eigh(rw_kernel)\n",
    "    phi = phi[:, ::-1]\n",
    "    lambda_ns = lambda_ns[::-1]\n",
    "\n",
    "eigh_time = time.time() - start_time\n",
    "time_label.value = f\"<i>Eigenvalue decomposition: {eigh_time:.1f}s ({'GPU' if use_gpu_eigh else 'CPU'})</i>\"\n",
    "\n",
    "# Phase 2: Setting up parameters\n",
    "progress_bar.value = 30\n",
    "status_label.value = \"<b>Phase 2/5:</b> Setting up regularization parameters...\"\n",
    "tol = 1e-6\n",
    "reg = 1e-3\n",
    "# lambda_ = -lambda_ns + 1.0\n",
    "lambda_ = lambda_ns - 1.0\n",
    "inv_lambda = np.zeros_like(lambda_)\n",
    "inv_lambda[1:] = 1.0 / np.clip(lambda_[1:], 1e-12, None)\n",
    "inv_lambda *= eps_kswgd\n",
    "\n",
    "# Phase 3: Computing inverse eigenvalues\n",
    "progress_bar.value = 50\n",
    "status_label.value = \"<b>Phase 3/5:</b> Computing inverse eigenvalues...\"\n",
    "lambda_ns_inv = np.zeros_like(lambda_ns)\n",
    "mask = lambda_ns >= tol\n",
    "lambda_ns_inv[mask] = eps_kswgd / (lambda_ns[mask] + reg)\n",
    "above_tol = int(np.sum(mask))\n",
    "phi_trunc = phi[:, :above_tol]\n",
    "lambda_ns_s_ns = (lambda_ns_inv * inv_lambda * lambda_ns_inv)[:above_tol]\n",
    "\n",
    "# Phase 4: Computing target distribution\n",
    "progress_bar.value = 70\n",
    "status_label.value = \"<b>Phase 4/5:</b> Computing target distribution...\"\n",
    "p_tar = np.sum(data_kernel, axis=0)\n",
    "sqrt_p = np.sqrt(p_tar + 1e-12)\n",
    "D_vec = np.sum(data_kernel / sqrt_p[:, None] / sqrt_p[None, :], axis=1)\n",
    "\n",
    "# Phase 5: Finalizing\n",
    "progress_bar.value = 90\n",
    "elapsed = time.time() - start_time\n",
    "status_label.value = \"<b>Phase 5/5:</b> Finalizing results...\"\n",
    "\n",
    "# Complete\n",
    "progress_bar.value = 100\n",
    "progress_bar.bar_style = 'success'\n",
    "total_time = time.time() - start_time\n",
    "status_label.value = \"<b style='color: green;'>✓ Completed!</b>\"\n",
    "time_label.value = f\"<i>Total time: {total_time:.1f}s (eigh: {eigh_time:.1f}s on {'GPU' if use_gpu_eigh else 'CPU'})</i>\"\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Eigenvalue decomposition: {'GPU' if use_gpu_eigh else 'CPU'} ({eigh_time:.1f}s)\")\n",
    "print(\"Retained eigenvectors for KSWGD:\", above_tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef7d619",
   "metadata": {},
   "source": [
    "## EDMD dictionary-learning pipeline\n",
    "We now replicate the TestÂ 2 EDMD workflow on the MNIST latents: learn a sparse dictionary on the standardized targets, build a stochastic DMD operator in the resulting feature space, and feed the Koopman spectrum into KSWGD for a third generative path. The next cells construct the KDE drift pairs that seed the dictionary stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87322729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define KSWGD sampler and decoding utilities (needed for both EDMD and DM paths)\n",
    "\n",
    "def run_kswgd_sampler(num_particles=64, num_iters=400, step_size=0.05, rng_seed=0):\n",
    "    \"\"\"Transport a batch of latent particles toward the target distribution.\"\"\"\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "    xp = cp if use_gpu_kswgd else np\n",
    "    grad_fn = grad_ker1_gpu if use_gpu_kswgd else grad_ker1\n",
    "    K_eval_fn = K_tar_eval_gpu if use_gpu_kswgd else K_tar_eval\n",
    "\n",
    "    # Preallocate histories\n",
    "    x_hist = xp.zeros((num_particles, latent_dim, num_iters), dtype=xp.float64)\n",
    "    init_block = rng.normal(0.0, 1.0, size=(num_particles, latent_dim))\n",
    "    x_hist[:, :, 0] = xp.asarray(init_block)\n",
    "\n",
    "    # Shared tensors on the selected backend\n",
    "    if use_gpu_kswgd:\n",
    "        X_tar_dev = cp.asarray(X_tar)\n",
    "        p_tar_dev = cp.asarray(p_tar)\n",
    "        sq_tar_dev = cp.asarray(sq_tar)\n",
    "        D_vec_dev = cp.asarray(D_vec)\n",
    "        phi_trunc_dev = cp.asarray(phi_trunc)\n",
    "        lambda_weights = cp.asarray(lambda_ns_s_ns)\n",
    "    else:\n",
    "        X_tar_dev = X_tar\n",
    "        p_tar_dev = p_tar\n",
    "        sq_tar_dev = sq_tar\n",
    "        D_vec_dev = D_vec\n",
    "        phi_trunc_dev = phi_trunc\n",
    "        lambda_weights = lambda_ns_s_ns\n",
    "\n",
    "    iterator = trange(num_iters - 1, desc=\"KSWGD\", unit=\"step\")\n",
    "    for t in iterator:\n",
    "        current = x_hist[:, :, t]\n",
    "        grad_matrix = grad_fn(current, X_tar_dev, p_tar_dev, sq_tar_dev, D_vec_dev, eps_kswgd)\n",
    "        cross_matrix = K_eval_fn(X_tar_dev, current, p_tar_dev, sq_tar_dev, D_vec_dev, eps_kswgd)\n",
    "        tmp = phi_trunc_dev.T @ cross_matrix\n",
    "        tmp = lambda_weights[:, None] * tmp\n",
    "        kswgd_push = phi_trunc_dev @ tmp\n",
    "\n",
    "        for dim in range(latent_dim):\n",
    "            sum_term = grad_matrix[:, :, dim] @ kswgd_push\n",
    "            x_hist[:, dim, t + 1] = x_hist[:, dim, t] - (step_size / num_particles) * xp.sum(sum_term, axis=1)\n",
    "\n",
    "        if (t + 1) % 50 == 0 or t + 1 == num_iters - 1:\n",
    "            step_norm = x_hist[:, :, t + 1] - x_hist[:, :, t]\n",
    "            mean_disp = float(xp.mean(xp.linalg.norm(step_norm, axis=1)))\n",
    "            iterator.set_postfix({\"mean_step\": f\"{mean_disp:.3e}\"})\n",
    "\n",
    "    samples_std = x_hist[:, :, -1]\n",
    "    if use_gpu_kswgd:\n",
    "        samples_std = cp.asnumpy(samples_std)\n",
    "    return np.asarray(samples_std, dtype=np.float64)\n",
    "\n",
    "\n",
    "def decode_latents(flat_latents: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\"Decode flattened latent vectors back into RGB images.\"\"\"\n",
    "    autoencoder.eval()\n",
    "    latents_tensor = torch.from_numpy(flat_latents).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        decoded = autoencoder.decode(latents_tensor)\n",
    "    return decoded.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87fe77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct EDMD dictionary and Koopman operator, then visualize spectrum\n",
    "# This cell reconstructs the drift dynamics, learns the dictionary, and computes K_edmd\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Setup data for EDMD (subsample for speed)\n",
    "# =============================================================================\n",
    "n_edmd_samples = 5000\n",
    "if X_tar.shape[0] > n_edmd_samples:\n",
    "    # Stratified subsample if labels available, else random\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, train_size=n_edmd_samples, random_state=42)\n",
    "    idx_edmd, _ = next(splitter.split(X_tar, labels_dm))\n",
    "    X_edmd = X_tar[idx_edmd]\n",
    "    y_edmd_labels = labels_dm[idx_edmd]\n",
    "else:\n",
    "    X_edmd = X_tar\n",
    "    y_edmd_labels = labels_dm\n",
    "\n",
    "print(f\"EDMD training set size: {X_edmd.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Compute KDE drift (score function)\n",
    "# =============================================================================\n",
    "# Bandwidth h via median heuristic on the subset\n",
    "dists_edmd = pairwise_distances(X_edmd, metric=\"euclidean\")\n",
    "med_sq_edmd = np.median(dists_edmd**2)\n",
    "h_bw = np.sqrt(med_sq_edmd)\n",
    "print(f\"KDE bandwidth h: {h_bw:.4f}\")\n",
    "\n",
    "# Compute weights W_ij = exp(-dist^2 / (2*h^2))\n",
    "W = np.exp(-dists_edmd**2 / (2 * h_bw**2))\n",
    "# Row normalize for mean shift\n",
    "row_sums_W = W.sum(axis=1, keepdims=True)\n",
    "P_W = W / row_sums_W\n",
    "\n",
    "# Mean shift vector: m(x) = \\sum P_ij x_j - x_i\n",
    "drift = P_W @ X_edmd - X_edmd\n",
    "# Score = m(x) / h^2\n",
    "score = drift / (h_bw**2)\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Generate pairs (X, Y)\n",
    "# =============================================================================\n",
    "dt = 0.1\n",
    "noise_scale = np.sqrt(2 * dt)\n",
    "noise = np.random.normal(0, 1, X_edmd.shape) * noise_scale\n",
    "Y_edmd = X_edmd + score * dt + noise\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Learn Dictionary\n",
    "# =============================================================================\n",
    "n_dict = 500  # Increased for CIFAR-10 (latent dim ~64)\n",
    "alpha_sparsity = 1.0\n",
    "# FIX: Changed n_iter to max_iter for newer scikit-learn versions\n",
    "dict_learner = MiniBatchDictionaryLearning(\n",
    "    n_components=n_dict, alpha=alpha_sparsity, max_iter=20, batch_size=128, random_state=42, verbose=0\n",
    ")\n",
    "print(f\"Training dictionary with {n_dict+1} dictionary functions...\")\n",
    "# Fit on both X and Y to cover the space\n",
    "data_combined = np.vstack([X_edmd, Y_edmd])\n",
    "dict_learner.fit(data_combined)\n",
    "\n",
    "# =============================================================================\n",
    "# 5. Compute features Psi(X), Psi(Y)\n",
    "# =============================================================================\n",
    "print(\"Computing sparse codes...\")\n",
    "Psi_X = dict_learner.transform(X_edmd)\n",
    "Psi_Y = dict_learner.transform(Y_edmd)\n",
    "\n",
    "# Add constant feature (for probability conservation/bias)\n",
    "Psi_X = np.hstack([np.ones((Psi_X.shape[0], 1)), Psi_X])\n",
    "Psi_Y = np.hstack([np.ones((Psi_Y.shape[0], 1)), Psi_Y])\n",
    "\n",
    "# =============================================================================\n",
    "# 6. Compute Koopman Operator K\n",
    "# =============================================================================\n",
    "# K = pinv(Psi_X) @ Psi_Y\n",
    "print(\"Solving for Koopman operator...\")\n",
    "# Ridge regression for stability\n",
    "lambda_reg = 1e-5\n",
    "A = Psi_X.T @ Psi_X\n",
    "B = Psi_X.T @ Psi_Y\n",
    "A_reg = A + lambda_reg * np.eye(A.shape[0])\n",
    "K_edmd = np.linalg.solve(A_reg, B)\n",
    "\n",
    "print(f\"Koopman matrix shape: {K_edmd.shape}\")\n",
    "koopman_matrix = K_edmd  # Set for visualization\n",
    "\n",
    "# =============================================================================\n",
    "# 7. Visualize Spectrum\n",
    "# =============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "\n",
    "# Compute eigenvalues and eigenvectors\n",
    "eigvals, eigvecs = np.linalg.eig(koopman_matrix)\n",
    "\n",
    "# Sort by real part\n",
    "sorted_idx = np.argsort(-eigvals.real)\n",
    "eigvals_sorted = eigvals[sorted_idx]\n",
    "\n",
    "print(\"Eigenvalues sorted by decreasing real part (top 10):\")\n",
    "for idx, val in enumerate(eigvals_sorted[:10]):\n",
    "    print(f\"{idx:3d}: real={val.real:+.4f}, imag={val.imag:+.4f}, |λ|={np.abs(val):.4f}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "unit_circle = Circle((0, 0), 1.0, color=\"lightgray\", fill=False, linestyle=\"--\", linewidth=1.5)\n",
    "ax.add_patch(unit_circle)\n",
    "ax.scatter(eigvals.real, eigvals.imag, c=\"tab:blue\", s=30, alpha=0.8)\n",
    "ax.set_xlabel(\"Real part\")\n",
    "ax.set_ylabel(\"Imaginary part\")\n",
    "ax.set_title(\"EDMD Koopman spectrum inside the unit disk\")\n",
    "ax.axhline(0, color=\"black\", linewidth=0.5)\n",
    "ax.axvline(0, color=\"black\", linewidth=0.5)\n",
    "ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "ax.set_xlim(-1.2, 1.2)\n",
    "ax.set_ylim(-1.2, 1.2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 8. Run KSWGD with EDMD Spectrum (Discarding Imaginary Parts)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Running KSWGD using EDMD spectrum...\")\n",
    "\n",
    "# 1. Extract Real Parts of Eigenvalues and Eigenfunctions\n",
    "# Sort eigenvectors to match sorted eigenvalues\n",
    "eigvecs_sorted = eigvecs[:, sorted_idx]\n",
    "\n",
    "# Compute eigenfunctions on the data: Phi = Psi @ V\n",
    "# Psi_X is (N, K+1), eigvecs is (K+1, K+1) -> Phi is (N, K+1)\n",
    "Phi_edmd_complex = Psi_X @ eigvecs_sorted\n",
    "\n",
    "# Take Real Parts (Discard Imaginary)\n",
    "lambda_edmd_real = np.real(eigvals_sorted)\n",
    "Phi_edmd_real = np.real(Phi_edmd_complex)\n",
    "\n",
    "# 2. Select top modes for KSWGD\n",
    "# We skip the first trivial mode (lambda=1, constant function) if it dominates,\n",
    "# but KSWGD logic usually handles it via the regularization.\n",
    "# Let's keep top 80 modes as suggested in notes.\n",
    "n_modes_edmd = 80\n",
    "lambda_edmd_trunc = lambda_edmd_real[:n_modes_edmd]\n",
    "phi_edmd_trunc = Phi_edmd_real[:, :n_modes_edmd]\n",
    "\n",
    "print(f\"Using top {n_modes_edmd} real modes for KSWGD.\")\n",
    "\n",
    "# 3. Compute Spectral Weights (Regularization)\n",
    "# Replicating the logic from Cell 18 but for EDMD eigenvalues\n",
    "# lambda_ns_s_ns = (lambda_inv * inv_lambda * lambda_inv)\n",
    "# where inv_lambda ~ 1/lambda (with epsilon scaling)\n",
    "\n",
    "tol = 1e-6\n",
    "reg = 1e-3\n",
    "\n",
    "# Inverse eigenvalues with regularization\n",
    "lambda_inv_edmd = np.zeros_like(lambda_edmd_trunc)\n",
    "mask_edmd = np.abs(lambda_edmd_trunc) >= tol\n",
    "lambda_inv_edmd[mask_edmd] = eps_kswgd / (lambda_edmd_trunc[mask_edmd] + reg)\n",
    "\n",
    "# The \"inv_lambda\" term from Cell 18 (approx 1/lambda)\n",
    "# Note: In Cell 18, lambda_ = lambda_ns - 1.0 was used for some reason,\n",
    "# but standard KSWGD often uses just 1/lambda. Let's stick to the structure:\n",
    "# inv_lambda[1:] = 1.0 / lambda[1:]\n",
    "inv_lambda_edmd = np.zeros_like(lambda_edmd_trunc)\n",
    "inv_lambda_edmd[1:] = 1.0 / np.clip(lambda_edmd_trunc[1:], 1e-12, None)\n",
    "inv_lambda_edmd *= eps_kswgd\n",
    "\n",
    "# Combined weights\n",
    "lambda_weights_edmd = (lambda_inv_edmd * inv_lambda_edmd * lambda_inv_edmd)\n",
    "\n",
    "# 4. Swap Globals and Run Sampler\n",
    "# Backup Diffusion Map globals\n",
    "phi_trunc_backup = phi_trunc\n",
    "lambda_weights_backup = lambda_ns_s_ns\n",
    "\n",
    "try:\n",
    "    # Inject EDMD globals\n",
    "    phi_trunc = phi_edmd_trunc\n",
    "    lambda_ns_s_ns = lambda_weights_edmd\n",
    "    \n",
    "    # Run Sampler\n",
    "    print(\"Executing KSWGD sampler with EDMD basis...\")\n",
    "    kswgd_config_edmd = dict(num_particles=128, num_iters=600, step_size=0.03, rng_seed=2)\n",
    "    Z_new_kswgd_edmd_std = run_kswgd_sampler(**kswgd_config_edmd)\n",
    "    \n",
    "    # Un-standardize\n",
    "    Z_new_kswgd_edmd = Z_new_kswgd_edmd_std * Z_dm_std + Z_dm_mean\n",
    "    print(f\"Generated EDMD samples: {Z_new_kswgd_edmd.shape}\")\n",
    "    \n",
    "finally:\n",
    "    # Restore Diffusion Map globals so other cells don't break\n",
    "    phi_trunc = phi_trunc_backup\n",
    "    lambda_ns_s_ns = lambda_weights_backup\n",
    "    print(\"Restored Diffusion Map globals.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785050d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a single KSWGD transport in latent space and decode the resulting samples\n",
    "# (Functions run_kswgd_sampler and decode_latents are defined in a previous cell)\n",
    "\n",
    "kswgd_config_dm = dict(num_particles=128, num_iters=600, step_size=0.03, rng_seed=1)\n",
    "print(\"KSWGD config (DM path):\", kswgd_config_dm)\n",
    "\n",
    "Z_new_kswgd_dm_std = run_kswgd_sampler(**kswgd_config_dm)\n",
    "Z_new_kswgd_dm = Z_new_kswgd_dm_std * Z_dm_std + Z_dm_mean\n",
    "dm_images = decode_latents(Z_new_kswgd_dm)\n",
    "dm_images_cpu = dm_images.numpy()\n",
    "print(f\"Generated latent samples: {Z_new_kswgd_dm.shape}\")\n",
    "print(f\"Decoded image batch: {dm_images_cpu.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6573cc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CIFAR-10 samples generated via diffusion-map KSWGD\n",
    "if \"dm_images_cpu\" not in globals():\n",
    "    raise RuntimeError(\"Run the KSWGD sampling cell first to populate `dm_images_cpu`.\")\n",
    "\n",
    "n_cols_dm = 8\n",
    "n_rows_dm = max(1, dm_images_cpu.shape[0] // n_cols_dm)\n",
    "n_show_dm = n_rows_dm * n_cols_dm\n",
    "fig, axes = plt.subplots(n_rows_dm, n_cols_dm, figsize=(2 * n_cols_dm, 2 * n_rows_dm))\n",
    "axes = np.asarray(axes).reshape(n_rows_dm, n_cols_dm)\n",
    "\n",
    "for idx in range(n_show_dm):\n",
    "    ax = axes[idx // n_cols_dm, idx % n_cols_dm]\n",
    "    img = np.transpose(dm_images_cpu[idx], (1, 2, 0))\n",
    "    ax.imshow(np.clip(img, 0.0, 1.0))\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"KSWGD (DM) samples decoded by the pretrained VAE\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8750daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally decode EDMD + KSWGD latent vectors if they are defined\n",
    "if \"Z_new_kswgd_edmd\" not in globals():\n",
    "    print(\"EDMD-based latent samples not available; skipping this cell.\")\n",
    "else:\n",
    "    edmd_images = decode_latents(Z_new_kswgd_edmd)\n",
    "    edmd_images_cpu = edmd_images.numpy()\n",
    "    print(\"EDMD-KSWGD images shape:\", edmd_images_cpu.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5873eaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize EDMD + KSWGD samples when available\n",
    "if \"edmd_images_cpu\" not in globals():\n",
    "    print(\"No EDMD reconstructions to display.\")\n",
    "else:\n",
    "    n_cols_edmd = 8\n",
    "    n_rows_edmd = max(1, edmd_images_cpu.shape[0] // n_cols_edmd)\n",
    "    n_show_edmd = n_rows_edmd * n_cols_edmd\n",
    "    fig, axes = plt.subplots(n_rows_edmd, n_cols_edmd, figsize=(2 * n_cols_edmd, 2 * n_rows_edmd))\n",
    "    axes = np.asarray(axes).reshape(n_rows_edmd, n_cols_edmd)\n",
    "    for idx in range(n_show_edmd):\n",
    "        ax = axes[idx // n_cols_edmd, idx % n_cols_edmd]\n",
    "        img = np.transpose(edmd_images_cpu[idx], (1, 2, 0))\n",
    "        ax.imshow(np.clip(img, 0.0, 1.0))\n",
    "        ax.axis(\"off\")\n",
    "    plt.suptitle(\"EDMD + KSWGD CIFAR-10 samples\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddbaf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep several KSWGD hyper-parameters and visualize decoded CIFAR-10 samples\n",
    "from itertools import product\n",
    "\n",
    "if 'run_kswgd_sampler' not in globals():\n",
    "    raise RuntimeError(\"Define `run_kswgd_sampler` before launching the sweep.\")\n",
    "\n",
    "# Parameter grids tuned for the CIFAR-10 latent geometry\n",
    "rng_seed_list = [2, 3]\n",
    "m_particles_list = [64, 128]\n",
    "step_size_list = [0.02, 0.04]\n",
    "num_iters_list = [400, 600]\n",
    "\n",
    "param_combinations = list(product(rng_seed_list, m_particles_list, step_size_list, num_iters_list))\n",
    "print(f\"Total configurations: {len(param_combinations)}\")\n",
    "print(\"Loop order: rng_seed -> m_particles -> step_size -> num_iters\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for idx, (rng_seed_loop, m_particles_loop, step_size_loop, num_iters_loop) in enumerate(param_combinations, start=1):\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"Config {idx}/{len(param_combinations)}\")\n",
    "    print(f\"  rng_seed={rng_seed_loop}, m_particles={m_particles_loop}, step_size={step_size_loop}, num_iters={num_iters_loop}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    samples_std = run_kswgd_sampler(\n",
    "        num_particles=m_particles_loop,\n",
    "        num_iters=num_iters_loop,\n",
    "        step_size=step_size_loop,\n",
    "        rng_seed=rng_seed_loop,\n",
    "    )\n",
    "    samples = samples_std * Z_dm_std + Z_dm_mean\n",
    "    loop_images = decode_latents(samples).numpy()\n",
    "\n",
    "    n_cols_loop = 16\n",
    "    n_rows_loop = max(1, m_particles_loop // n_cols_loop)\n",
    "    n_show_loop = n_rows_loop * n_cols_loop\n",
    "    fig, axes = plt.subplots(n_rows_loop, n_cols_loop, figsize=(1.5 * n_cols_loop, 1.5 * n_rows_loop))\n",
    "    axes = np.asarray(axes).reshape(n_rows_loop, n_cols_loop)\n",
    "    for i in range(n_show_loop):\n",
    "        ax = axes[i // n_cols_loop, i % n_cols_loop]\n",
    "        img = np.transpose(loop_images[i], (1, 2, 0))\n",
    "        ax.imshow(np.clip(img, 0.0, 1.0))\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    title_str = (\n",
    "        f\"KSWGD (DM) | seed={rng_seed_loop}, particles={m_particles_loop}, \"\n",
    "        f\"step={step_size_loop}, iters={num_iters_loop}\"\n",
    "    )\n",
    "    plt.suptitle(title_str, fontsize=18, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.94])\n",
    "    plt.show()\n",
    "\n",
    "    print(\"[Parameter Summary]\")\n",
    "    print(f\"  - Random seed: {rng_seed_loop}\")\n",
    "    print(f\"  - Particles: {m_particles_loop}\")\n",
    "    print(f\"  - Step size: {step_size_loop}\")\n",
    "    print(f\"  - Iterations: {num_iters_loop}\")\n",
    "    print(f\"  - Image tensor: {loop_images.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22616a23",
   "metadata": {},
   "source": [
    "# Technical Notes & Parameter Reference\n",
    "---\n",
    "## 1. Dataset, Autoencoder & Dataloaders\n",
    "- **Dataset:** CIFAR-10 (32×32 RGB) loaded via `transforms.ToTensor()` with no additional normalization.\n",
    "- **Batch size:** 128 for both training and validation loaders to keep GPU RAM predictable.\n",
    "- **Latent model:** Pretrained `stabilityai/sd-vae-ft-mse` `AutoencoderKL` wrapped in `DiffusersAutoencoderWrapper`. Encoder blocks are frozen except the final down block, matching the CIFAR-10 distribution without extra fine-tuning by default.\n",
    "- **Latent dimension:** Determined dynamically (`latent_dim = autoencoder.latent_dim`, ~64 for 32×32 inputs).\n",
    "- **Scaling:** The diffusers VAE uses scaling factor $s$ so encoder/decoder follow\n",
    "  $$z = s \\cdot \\text{mode}(\\mathcal{E}(2x-1)), \\qquad x = \\operatorname{clip}\\left(\\frac{\\mathcal{D}(z/s)+1}{2}, 0, 1\\right).$$\n",
    "---\n",
    "## 2. Latent Collection & Diffusion Maps\n",
    "1. **Latent harvesting:** Iterate through the CIFAR-10 training loader, collecting latent vectors $Z \\in \\mathbb{R}^{N\\times d}$ and class labels.\n",
    "2. **Sampling budget:** `max_dm_samples = 20000` via stratified `StratifiedShuffleSplit` to keep class balance.\n",
    "3. **Standardization:**\n",
    "   $$Z_{dm} = \\frac{Z_{raw} - \\mu_{dm}}{\\sigma_{dm}+10^{-8}}$$\n",
    "4. **Kernel + Markov matrix:** Pairwise distances on $Z_{dm}$ feed a Gaussian kernel with bandwidth\n",
    "   $$\\epsilon = \\frac{\\text{median}(d_{ij}^2)}{2\\ln N}, \\quad K_{ij} = \\exp\\Big(-\\frac{d_{ij}^2}{2\\epsilon}\\Big), \\quad P = D^{-1}K.$$\n",
    "5. **Eigenproblem:** `m = 32` leading eigenpairs of $P^T$ (diffusion time `t_diffusion = 1`).\n",
    "6. **Diagnostics:**\n",
    "   - 2D scatter keeps top $85\\%$ GMM likelihood mass (`n_gmm_components_2d = 10`).\n",
    "   - 3D visualization spreads clusters with `SPREAD_FACTOR = 1.5`, IQR filtering, and a 12-component GMM removing the weakest 10% log-likelihood tail.\n",
    "7. **Moment stats:** `num_dc = 16` diffusion coordinates feed the Gaussian diagnostics and downstream summaries.\n",
    "---\n",
    "## 3. KSWGD Target Construction\n",
    "- **Target samples:** `X_tar = Z_dm` (already standardized).\n",
    "- **Bandwidth:**\n",
    "  $$\\epsilon_{kswgd} = \\max\\left(\\frac{\\text{median}(d_{ij}^2)}{2\\ln(N+1)}, 10^{-6}\\right).$$\n",
    "- **Random-walk kernel:**\n",
    "  1. `p_x = sqrt(sum_j K_ij)`\n",
    "  2. `data_kernel_norm = K / (p_x p_x^T)`\n",
    "  3. Symmetrize to obtain `rw_kernel`, ensuring numerical stability with `np.nan_to_num`.\n",
    "- **Spectral prep:** retain eigenmodes with `tol = 1e-6`, apply `reg = 1e-3`, and reuse `phi_trunc`, `lambda_ns_s_ns` on both CPU and CuPy backends.\n",
    "---\n",
    "## 4. Baseline KSWGD Transport (Diffusion-Map Path)\n",
    "| Hyper-parameter | Value | Notes |\n",
    "| --- | --- | --- |\n",
    "| `num_particles` | 128 | More particles for the higher-dimensional CIFAR latent |\n",
    "| `num_iters` | 600 | Longer run stabilizes transport |\n",
    "| `step_size` | 0.03 | Conservative step for the SD-VAE latent geometry |\n",
    "| `rng_seed` | 1 | NumPy RNG seed for deterministic initialization |\n",
    "| Backend | CPU or CuPy | auto-selects GPU kernels when available |\n",
    "Workflow: initialize $x^{(0)} \\sim \\mathcal{N}(0, I)$, iterate\n",
    "$$x^{(t+1)} = x^{(t)} - \\frac{\\eta}{m}\\sum_j \\nabla_x K(x^{(t)}, X_{tar})_{:,j}\\; \\Phi\\Lambda\\Phi^T K(X_{tar}, x^{(t)})_j,$$\n",
    "then un-standardize via $x = x_{std}\\,\\sigma_{dm} + \\mu_{dm}$ and decode with the pretrained VAE. The resulting tensor (`dm_images_cpu`) drives the CIFAR-10 grids.\n",
    "---\n",
    "## 5. Hyper-parameter Sweep\n",
    "- **Seeds:** `[1, 2, 3]`\n",
    "- **Particles:** `[64, 128]`\n",
    "- **Step sizes:** `[0.02, 0.04, 0.06]`\n",
    "- **Iterations:** `[400, 600]`\n",
    "All $3\\times2\\times3\\times2 = 36$ configurations are evaluated. Each run prints the configuration, produces a color grid (16 columns), and reports the decoded tensor shape for bookkeeping.\n",
    "---\n",
    "## 6. Koopman / EDMD Notes (Placeholder)\n",
    "Although the EDMD dictionary-learning path is currently disabled, re-enabling it will produce a Koopman operator $\\mathbf{K} \\in \\mathbb{R}^{(K+1) \\times (K+1)}$, where $K$ is the number of dictionary atoms and the extra row/column correspond to the constant observable. For the prior MNIST setup (`n_dict_components = 100`), this yields a 101×101 matrix. When adapting to CIFAR-10 you can safely double the atom count (e.g., 200) and keep the top $\\approx 80$ eigenmodes after thresholding at $10^{-6}$ to supply richer spectra to KSWGD. Adjust `m` in the diffusion-map cell in tandem so the Koopman truncation has enough signal.\n",
    "---\n",
    "## 7. Reproducibility Checklist\n",
    "1. Configure seeds (`np.random.seed(1)`, `torch.manual_seed(1)`) and device detection.\n",
    "2. Run dataset + VAE setup cells (1–7).\n",
    "3. Gather latents and execute diffusion-map diagnostics (Cells 11–16) with `m = 32`.\n",
    "4. Build KSWGD kernels and spectra (Cells 17–18).\n",
    "5. Execute the baseline sampler (Cell 19) to populate `dm_images_cpu`.\n",
    "6. Run the sweep (Cell 23) or re-enable the EDMD track as needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kswgd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
