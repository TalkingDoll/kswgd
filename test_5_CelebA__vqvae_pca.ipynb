{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "035e1915",
   "metadata": {},
   "source": [
    "## 1. Install and Import Dependencies\n",
    "\n",
    "If dependencies are not installed, run first:\n",
    "```\n",
    "pip install diffusers transformers accelerate datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ba208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Check GPU\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    print(f\"    Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792f749f",
   "metadata": {},
   "source": [
    "## 2. Download and Load CelebA-HQ Dataset\n",
    "\n",
    "We load the CelebA-HQ dataset first. This dataset serves as the **Target Distribution** for KSWGD and the **Ground Truth** for LDM's evaluation (FID). By loading it here, we ensure both methods are compared against the same data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a3f4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CelebA-HQ to data/ folder\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from datasets import load_dataset\n",
    "except ImportError:\n",
    "    print(\"Installing datasets library...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"datasets\", \"-q\"])\n",
    "    from datasets import load_dataset\n",
    "\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set data directory (alongside MNIST, CIFAR-10)\n",
    "DATA_DIR = \"/workspace/kswgd/data\"\n",
    "CELEBAHQ_CACHE = os.path.join(DATA_DIR, \"CelebA-HQ\")\n",
    "os.makedirs(CELEBAHQ_CACHE, exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"CelebA-HQ cache directory: {CELEBAHQ_CACHE}\")\n",
    "\n",
    "# Download CelebA-HQ dataset to data/ folder\n",
    "# Try multiple possible dataset sources\n",
    "print(\"\\nDownloading CelebA-HQ dataset...\")\n",
    "print(\"Data will be saved to data/CelebA-HQ/ folder\")\n",
    "\n",
    "dataset_sources = [\n",
    "    \"mattymchen/celeba-hq\",           # Alternative source 1\n",
    "    \"datasets-community/CelebA-HQ\",   # Alternative source 2\n",
    "    \"xinrongzhang2022/celeba-hq\",     # Alternative source 3\n",
    "]\n",
    "\n",
    "celebahq_dataset = None\n",
    "for source in dataset_sources:\n",
    "    try:\n",
    "        print(f\"Trying to load: {source}\")\n",
    "        celebahq_dataset = load_dataset(\n",
    "            source, \n",
    "            split=\"train\",\n",
    "            cache_dir=CELEBAHQ_CACHE,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(f\"âœ“ Successfully loaded: {source}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Failed to load: {e}\")\n",
    "        continue\n",
    "\n",
    "if celebahq_dataset is None:\n",
    "    raise RuntimeError(\"Unable to load CelebA-HQ dataset, please check network connection or download manually\")\n",
    "\n",
    "print(f\"\\nâœ“ CelebA-HQ loaded!\")\n",
    "print(f\"  Save location: {CELEBAHQ_CACHE}\")\n",
    "print(f\"  Total available samples: {len(celebahq_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbea9fc",
   "metadata": {},
   "source": [
    "## 3. Load Pretrained Unconditional Latent Diffusion Model\n",
    "\n",
    "Using `CompVis/ldm-celebahq-256`, an **unconditional** Latent Diffusion Model (LDM) trained on CelebA-HQ.\n",
    "\n",
    "**LDM Architecture:**\n",
    "- VAE: Image â†” Latent Space\n",
    "- UNet: Denoising in Latent Space (unconditional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659276e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "# Load Unconditional Latent Diffusion Model (LDM)\n",
    "# This is a true unconditional model working in latent space\n",
    "print(\"Loading Unconditional LDM (CompVis/ldm-celebahq-256)...\")\n",
    "print(\"This is an unconditional Latent Diffusion Model trained on CelebA-HQ\")\n",
    "\n",
    "ldm_pipe = DiffusionPipeline.from_pretrained(\"CompVis/ldm-celebahq-256\")\n",
    "ldm_pipe = ldm_pipe.to(\"cuda\")\n",
    "\n",
    "# Fix VQ-VAE scaling factor for proper image generation\n",
    "# The default scaling_factor may cause color/brightness issues\n",
    "ldm_pipe.vqvae.config.scaling_factor = 1.0\n",
    "\n",
    "print(\"\\nâœ“ Unconditional LDM loaded!\")\n",
    "print(f\"  Model: CompVis/ldm-celebahq-256\")\n",
    "print(f\"  Output size: 256Ã—256\")\n",
    "print(f\"  Latent Space: Yes (VAE)\")\n",
    "print(f\"  VAE scaling factor: {ldm_pipe.vqvae.config.scaling_factor}\")\n",
    "print(f\"  Generation: Pure unconditional, no condition input required\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993b3e49",
   "metadata": {},
   "source": [
    "## 3.1 Extract LDM's VAE\n",
    "\n",
    "Directly use LDM's built-in VAE to maintain consistent latent space. The VAE is pre-trained as part of the LDM, so we demonstrate its reconstruction quality here before proceeding with image generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9c23ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directly use LDM's built-in VAE\n",
    "# This way KSWGD and LDM use exactly the same latent space\n",
    "\n",
    "print(\"Extracting LDM's VAE...\")\n",
    "\n",
    "# LDM pipeline contains vqvae (VQ-VAE)\n",
    "vae = ldm_pipe.vqvae\n",
    "vae_scaling = 1.0  # LDM's VQ-VAE doesn't need extra scaling\n",
    "\n",
    "print(f\"\\nâœ“ VAE extracted!\")\n",
    "print(f\"  Source: LDM's built-in VQ-VAE\")\n",
    "print(f\"  Scaling factor: {vae_scaling}\")\n",
    "print(f\"  Advantage: KSWGD and LDM share the same latent space\")\n",
    "\n",
    "# VAE helper functions\n",
    "def _to_vae_range(x):\n",
    "    \"\"\"[0,1] â†’ [-1,1]\"\"\"\n",
    "    return (x * 2.0) - 1.0\n",
    "\n",
    "def _from_vae_range(x):\n",
    "    \"\"\"[-1,1] â†’ [0,1]\"\"\"\n",
    "    return torch.clamp((x + 1.0) * 0.5, 0.0, 1.0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce92c96",
   "metadata": {},
   "source": [
    "### 3.2 VAE Reconstruction Quality Test\n",
    "\n",
    "**ðŸ“ Detailed Answers to Your Questions**:\n",
    "\n",
    "**1. Why are the images always the same?**\n",
    "- **Reason**: Previous code fixed to first 5 samples (`for i in range(5)`)\n",
    "- **Solution**: Changed to random sampling or selectable sampling method\n",
    "\n",
    "**2. Where do these images come from?**\n",
    "- **Source**: CelebA-HQ dataset (~30,000 high-quality face images)\n",
    "- **Purpose**: Test VAE's encodeâ†’decode reconstruction capability\n",
    "- **Order**: Dataset loaded in original order after loading\n",
    "\n",
    "**3. Is the VAE pre-trained?**\n",
    "- **Yes!** From `CompVis/ldm-celebahq-256` pre-trained model\n",
    "- **Contains**: Complete VQ-VAE encoder + decoder\n",
    "- **Training data**: Pre-trained on entire CelebA-HQ dataset\n",
    "\n",
    "**4. What is being tested here?**\n",
    "- **Purpose**: Validate VAE reconstruction quality (image â†’ latent vector â†’ image)\n",
    "- **Importance**: VAE quality directly affects KSWGD generation performance\n",
    "- **Evaluation**: Visual comparison of original vs reconstructed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64a9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test VAE reconstruction on real images\n",
    "# ðŸ” VAE Quality Test: Demonstrate LDM's built-in VQ-VAE reconstruction capability\n",
    "# VAE Source: Built-in VQ-VAE from CompVis/ldm-celebahq-256 pretrained model\n",
    "\n",
    "import random\n",
    "\n",
    "# ============== Sampling Settings ==============\n",
    "n_recon = 5\n",
    "USE_RANDOM_SAMPLES = True  # True: random sampling, False: first n_recon samples\n",
    "# RANDOM_SEED = 42  # Set random seed for reproducible results (comment out for pure random)\n",
    "# ==============================================\n",
    "\n",
    "print(f\"=== VAE Reconstruction Quality Test ===\\n\")\n",
    "print(f\"Dataset size: {len(celebahq_dataset)} CelebA-HQ images\")\n",
    "print(f\"Test samples: {n_recon}\")\n",
    "print(f\"Sampling method: {'Random sampling' if USE_RANDOM_SAMPLES else 'First '+str(n_recon)+' samples'}\")\n",
    "print(f\"VAE source: LDM pretrained model built-in VQ-VAE\")\n",
    "print(f\"Test purpose: Validate VAE encodeâ†’decode reconstruction quality\\n\")\n",
    "\n",
    "# Select test samples\n",
    "if USE_RANDOM_SAMPLES:\n",
    "    # Check if RANDOM_SEED is defined, if not use pure random\n",
    "    if 'RANDOM_SEED' in locals():\n",
    "        random.seed(RANDOM_SEED)\n",
    "        print(f\"Using random seed: {RANDOM_SEED}\")\n",
    "    else:\n",
    "        print(\"Using pure random (no seed)\")\n",
    "    \n",
    "    test_indices = random.sample(range(len(celebahq_dataset)), n_recon)\n",
    "    print(f\"Randomly selected sample indices: {test_indices}\")\n",
    "else:\n",
    "    test_indices = list(range(n_recon))\n",
    "    print(f\"Fixed sample indices: {test_indices}\")\n",
    "\n",
    "fig, axes = plt.subplots(2, n_recon, figsize=(15, 6))\n",
    "\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    for plot_idx, sample_idx in enumerate(test_indices):\n",
    "        # Original\n",
    "        img = celebahq_dataset[sample_idx][\"image\"]\n",
    "        img_tensor = T.Compose([T.Resize((256, 256)), T.ToTensor()])(img).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Encode & Decode\n",
    "        latent = vae.encode(_to_vae_range(img_tensor))\n",
    "        \n",
    "        if hasattr(latent, 'latents'):\n",
    "            latent_code = latent.latents\n",
    "        elif hasattr(latent, 'latent_dist'):\n",
    "            latent_code = latent.latent_dist.mode()\n",
    "        else:\n",
    "            latent_code = latent[0] if isinstance(latent, tuple) else latent\n",
    "            \n",
    "        recon = vae.decode(latent_code).sample\n",
    "        recon_img = _from_vae_range(recon).cpu().squeeze(0).permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Plot\n",
    "        axes[0, plot_idx].imshow(np.array(img.resize((256, 256))))\n",
    "        axes[0, plot_idx].set_title(f\"Original #{sample_idx}\", fontsize=10)\n",
    "        axes[0, plot_idx].axis('off')\n",
    "        \n",
    "        axes[1, plot_idx].imshow(recon_img)\n",
    "        axes[1, plot_idx].set_title(f\"VAE Reconstructed #{sample_idx}\", fontsize=10)\n",
    "        axes[1, plot_idx].axis('off')\n",
    "\n",
    "plt.suptitle(\"LDM Built-in VQ-VAE Reconstruction Quality Test (Encodeâ†’Decode)\", fontsize=22)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ VAE reconstruction test complete!\")\n",
    "print(f\"ðŸ“Š Evaluation metric: Visual comparison of original vs reconstructed images\")\n",
    "print(f\"ðŸŽ¯ Good reconstruction quality â†’ VAE can accurately preserve and recover image information\")\n",
    "print(f\"âš ï¸  Poor reconstruction quality â†’ May affect subsequent KSWGD generation quality\")\n",
    "print(f\"\\nðŸ’¡ Want to see different images?\")\n",
    "print(f\"   - Comment out RANDOM_SEED for pure random sampling\")\n",
    "print(f\"   - Set RANDOM_SEED to different values for reproducible results\")\n",
    "print(f\"   - Or simply re-run this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b518a6e",
   "metadata": {},
   "source": [
    "## 4. Batch Unconditional LDM Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719d650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Batch generate multiple unconditional images\n",
    "n_generate = 2  # Generate 5000 images for development testing (reliable FID)\n",
    "batch_size = 64    # Generate in batches (increased for 80GB GPUs)\n",
    "\n",
    "# Check if dual-GPU is available for LDM generation\n",
    "NUM_GPUS_LDM = torch.cuda.device_count()\n",
    "USE_DUAL_GPU_LDM = NUM_GPUS_LDM >= 2\n",
    "\n",
    "print(f\"Detected {NUM_GPUS_LDM} GPU(s) for LDM generation\")\n",
    "print(f\"Mode: {'Dual-GPU parallel' if USE_DUAL_GPU_LDM else 'Single-GPU'}\")\n",
    "print(f\"Generating {n_generate} unconditional LDM images (batch_size={batch_size})...\")\n",
    "print(f\"This will take a while...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "if USE_DUAL_GPU_LDM:\n",
    "    # ============== Dual-GPU LDM Generation ==============\n",
    "    from diffusers import DiffusionPipeline\n",
    "    \n",
    "    # Calculate split: each GPU generates half\n",
    "    n_gpu0 = n_generate // 2\n",
    "    n_gpu1 = n_generate - n_gpu0\n",
    "    \n",
    "    print(f\"  GPU 0: {n_gpu0} images\")\n",
    "    print(f\"  GPU 1: {n_gpu1} images\")\n",
    "    \n",
    "    # Load LDM on GPU 1 (GPU 0 already has ldm_pipe)\n",
    "    print(\"Loading LDM on GPU 1...\")\n",
    "    ldm_pipe_gpu1 = DiffusionPipeline.from_pretrained(\"CompVis/ldm-celebahq-256\")\n",
    "    ldm_pipe_gpu1 = ldm_pipe_gpu1.to(\"cuda:1\")\n",
    "    ldm_pipe_gpu1.vqvae.config.scaling_factor = 1.0\n",
    "    ldm_pipe_gpu1.set_progress_bar_config(disable=True)\n",
    "    \n",
    "    # Also disable progress bar for GPU 0\n",
    "    ldm_pipe.set_progress_bar_config(disable=True)\n",
    "    \n",
    "    def generate_on_gpu(pipe, n_images, batch_size, gpu_id):\n",
    "        \"\"\"Generate images on a specific GPU\"\"\"\n",
    "        images = []\n",
    "        n_batches = (n_images + batch_size - 1) // batch_size\n",
    "        \n",
    "        for batch_idx in tqdm(range(n_batches), desc=f\"LDM (GPU {gpu_id})\"):\n",
    "            current_batch_size = min(batch_size, n_images - batch_idx * batch_size)\n",
    "            result = pipe(\n",
    "                batch_size=current_batch_size,\n",
    "                num_inference_steps=500,\n",
    "            )\n",
    "            images.extend(result.images)\n",
    "            \n",
    "            # Clear cache periodically\n",
    "            if (batch_idx + 1) % 20 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        return images\n",
    "    \n",
    "    # Run both GPUs in parallel using ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        future_gpu0 = executor.submit(generate_on_gpu, ldm_pipe, n_gpu0, batch_size, 0)\n",
    "        future_gpu1 = executor.submit(generate_on_gpu, ldm_pipe_gpu1, n_gpu1, batch_size, 1)\n",
    "        \n",
    "        images_gpu0 = future_gpu0.result()\n",
    "        images_gpu1 = future_gpu1.result()\n",
    "    \n",
    "    # Combine results\n",
    "    ldm_generated_images = images_gpu0 + images_gpu1\n",
    "    \n",
    "    # Clean up GPU 1 pipeline to free memory\n",
    "    del ldm_pipe_gpu1\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "else:\n",
    "    # ============== Single-GPU LDM Generation ==============\n",
    "    ldm_pipe.set_progress_bar_config(disable=True)\n",
    "    \n",
    "    ldm_generated_images = []\n",
    "    n_batches = (n_generate + batch_size - 1) // batch_size\n",
    "\n",
    "    for batch_idx in tqdm(range(n_batches), desc=\"LDM Generation\"):\n",
    "        current_batch_size = min(batch_size, n_generate - batch_idx * batch_size)\n",
    "        result = ldm_pipe(\n",
    "            batch_size=current_batch_size,\n",
    "            num_inference_steps=500,\n",
    "        )\n",
    "        ldm_generated_images.extend(result.images)\n",
    "        \n",
    "        # Clear cache periodically\n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nâœ“ LDM generation complete!\")\n",
    "print(f\"Total images: {len(ldm_generated_images)}\")\n",
    "print(f\"Total time: {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)\")\n",
    "print(f\"Average per image: {elapsed/len(ldm_generated_images):.2f} seconds\")\n",
    "if USE_DUAL_GPU_LDM:\n",
    "    print(f\"Speedup: ~{2.0:.1f}x (dual-GPU parallel)\")\n",
    "\n",
    "# Visualization (show first 16)\n",
    "fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx in range(min(16, len(ldm_generated_images))):\n",
    "    axes[idx].imshow(ldm_generated_images[idx])\n",
    "    # axes[idx].set_title(f\"#{idx+1}\")\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "# Get image size from generated images\n",
    "_ldm_img_size = ldm_generated_images[0].size[0] if hasattr(ldm_generated_images[0], 'size') else 256\n",
    "plt.suptitle(f\"LDM Generated Images (CelebA-HQ {_ldm_img_size}x{_ldm_img_size})\", fontsize=22, y=0.98)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86fb677",
   "metadata": {},
   "source": [
    "## 4.1 Super-Resolution (Real-ESRGAN)\n",
    "\n",
    "The native output of LDM is 256x256. To match the visual quality of high-resolution papers, we apply **Real-ESRGAN** super-resolution to upscale images to 1024x1024.\n",
    "\n",
    "This is **Method A** - the same approach used in many diffusion model papers for high-quality output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9d9796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-ESRGAN + GFPGAN Super-Resolution Setup (Dual-GPU Support)\n",
    "import os\n",
    "import cv2\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "from realesrgan import RealESRGANer\n",
    "from gfpgan import GFPGANer\n",
    "\n",
    "# Create figures directory if not exists\n",
    "os.makedirs('/workspace/kswgd/figures', exist_ok=True)\n",
    "\n",
    "# Check number of GPUs\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "USE_DUAL_GPU = NUM_GPUS >= 2\n",
    "print(f\"Detected {NUM_GPUS} GPU(s), using {'Dual-GPU' if USE_DUAL_GPU else 'Single-GPU'} mode for upscaling\")\n",
    "\n",
    "# Initialize Real-ESRGAN and GFPGAN for each GPU\n",
    "model_path = '/workspace/kswgd/weights/RealESRGAN_x4plus.pth'\n",
    "gfpgan_path = '/workspace/kswgd/weights/GFPGANv1.3.pth'\n",
    "\n",
    "def create_upscaler(gpu_id):\n",
    "    \"\"\"Create Real-ESRGAN + GFPGAN instances on specified GPU\"\"\"\n",
    "    model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n",
    "    upsampler = RealESRGANer(\n",
    "        scale=4,\n",
    "        model_path=model_path,\n",
    "        model=model,\n",
    "        tile=0,\n",
    "        tile_pad=10,\n",
    "        pre_pad=0,\n",
    "        half=True,\n",
    "        gpu_id=gpu_id\n",
    "    )\n",
    "    face_enhancer = GFPGANer(\n",
    "        model_path=gfpgan_path,\n",
    "        upscale=4,\n",
    "        arch='clean',\n",
    "        channel_multiplier=2,\n",
    "        bg_upsampler=upsampler\n",
    "    )\n",
    "    return upsampler, face_enhancer\n",
    "\n",
    "print(\"Initializing Real-ESRGAN + GFPGAN...\")\n",
    "if USE_DUAL_GPU:\n",
    "    # Create instances on both GPUs\n",
    "    upsampler_0, face_enhancer_0 = create_upscaler(0)\n",
    "    upsampler_1, face_enhancer_1 = create_upscaler(1)\n",
    "    upsamplers = [upsampler_0, upsampler_1]\n",
    "    face_enhancers = [face_enhancer_0, face_enhancer_1]\n",
    "    print(f\"  âœ“ GPU 0: Real-ESRGAN + GFPGAN initialized\")\n",
    "    print(f\"  âœ“ GPU 1: Real-ESRGAN + GFPGAN initialized\")\n",
    "else:\n",
    "    # Single GPU mode\n",
    "    upsampler_0, face_enhancer_0 = create_upscaler(0)\n",
    "    upsamplers = [upsampler_0]\n",
    "    face_enhancers = [face_enhancer_0]\n",
    "    print(f\"  âœ“ GPU 0: Real-ESRGAN + GFPGAN initialized\")\n",
    "\n",
    "def preprocess_image(img_bgr, use_gaussian=True, use_bilateral=True, use_color_norm=True):\n",
    "    \"\"\"\n",
    "    Apply preprocessing before GFPGAN:\n",
    "    1. Gaussian blur - reduce noise\n",
    "    2. Bilateral filter - edge-preserving smoothing\n",
    "    3. Color normalization - adjust color distribution (mean normalization)\n",
    "    \"\"\"\n",
    "    processed = img_bgr.copy().astype(np.float32)\n",
    "    \n",
    "    # 1. Gaussian Blur (slight smoothing, small kernel size)\n",
    "    if use_gaussian:\n",
    "        processed = cv2.GaussianBlur(processed, (3, 3), sigmaX=0.5)\n",
    "    \n",
    "    # 2. Bilateral Filter (edge-preserving smoothing)\n",
    "    if use_bilateral:\n",
    "        processed = cv2.bilateralFilter(processed.astype(np.uint8), d=5, sigmaColor=30, sigmaSpace=30).astype(np.float32)\n",
    "    \n",
    "    # 3. Color Normalization (color space mean normalization)\n",
    "    if use_color_norm:\n",
    "        mean_b = np.mean(processed[:, :, 0])\n",
    "        mean_g = np.mean(processed[:, :, 1])\n",
    "        mean_r = np.mean(processed[:, :, 2])\n",
    "        target_mean = 127.5\n",
    "        alpha = 0.3\n",
    "        processed[:, :, 0] = processed[:, :, 0] + alpha * (target_mean - mean_b)\n",
    "        processed[:, :, 1] = processed[:, :, 1] + alpha * (target_mean - mean_g)\n",
    "        processed[:, :, 2] = processed[:, :, 2] + alpha * (target_mean - mean_r)\n",
    "        processed = np.clip(processed, 0, 255)\n",
    "    \n",
    "    return processed.astype(np.uint8)\n",
    "\n",
    "def process_single_image(args):\n",
    "    \"\"\"Process a single image with specified GPU\"\"\"\n",
    "    img, gpu_id, use_face_enhance, use_preprocess = args\n",
    "    face_enhancer = face_enhancers[gpu_id]\n",
    "    upsampler = upsamplers[gpu_id]\n",
    "    \n",
    "    if isinstance(img, Image.Image):\n",
    "        img_np = np.array(img)\n",
    "    else:\n",
    "        if img.ndim == 3 and img.shape[0] == 3:\n",
    "            img_np = np.transpose(img, (1, 2, 0))\n",
    "        else:\n",
    "            img_np = img\n",
    "        if img_np.max() <= 1.0:\n",
    "            img_np = (img_np * 255).astype(np.uint8)\n",
    "    \n",
    "    img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    if use_preprocess:\n",
    "        img_bgr = preprocess_image(img_bgr, use_gaussian=True, use_bilateral=True, use_color_norm=True)\n",
    "    \n",
    "    if use_face_enhance:\n",
    "        _, _, output_bgr = face_enhancer.enhance(img_bgr, has_aligned=False, only_center_face=False, paste_back=True)\n",
    "    else:\n",
    "        output_bgr, _ = upsampler.enhance(img_bgr, outscale=4)\n",
    "    \n",
    "    output_rgb = cv2.cvtColor(output_bgr, cv2.COLOR_BGR2RGB)\n",
    "    return output_rgb\n",
    "\n",
    "def upscale_images(images_list, use_face_enhance=True, use_preprocess=True, desc=\"Upscaling\"):\n",
    "    \"\"\"\n",
    "    Upscale images using GFPGAN (face enhancement) or Real-ESRGAN (basic upscale).\n",
    "    Automatically uses dual-GPU parallel processing if available.\n",
    "    \"\"\"\n",
    "    n_images = len(images_list)\n",
    "    \n",
    "    if USE_DUAL_GPU and n_images >= 2:\n",
    "        # Dual-GPU parallel processing\n",
    "        print(f\"  Using Dual-GPU parallel processing...\")\n",
    "        mid = n_images // 2\n",
    "        \n",
    "        # Assign images to GPUs: first half to GPU 0, second half to GPU 1\n",
    "        args_gpu0 = [(img, 0, use_face_enhance, use_preprocess) for img in images_list[:mid]]\n",
    "        args_gpu1 = [(img, 1, use_face_enhance, use_preprocess) for img in images_list[mid:]]\n",
    "        \n",
    "        results_gpu0 = []\n",
    "        results_gpu1 = []\n",
    "        \n",
    "        # Use ThreadPoolExecutor to run both GPUs in parallel\n",
    "        with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "            # Submit GPU 0 batch\n",
    "            future_gpu0 = executor.submit(\n",
    "                lambda args_list: [process_single_image(args) for args in tqdm(args_list, desc=f\"{desc} (GPU 0)\")],\n",
    "                args_gpu0\n",
    "            )\n",
    "            # Submit GPU 1 batch  \n",
    "            future_gpu1 = executor.submit(\n",
    "                lambda args_list: [process_single_image(args) for args in tqdm(args_list, desc=f\"{desc} (GPU 1)\")],\n",
    "                args_gpu1\n",
    "            )\n",
    "            \n",
    "            results_gpu0 = future_gpu0.result()\n",
    "            results_gpu1 = future_gpu1.result()\n",
    "        \n",
    "        # Combine results in original order\n",
    "        upscaled_images = results_gpu0 + results_gpu1\n",
    "    else:\n",
    "        # Single-GPU processing\n",
    "        upscaled_images = []\n",
    "        for img in tqdm(images_list, desc=desc):\n",
    "            result = process_single_image((img, 0, use_face_enhance, use_preprocess))\n",
    "            upscaled_images.append(result)\n",
    "    \n",
    "    return upscaled_images\n",
    "\n",
    "print(\"âœ“ Upscalers initialized!\")\n",
    "print(f\"  - Real-ESRGAN: 4x basic upscale\")\n",
    "print(f\"  - GFPGAN: 4x upscale + face enhancement + color correction\")\n",
    "print(f\"  - Preprocessing: Gaussian blur + Bilateral filter + Color normalization\")\n",
    "print(f\"  - Mode: {'Dual-GPU parallel' if USE_DUAL_GPU else 'Single-GPU sequential'}\")\n",
    "\n",
    "# ============== Upscale LDM Generated Images ==============\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Upscaling LDM Generated Images with GFPGAN\")\n",
    "print(\"(with preprocessing: Gaussian + Bilateral + Color Norm)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use all generated images from previous cell\n",
    "n_test = len(ldm_generated_images)\n",
    "print(f\"\\nEnhancing {n_test} LDM images with preprocessing + GFPGAN...\")\n",
    "ldm_enhanced = upscale_images(ldm_generated_images, use_face_enhance=True, use_preprocess=True, desc=\"Preprocess+GFPGAN\")\n",
    "\n",
    "# Visualization (show up to 16 in a grid)\n",
    "n_show = min(16, n_test)\n",
    "n_cols = 4\n",
    "n_rows = (n_show + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))\n",
    "axes = np.asarray(axes).reshape(-1)\n",
    "\n",
    "for i in range(n_show):\n",
    "    axes[i].imshow(ldm_enhanced[i])\n",
    "    # axes[i].set_title(f\"#{i+1}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "for i in range(n_show, len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "# Get image size from enhanced images\n",
    "_ldm_enhanced_size = ldm_enhanced[0].shape[1] if isinstance(ldm_enhanced[0], np.ndarray) else ldm_enhanced[0].size[0]\n",
    "plt.suptitle(f\"LDM Enhanced Images (CelebA-HQ {_ldm_enhanced_size}x{_ldm_enhanced_size})\", fontsize=22, y=0.98)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.savefig('/workspace/kswgd/figures/ldm_gfpgan_enhanced.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Store enhanced images for later use\n",
    "ldm_upscaled = ldm_enhanced\n",
    "\n",
    "print(f\"\\nâœ“ Enhancement complete!\")\n",
    "print(f\"  Original LDM: 256x256\")\n",
    "print(f\"  Enhanced: {ldm_upscaled[0].shape[1]}x{ldm_upscaled[0].shape[0]}\")\n",
    "print(f\"  Preprocessing applied: Gaussian blur (3x3, Ïƒ=0.5) + Bilateral (d=5) + Color norm (Î±=0.3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f537461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6.5: Memory Cleanup\n",
    "# To avoid OOM during KSWGD, we release the LDM UNet and other components that are no longer needed.\n",
    "import gc\n",
    "\n",
    "print(f\"GPU Memory before cleanup: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "if 'ldm_pipe' in globals():\n",
    "    # We only need the VAE for KSWGD, so we can release the UNet (the largest part)\n",
    "    # This can free up 2-4 GB of VRAM.\n",
    "    print(\"Releasing LDM UNet and other components...\")\n",
    "    if hasattr(ldm_pipe, 'unet'):\n",
    "        ldm_pipe.unet = None\n",
    "    if hasattr(ldm_pipe, 'text_encoder'):\n",
    "        ldm_pipe.text_encoder = None\n",
    "    if hasattr(ldm_pipe, 'scheduler'):\n",
    "        ldm_pipe.scheduler = None\n",
    "    \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"GPU Memory after cleanup: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bd88d5",
   "metadata": {},
   "source": [
    "## 7. KSWGD Unconditional Generation (Core Experiment)\n",
    "\n",
    "Using KSWGD (Koopman Spectral Wasserstein Gradient Descent) to perform **unconditional** particle transport generation in latent space.\n",
    "\n",
    "**Unconditional Pipeline:**\n",
    "```\n",
    "Training Data (CelebA-HQ) â†’ VAE Encoder â†’ Latent Z_tar (Target Distribution Samples)\n",
    "                                              â†“\n",
    "Random Noise N(0,1) â†’ KSWGD Transport â†’ Z_gen (Unconditional Generation)\n",
    "                                              â†“\n",
    "                                   VAE Decoder â†’ Generated Image\n",
    "```\n",
    "\n",
    "**Comparison of Two Unconditional Methods (Fair Comparison, Same Dataset):**\n",
    "| | LDM (Section 2-4) | KSWGD (Section 7) |\n",
    "|---|---|---|\n",
    "| Dataset | CelebA-HQ | CelebA-HQ |\n",
    "| Generation Process | Random Noise â†’ Denoising | Random Noise â†’ KSWGD |\n",
    "| Core Component | UNet | Kernel Matrix + Eigendecomposition |\n",
    "| Condition Input | None | None |\n",
    "| Iteration Steps | 200 | Adjustable (e.g., 300) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf49e157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KSWGD required libraries and custom kernel functions\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from tqdm.auto import trange\n",
    "\n",
    "# Import your kernel functions\n",
    "from grad_ker1 import grad_ker1\n",
    "from K_tar_eval import K_tar_eval\n",
    "\n",
    "# Try to import GPU version\n",
    "try:\n",
    "    import cupy as cp\n",
    "    from grad_ker1_gpu import grad_ker1 as grad_ker1_gpu\n",
    "    from K_tar_eval_gpu import K_tar_eval as K_tar_eval_gpu\n",
    "    GPU_KSWGD = True\n",
    "    print(\"âœ“ GPU KSWGD backend available (CuPy)\")\n",
    "except Exception as e:\n",
    "    cp = None\n",
    "    grad_ker1_gpu = None\n",
    "    K_tar_eval_gpu = None\n",
    "    GPU_KSWGD = False\n",
    "    print(f\"âœ— GPU KSWGD backend not available: {e}\")\n",
    "    print(\"  Using CPU backend instead\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3131fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode CelebA-HQ images and apply PCA dimensionality reduction\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from torchvision import transforms as T\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Image preprocessing\n",
    "transform_celebahq = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "# ============== PCA Settings ==============\n",
    "# Set USE_PCA = False to use full 1024-dim latent space (better quality, slower)\n",
    "# Set USE_PCA = True to reduce to REDUCED_DIM dimensions (faster, may lose detail)\n",
    "USE_PCA = True  # <-- Toggle this to switch between PCA and full latent space\n",
    "REDUCED_DIM = 64  # Only used if USE_PCA = True\n",
    "# ==========================================\n",
    "\n",
    "# ============== Caching Settings ==============\n",
    "# Set LOAD_FROM_CACHE = True to load pre-computed VAE encodings from cache\n",
    "# Set LOAD_FROM_CACHE = False to recompute (takes ~10 minutes)\n",
    "LOAD_FROM_CACHE = True\n",
    "CACHE_DIR = \"/workspace/kswgd/cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "# ==============================================\n",
    "\n",
    "# Define GPUPCAResult class BEFORE loading cache (required for pickle deserialization)\n",
    "class GPUPCAResult:\n",
    "    \"\"\"Custom PCA result class for GPU-accelerated PCA inverse transform\"\"\"\n",
    "    def __init__(self, components, mean, singular_values, explained_variance_ratio):\n",
    "        self.components_ = components.T  # (REDUCED_DIM, 1024)\n",
    "        self.mean_ = mean.squeeze()  # (1024,)\n",
    "        self.singular_values_ = singular_values\n",
    "        self.explained_variance_ratio_ = explained_variance_ratio\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        # X: (N, REDUCED_DIM) -> (N, 1024)\n",
    "        return X @ self.components_ + self.mean_\n",
    "\n",
    "max_samples = 28000\n",
    "\n",
    "# Generate cache filename based on settings\n",
    "pca_suffix = f\"pca{REDUCED_DIM}\" if USE_PCA else \"pca_off\"\n",
    "cache_path = os.path.join(CACHE_DIR, f\"vae_encoding_n{max_samples}_{pca_suffix}.pkl\")\n",
    "\n",
    "# Try to load from cache\n",
    "if LOAD_FROM_CACHE and os.path.exists(cache_path):\n",
    "    print(f\"Loading VAE encodings from cache: {cache_path}\")\n",
    "    with open(cache_path, 'rb') as f:\n",
    "        cache_data = pickle.load(f)\n",
    "    Z_all = cache_data['Z_all']\n",
    "    latent_dim = cache_data['latent_dim']\n",
    "    pca = cache_data['pca']\n",
    "    full_latent_shape = cache_data['full_latent_shape']\n",
    "    print(f\"âœ“ Loaded from cache!\")\n",
    "    print(f\"  Z_all shape: {Z_all.shape}\")\n",
    "    print(f\"  Latent dim: {latent_dim}\")\n",
    "    print(f\"  PCA: {'Enabled' if pca is not None else 'Disabled'}\")\n",
    "else:\n",
    "    print(f\"Cache not found or disabled. Computing VAE encodings...\")\n",
    "    \n",
    "    all_latents = []\n",
    "    vae.eval()\n",
    "    print(f\"Encoding {max_samples} images to latent space...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(min(max_samples, len(celebahq_dataset))), desc=\"Encoding\"):\n",
    "            img = celebahq_dataset[i][\"image\"]\n",
    "            img_tensor = transform_celebahq(img).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Encode to latent\n",
    "            latent = vae.encode(_to_vae_range(img_tensor))\n",
    "            latent_code = latent.latents if hasattr(latent, 'latents') else (latent.latent_dist.mode() if hasattr(latent, 'latent_dist') else latent[0])\n",
    "            latent_code = latent_code * vae_scaling # (1, 4, 16, 16)\n",
    "            \n",
    "            all_latents.append(latent_code.view(1, -1).cpu().numpy())\n",
    "\n",
    "    Z_flat = np.concatenate(all_latents, axis=0)\n",
    "    print(f\"Latent vectors collected: {Z_flat.shape}\")\n",
    "\n",
    "    # Apply PCA or use full latent space\n",
    "    if USE_PCA:\n",
    "        import time as _time\n",
    "        print(f\"Applying GPU-accelerated PCA (1024 -> {REDUCED_DIM})...\")\n",
    "        _pca_start = _time.time()\n",
    "        \n",
    "        # GPU PCA using PyTorch's pca_lowrank (much faster than sklearn CPU)\n",
    "        Z_flat_tensor = torch.from_numpy(Z_flat).float().to(device)\n",
    "        \n",
    "        # Center the data (required for PCA)\n",
    "        Z_mean_pca = Z_flat_tensor.mean(dim=0, keepdim=True)\n",
    "        Z_centered = Z_flat_tensor - Z_mean_pca\n",
    "        \n",
    "        # Compute PCA using randomized SVD (GPU-accelerated)\n",
    "        # q parameter controls oversampling for better accuracy\n",
    "        U, S, V = torch.pca_lowrank(Z_centered, q=REDUCED_DIM, niter=2)\n",
    "        \n",
    "        # Project to reduced space: Z_reduced = Z_centered @ V\n",
    "        Z_reduced_tensor = Z_centered @ V\n",
    "        Z_all = Z_reduced_tensor.cpu().numpy()\n",
    "        \n",
    "        # Store PCA components for inverse transform\n",
    "        pca_components = V.cpu().numpy()  # (1024, REDUCED_DIM)\n",
    "        pca_mean = Z_mean_pca.cpu().numpy()  # (1, 1024)\n",
    "        pca_singular_values = S.cpu().numpy()\n",
    "        \n",
    "        # Calculate explained variance ratio\n",
    "        total_var = torch.var(Z_flat_tensor, dim=0).sum().item()\n",
    "        explained_var = (S ** 2).cpu().numpy() / (Z_flat.shape[0] - 1)\n",
    "        explained_var_ratio = explained_var / (total_var + 1e-8)\n",
    "        \n",
    "        # Use the GPUPCAResult class defined above\n",
    "        pca = GPUPCAResult(pca_components, pca_mean, pca_singular_values, explained_var_ratio)\n",
    "        latent_dim = Z_all.shape[1]\n",
    "        \n",
    "        _pca_elapsed = _time.time() - _pca_start\n",
    "        print(f\"\\nâœ“ GPU PCA complete! Time: {_pca_elapsed:.2f}s\")\n",
    "        print(f\"  Backend: PyTorch GPU (torch.pca_lowrank)\")\n",
    "        print(f\"  Original latent shape: {Z_flat.shape[1]}\")\n",
    "        print(f\"  Reduced latent shape:  {Z_all.shape[1]}\")\n",
    "        print(f\"  Explained variance ratio: {np.sum(explained_var_ratio):.4f}\")\n",
    "        \n",
    "        # Clean up GPU memory\n",
    "        del Z_flat_tensor, Z_centered, U, S, V, Z_reduced_tensor, Z_mean_pca\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        print(f\"\\nâœ“ Using FULL 1024-dim latent space (no PCA)\")\n",
    "        Z_all = Z_flat  # Use full 1024-dim latent directly\n",
    "        latent_dim = Z_all.shape[1]\n",
    "        pca = None  # No PCA object\n",
    "        print(f\"  Latent dimension: {latent_dim}\")\n",
    "\n",
    "    # Record shapes for decoding\n",
    "    with torch.no_grad():\n",
    "        dummy = torch.zeros(1, 3, 256, 256, device=device)\n",
    "        dummy_latent = vae.encode(_to_vae_range(dummy))\n",
    "        dummy_code = dummy_latent.latents if hasattr(dummy_latent, 'latents') else (dummy_latent.latent_dist.mode() if hasattr(dummy_latent, 'latent_dist') else dummy_latent[0])\n",
    "        full_latent_shape = dummy_code.shape[1:] # (4, 16, 16)\n",
    "\n",
    "    # Save to cache\n",
    "    print(f\"\\nSaving to cache: {cache_path}\")\n",
    "    cache_data = {\n",
    "        'Z_all': Z_all,\n",
    "        'latent_dim': latent_dim,\n",
    "        'pca': pca,\n",
    "        'full_latent_shape': full_latent_shape,\n",
    "        'USE_PCA': USE_PCA,\n",
    "        'REDUCED_DIM': REDUCED_DIM if USE_PCA else None,\n",
    "        'max_samples': max_samples\n",
    "    }\n",
    "    with open(cache_path, 'wb') as f:\n",
    "        pickle.dump(cache_data, f)\n",
    "    print(f\"âœ“ Cache saved!\")\n",
    "\n",
    "print(f\"Full Latent Shape: {full_latent_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2746ae14",
   "metadata": {},
   "source": [
    "### 7.1 Latent Space Dimensionality Reduction: PCA\n",
    "\n",
    "We use **PCA (Principal Component Analysis)** to reduce the dimensionality of the VQ-VAE latent space from 1024 (4x16x16) to 128.\n",
    "\n",
    "**Why PCA?**\n",
    "1.  **Efficiency**: KSWGD's kernel matrix and eigendecomposition scale with the dimensionality of the particles. Reducing to 128 dimensions balances detail preservation and transport speed.\n",
    "2.  **Noise Reduction**: PCA captures the most significant variations in the latent space, effectively filtering out high-frequency noise.\n",
    "3.  **Reversibility**: We can use `pca.inverse_transform` to project the generated particles back into the 1024-dimensional space before decoding them with the VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345e7a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize latent codes and build KSWGD kernel operator\n",
    "# Standardization\n",
    "Z_mean = np.mean(Z_all, axis=0, keepdims=True)\n",
    "Z_std = np.std(Z_all, axis=0, keepdims=True) + 1e-8\n",
    "Z_std = Z_std.astype(np.float64)\n",
    "Z_mean = Z_mean.astype(np.float64)\n",
    "X_tar = ((Z_all - Z_mean) / Z_std).astype(np.float64)\n",
    "\n",
    "print(f\"After standardization: mean={X_tar.mean():.4f}, std={X_tar.std():.4f}\")\n",
    "\n",
    "# Compute squared sum of target samples (for kernel function)\n",
    "sq_tar = np.sum(X_tar ** 2, axis=1)\n",
    "\n",
    "# Compute pairwise distances and bandwidth epsilon\n",
    "dists = pairwise_distances(X_tar, metric=\"euclidean\")\n",
    "eps_kswgd = np.median(dists**2) / (2.0 * np.log(X_tar.shape[0] + 1))\n",
    "eps_kswgd = float(max(eps_kswgd, 1e-6))\n",
    "\n",
    "print(f\"KSWGD epsilon: {eps_kswgd:.6f}\")\n",
    "print(f\"Distance stats: min={dists[dists>0].min():.4f}, median={np.median(dists):.4f}, max={dists.max():.4f}\")\n",
    "\n",
    "# Build data kernel matrix\n",
    "data_kernel = np.exp(-dists**2 / (2.0 * eps_kswgd))\n",
    "\n",
    "# Normalization\n",
    "p_x = np.sqrt(np.sum(data_kernel, axis=1))\n",
    "data_kernel_norm = data_kernel / (p_x[:, None] * p_x[None, :] + 1e-12)\n",
    "D_y = np.sum(data_kernel_norm, axis=0)\n",
    "rw_kernel = 0.5 * (data_kernel_norm / (D_y + 1e-12) + data_kernel_norm / (D_y[:, None] + 1e-12))\n",
    "rw_kernel = np.nan_to_num(rw_kernel)\n",
    "\n",
    "print(f\"Kernel matrix built, shape: {rw_kernel.shape}\")\n",
    "\n",
    "# latent_dim is already set in the previous cell (Spatial Mean Pooling)\n",
    "print(f\"KSWGD latent_dim: {latent_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20772169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute eigendecomposition and KSWGD weights\n",
    "import time\n",
    "import threading\n",
    "from scipy.sparse.linalg import eigsh\n",
    "\n",
    "print(\"Computing eigendecomposition...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# ============== Truncated Eigendecomposition Settings ==============\n",
    "# k: number of largest eigenvalues to compute\n",
    "# Using truncated decomposition is much faster for large matrices (e.g., 30000x30000)\n",
    "k_eig = 300  # Default: compute top 300 eigenvalues (truncated)\n",
    "# k_eig = rw_kernel.shape[0]  # Uncomment this line to compute ALL eigenvalues (full decomposition)\n",
    "USE_GPU_EIGSH = True  # Use GPU for truncated eigendecomposition (torch.lobpcg)\n",
    "# ===================================================================\n",
    "\n",
    "n_samples = rw_kernel.shape[0]\n",
    "use_truncated = k_eig < n_samples\n",
    "\n",
    "# Estimate time based on matrix size\n",
    "if use_truncated:\n",
    "    est_time = n_samples * k_eig / 1e7  # Rough estimate for truncated\n",
    "else:\n",
    "    est_time = (n_samples ** 2.5) / 1e9  # O(n^3) but optimized\n",
    "\n",
    "# Progress tracking with elapsed time display\n",
    "_eig_done = threading.Event()\n",
    "_eig_result = {}\n",
    "\n",
    "def _show_progress():\n",
    "    \"\"\"Show elapsed time while eigendecomposition is running\"\"\"\n",
    "    import sys\n",
    "    symbols = ['â ‹', 'â ™', 'â ¹', 'â ¸', 'â ¼', 'â ´', 'â ¦', 'â §', 'â ‡', 'â ']\n",
    "    idx = 0\n",
    "    while not _eig_done.is_set():\n",
    "        elapsed = time.time() - start_time\n",
    "        sys.stdout.write(f\"\\r  {symbols[idx]} Eigendecomposition in progress... {elapsed:.1f}s elapsed\")\n",
    "        sys.stdout.flush()\n",
    "        idx = (idx + 1) % len(symbols)\n",
    "        _eig_done.wait(0.2)\n",
    "    sys.stdout.write(\"\\r\" + \" \" * 60 + \"\\r\")  # Clear line\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def _run_eigsh_cpu():\n",
    "    \"\"\"Run truncated eigendecomposition on CPU (scipy)\"\"\"\n",
    "    lambda_ns_partial, phi_partial = eigsh(rw_kernel, k=k_eig, which='LM')\n",
    "    _eig_result['lambda'] = lambda_ns_partial\n",
    "    _eig_result['phi'] = phi_partial\n",
    "    _eig_done.set()\n",
    "\n",
    "def _run_eigsh_gpu():\n",
    "    \"\"\"Run truncated eigendecomposition on GPU (torch.lobpcg)\"\"\"\n",
    "    # torch.lobpcg computes largest eigenvalues of symmetric positive definite matrices\n",
    "    rw_kernel_torch = torch.from_numpy(rw_kernel).float().to(device)\n",
    "    \n",
    "    # Random initial vectors for lobpcg\n",
    "    X0 = torch.randn(n_samples, k_eig, device=device, dtype=torch.float32)\n",
    "    \n",
    "    # lobpcg for largest eigenvalues (works well for positive semi-definite matrices)\n",
    "    try:\n",
    "        eigenvalues, eigenvectors = torch.lobpcg(rw_kernel_torch, k=k_eig, X=X0, largest=True, niter=100)\n",
    "        _eig_result['lambda'] = eigenvalues.cpu().numpy()\n",
    "        _eig_result['phi'] = eigenvectors.cpu().numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"\\n  Warning: GPU lobpcg failed ({e}), falling back to CPU...\")\n",
    "        _eig_done.set()\n",
    "        return False\n",
    "    \n",
    "    del rw_kernel_torch, X0, eigenvalues, eigenvectors\n",
    "    torch.cuda.empty_cache()\n",
    "    _eig_done.set()\n",
    "    return True\n",
    "\n",
    "def _run_eigh_gpu():\n",
    "    \"\"\"Run full eigendecomposition on GPU\"\"\"\n",
    "    rw_kernel_torch = torch.from_numpy(rw_kernel).to(device)\n",
    "    lambda_ns_torch, phi_torch = torch.linalg.eigh(rw_kernel_torch)\n",
    "    _eig_result['lambda'] = lambda_ns_torch.cpu().numpy()[::-1].copy()\n",
    "    _eig_result['phi'] = phi_torch.cpu().numpy()[:, ::-1].copy()\n",
    "    del rw_kernel_torch, lambda_ns_torch, phi_torch\n",
    "    torch.cuda.empty_cache()\n",
    "    _eig_done.set()\n",
    "\n",
    "def _run_eigh_cpu():\n",
    "    \"\"\"Run full eigendecomposition on CPU\"\"\"\n",
    "    lambda_ns_raw, phi_raw = np.linalg.eigh(rw_kernel)\n",
    "    _eig_result['lambda'] = lambda_ns_raw[::-1]\n",
    "    _eig_result['phi'] = phi_raw[:, ::-1]\n",
    "    _eig_done.set()\n",
    "\n",
    "if use_truncated:\n",
    "    print(f\"  Using TRUNCATED eigendecomposition (top {k_eig} of {n_samples})...\")\n",
    "    \n",
    "    # Try GPU first if enabled\n",
    "    use_gpu_eig = USE_GPU_EIGSH and torch.cuda.is_available()\n",
    "    \n",
    "    if use_gpu_eig:\n",
    "        print(f\"  Backend: PyTorch GPU (torch.lobpcg)\")\n",
    "        print(f\"  Estimated time: ~{est_time/5:.0f}-{est_time:.0f} seconds (GPU accelerated)\")\n",
    "        \n",
    "        # Start progress display thread\n",
    "        progress_thread = threading.Thread(target=_show_progress, daemon=True)\n",
    "        progress_thread.start()\n",
    "        \n",
    "        # Try GPU lobpcg\n",
    "        gpu_success = _run_eigsh_gpu()\n",
    "        progress_thread.join(timeout=1)\n",
    "        \n",
    "        if not gpu_success or 'lambda' not in _eig_result:\n",
    "            # Fallback to CPU\n",
    "            print(f\"  Falling back to CPU (scipy.sparse.linalg.eigsh)...\")\n",
    "            _eig_done.clear()\n",
    "            _eig_result.clear()\n",
    "            start_time = time.time()  # Reset timer\n",
    "            progress_thread = threading.Thread(target=_show_progress, daemon=True)\n",
    "            progress_thread.start()\n",
    "            _run_eigsh_cpu()\n",
    "            progress_thread.join(timeout=1)\n",
    "    else:\n",
    "        print(f\"  Backend: CPU (scipy.sparse.linalg.eigsh)\")\n",
    "        print(f\"  Estimated time: ~{est_time:.0f}-{est_time*2:.0f} seconds\")\n",
    "        \n",
    "        # Start progress display thread\n",
    "        progress_thread = threading.Thread(target=_show_progress, daemon=True)\n",
    "        progress_thread.start()\n",
    "        \n",
    "        # Run CPU eigendecomposition\n",
    "        _run_eigsh_cpu()\n",
    "        progress_thread.join(timeout=1)\n",
    "    \n",
    "    # Sort in descending order\n",
    "    sort_idx = np.argsort(_eig_result['lambda'])[::-1]\n",
    "    lambda_ns = _eig_result['lambda'][sort_idx]\n",
    "    phi = _eig_result['phi'][:, sort_idx]\n",
    "else:\n",
    "    print(f\"  Using FULL eigendecomposition ({n_samples} x {n_samples})...\")\n",
    "    print(f\"  Estimated time: ~{est_time:.0f}-{est_time*3:.0f} seconds\")\n",
    "    \n",
    "    # Start progress display thread\n",
    "    progress_thread = threading.Thread(target=_show_progress, daemon=True)\n",
    "    progress_thread.start()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(\"    Using GPU acceleration...\")\n",
    "        _run_eigh_gpu()\n",
    "    else:\n",
    "        print(\"    Using CPU...\")\n",
    "        _run_eigh_cpu()\n",
    "    \n",
    "    progress_thread.join(timeout=1)\n",
    "    lambda_ns = _eig_result['lambda']\n",
    "    phi = _eig_result['phi']\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"âœ“ Eigendecomposition complete! Time: {elapsed:.1f}s\")\n",
    "\n",
    "# Set regularization parameters\n",
    "tol = 1e-6\n",
    "reg = 1e-3\n",
    "latent_dim = X_tar.shape[1]\n",
    "\n",
    "# Compute inverse eigenvalues (fix negative number issue)\n",
    "lambda_ = lambda_ns - 1.0\n",
    "inv_lambda = np.zeros_like(lambda_)\n",
    "# Only compute inverse for positive lambda_, set negative to 0\n",
    "positive_mask = lambda_[1:] > tol\n",
    "inv_lambda[1:][positive_mask] = 1.0 / (np.abs(lambda_[1:][positive_mask]) + reg)\n",
    "inv_lambda *= eps_kswgd\n",
    "\n",
    "# Count eigenvalues above tolerance (always compute this regardless of truncation)\n",
    "above_tol = int(np.sum(lambda_ns >= tol))\n",
    "\n",
    "# Truncate small eigenvalues\n",
    "lambda_ns_inv = np.zeros_like(lambda_ns)\n",
    "mask = lambda_ns >= tol\n",
    "lambda_ns_inv[mask] = eps_kswgd / (lambda_ns[mask] + reg)\n",
    "phi_trunc = phi[:, :above_tol]\n",
    "lambda_ns_s_ns = (lambda_ns_inv * inv_lambda * lambda_ns_inv)[:above_tol]\n",
    "\n",
    "# Clean NaN and Inf\n",
    "lambda_ns_s_ns = np.nan_to_num(lambda_ns_s_ns, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# Target distribution weights\n",
    "p_tar = np.sum(data_kernel, axis=0)\n",
    "sqrt_p = np.sqrt(p_tar + 1e-12)\n",
    "D_vec = np.sum(data_kernel / sqrt_p[:, None] / sqrt_p[None, :], axis=1)\n",
    "\n",
    "print(f\"\\nâœ“ Eigendecomposition Summary:\")\n",
    "print(f\"  Matrix size: {n_samples} x {n_samples}\")\n",
    "print(f\"  Computed eigenvalues: {len(lambda_ns)} ({'truncated' if use_truncated else 'full'})\")\n",
    "print(f\"  Eigenvalues >= {tol}: {above_tol}\")\n",
    "print(f\"  Retained eigenvectors: {above_tol}\")\n",
    "print(f\"  Top 10 eigenvalues: {lambda_ns[:10]}\")\n",
    "print(f\"  lambda_ns_s_ns stats: min={lambda_ns_s_ns.min():.6f}, max={lambda_ns_s_ns.max():.6f}, non-zero={np.sum(lambda_ns_s_ns != 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b1fe7d",
   "metadata": {},
   "source": [
    "### 6.2 EDMD Dictionary Learning for True Koopman Operator (KSWGD)\n",
    "\n",
    "The previous cell computed the **Diffusion Map** spectral decomposition (DMPS method).\n",
    "Now we implement the **true KSWGD** using EDMD (Extended Dynamic Mode Decomposition):\n",
    "\n",
    "1. **KDE-based drift estimation**: Estimate the score function $\\nabla \\log p(x)$ using kernel density estimation\n",
    "2. **Langevin evolution**: Generate time-series pairs $(X_t, X_{t+\\Delta t})$ via Langevin dynamics\n",
    "3. **Dictionary learning**: Learn a sparse dictionary on the latent space\n",
    "4. **EDMD operator**: Build the Koopman operator in dictionary space\n",
    "\n",
    "| Method | Basis Functions | Dynamics |\n",
    "|--------|----------------|----------|\n",
    "| DMPS | Diffusion map eigenvectors | Static kernel |\n",
    "| KSWGD (EDMD) | Dictionary + Koopman eigenfunctions | Learned Langevin dynamics |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad929e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE-based drift estimation and Langevin evolution to build EDMD pairs\n",
    "from sklearn.decomposition import MiniBatchDictionaryLearning\n",
    "from scipy.linalg import eig\n",
    "\n",
    "print(\"=== EDMD Pipeline for True Koopman KSWGD ===\")\n",
    "\n",
    "# Step 1: KDE-based score function estimation\n",
    "dt_edmd = 0.1  # time step for EDMD Langevin evolution\n",
    "dist2_edmd = pairwise_distances(X_tar, metric=\"sqeuclidean\")  # squared distances\n",
    "h_edmd = np.sqrt(np.median(dist2_edmd) + 1e-12)  # KDE bandwidth\n",
    "W_edmd = np.exp(-dist2_edmd / (2.0 * (h_edmd ** 2)))\n",
    "sumW_edmd = np.sum(W_edmd, axis=1, keepdims=True) + 1e-12\n",
    "weighted_means_edmd = W_edmd @ X_tar / sumW_edmd\n",
    "score_edmd = (weighted_means_edmd - X_tar) / (h_edmd ** 2)  # KDE score = drift term\n",
    "\n",
    "# Step 2: Langevin step to generate (X_t, X_{t+dt}) pairs\n",
    "xi_edmd = np.random.normal(0.0, 1.0, size=X_tar.shape)\n",
    "X_tar_next = X_tar + dt_edmd * score_edmd + np.sqrt(2.0 * dt_edmd) * xi_edmd\n",
    "\n",
    "print(f\"EDMD drift bandwidth h: {h_edmd:.4f}\")\n",
    "print(f\"X_tar stats -> mean {X_tar.mean():.4f}, std {X_tar.std():.4f}\")\n",
    "print(f\"X_tar_next stats -> mean {X_tar_next.mean():.4f}, std {X_tar_next.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe0ed5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Dictionary learning on latent space\n",
    "# Rule of thumb: dictionary size = 3-6x latent_dim, with N/m > 50 to avoid overfitting\n",
    "# For N=30000, d=64: recommended m = 300-500 (gives N/m = 60-100)\n",
    "n_dict_components = 200  # Dictionary atoms (total basis = n_dict_components + 1 for constant)\n",
    "dict_alpha = 1e-3  # Sparsity penalty\n",
    "dict_batch = 256\n",
    "dict_max_iter = 500\n",
    "dict_random_state = 42\n",
    "\n",
    "print(f\"\\n=== Dictionary Learning ===\")\n",
    "print(f\"Data: N={X_tar.shape[0]} samples, d={X_tar.shape[1]} dimensions\")\n",
    "print(f\"Dictionary: {n_dict_components} atoms â†’ {n_dict_components + 1} total basis functions\")\n",
    "print(f\"Oversampling ratio: N/m = {X_tar.shape[0] / (n_dict_components + 1):.1f}x (recommended > 50)\")\n",
    "print(f\"Learning dictionary...\")\n",
    "\n",
    "dict_model = MiniBatchDictionaryLearning(\n",
    "    n_components=n_dict_components,\n",
    "    alpha=dict_alpha,\n",
    "    batch_size=dict_batch,\n",
    "    max_iter=dict_max_iter,\n",
    "    random_state=dict_random_state,\n",
    "    verbose=0,\n",
    "    fit_algorithm=\"lars\"\n",
    ")\n",
    "dict_model.fit(X_tar)\n",
    "\n",
    "# Transform to dictionary space and add constant term\n",
    "Phi_X = dict_model.transform(X_tar)\n",
    "Phi_Y = dict_model.transform(X_tar_next)\n",
    "Phi_X = np.hstack([np.ones((Phi_X.shape[0], 1)), Phi_X])  # Add constant basis\n",
    "Phi_Y = np.hstack([np.ones((Phi_Y.shape[0], 1)), Phi_Y])\n",
    "\n",
    "print(f\"Dictionary atoms shape: {dict_model.components_.shape}\")\n",
    "print(f\"Dictionary codes (current) Phi_X: {Phi_X.shape}\")\n",
    "print(f\"Dictionary codes (next) Phi_Y: {Phi_Y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e6d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Build EDMD Koopman operator in dictionary space\n",
    "print(\"\\n=== EDMD Koopman Operator ===\")\n",
    "\n",
    "reg_edmd = 1e-3\n",
    "N_edmd, m_edmd = Phi_X.shape\n",
    "\n",
    "# EDMD: K = (Phi_X^T Phi_X + reg*I)^{-1} Phi_X^T Phi_Y\n",
    "G_edmd = (Phi_X.T @ Phi_X) / N_edmd + reg_edmd * np.eye(m_edmd)\n",
    "A_edmd = (Phi_X.T @ Phi_Y) / N_edmd\n",
    "\n",
    "# Generalized eigenvalue problem: A v = lambda G v\n",
    "eigvals_edmd, eigvecs_edmd = eig(A_edmd, G_edmd)\n",
    "idx_edmd = np.argsort(-eigvals_edmd.real)\n",
    "eigvals_edmd = eigvals_edmd[idx_edmd]\n",
    "eigvecs_edmd = eigvecs_edmd[:, idx_edmd]\n",
    "\n",
    "# Koopman eigenfunctions evaluated at data points\n",
    "efuns_edmd = Phi_X @ eigvecs_edmd\n",
    "\n",
    "print(f\"EDMD eigenvalues (first 10): {np.round(eigvals_edmd[:10].real, 4)}\")\n",
    "print(f\"EDMD eigenfunctions shape: {efuns_edmd.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc74646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Prepare EDMD-derived KSWGD weights (skip constant mode)\n",
    "print(\"\\n=== Prepare EDMD-KSWGD Spectral Weights ===\")\n",
    "\n",
    "lambda_ns_edmd = eigvals_edmd.real\n",
    "lambda_gen_edmd = (lambda_ns_edmd - 1.0) / dt_edmd  # Generator eigenvalues\n",
    "\n",
    "mode_skip_edmd = 1  # Skip the constant eigenfunction (eigenvalue ~ 1)\n",
    "eig_threshold_edmd = 1e-6  # Keep modes with eigenvalue > threshold\n",
    "\n",
    "valid_idx_edmd = np.arange(mode_skip_edmd, lambda_ns_edmd.shape[0])\n",
    "valid_mask_edmd = lambda_ns_edmd[mode_skip_edmd:] > eig_threshold_edmd\n",
    "valid_idx_edmd = valid_idx_edmd[valid_mask_edmd]\n",
    "\n",
    "if valid_idx_edmd.size == 0:\n",
    "    raise RuntimeError(\"No EDMD modes survived the threshold; adjust eig_threshold_edmd or dictionary size.\")\n",
    "\n",
    "# Truncated Koopman eigenfunctions for KSWGD\n",
    "phi_trunc_edmd = np.real(efuns_edmd[:, valid_idx_edmd])\n",
    "\n",
    "# Generator inverse weights\n",
    "lambda_gen_inv_edmd = np.zeros_like(lambda_gen_edmd)\n",
    "mask_nonzero_edmd = np.abs(lambda_gen_edmd) > 1e-6\n",
    "lambda_gen_inv_edmd[mask_nonzero_edmd] = 1.0 / lambda_gen_edmd[mask_nonzero_edmd]\n",
    "lambda_ns_s_ns_edmd = lambda_gen_inv_edmd[valid_idx_edmd].real\n",
    "\n",
    "print(f\"EDMD kept {valid_idx_edmd.size} Koopman modes (threshold {eig_threshold_edmd})\")\n",
    "print(f\"phi_trunc_edmd shape: {phi_trunc_edmd.shape}\")\n",
    "print(f\"lambda_ns_s_ns_edmd stats: min={lambda_ns_s_ns_edmd.min():.6f}, max={lambda_ns_s_ns_edmd.max():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d399ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Diffusion Map (DMPS) and EDMD (KSWGD) spectra\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_show_eigs = 20\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Eigenvalue comparison\n",
    "ax1 = axes[0]\n",
    "ax1.semilogy(range(1, n_show_eigs + 1), lambda_ns[:n_show_eigs], 'o-', label=\"DMPS (Diffusion Map)\", markersize=6)\n",
    "ax1.semilogy(range(1, min(n_show_eigs, lambda_ns_edmd.size) + 1), \n",
    "             lambda_ns_edmd[:n_show_eigs], '^-', label=\"KSWGD (EDMD Koopman)\", markersize=6)\n",
    "ax1.set_xlabel(\"Eigen-index\")\n",
    "ax1.set_ylabel(\"Eigenvalue (log scale)\")\n",
    "ax1.set_title(\"Spectral Decay Comparison\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Generator eigenvalues\n",
    "ax2 = axes[1]\n",
    "lambda_gen_dmps = (lambda_ns[:n_show_eigs] - 1.0) / eps_kswgd\n",
    "ax2.plot(range(1, n_show_eigs + 1), lambda_gen_dmps, 'o-', label=\"DMPS Generator\", markersize=6)\n",
    "ax2.plot(range(1, min(n_show_eigs, lambda_gen_edmd.size) + 1), \n",
    "         lambda_gen_edmd[:n_show_eigs], '^-', label=\"EDMD Generator\", markersize=6)\n",
    "ax2.set_xlabel(\"Eigen-index\")\n",
    "ax2.set_ylabel(\"Generator Eigenvalue\")\n",
    "ax2.set_title(\"Generator Spectrum Comparison\")\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/dmps_vs_kswgd_spectrum.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== Method Comparison ===\")\n",
    "print(f\"DMPS: {above_tol} diffusion map modes\")\n",
    "print(f\"KSWGD: {valid_idx_edmd.size} Koopman modes from EDMD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21691815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define unified sampler supporting both DMPS and KSWGD (EDMD) methods\n",
    "def run_particle_sampler(num_particles=16, num_iters=200, step_size=0.05, rng_seed=42, method=\"dmps\"):\n",
    "    \"\"\"\n",
    "    Unified Particle Transport Sampler\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    method : str\n",
    "        \"dmps\" - Diffusion Map Particle Sampling (uses diffusion map eigenvectors)\n",
    "        \"kswgd\" - True KSWGD with EDMD Koopman operator\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "    use_gpu = GPU_KSWGD and torch.cuda.is_available()\n",
    "    xp = cp if use_gpu else np\n",
    "    grad_fn = grad_ker1_gpu if use_gpu else grad_ker1\n",
    "    K_eval_fn = K_tar_eval_gpu if use_gpu else K_tar_eval\n",
    "    \n",
    "    method = method.lower()\n",
    "    if method == \"dmps\":\n",
    "        method_name = \"DMPS (Diffusion Map)\"\n",
    "        phi_use = phi_trunc\n",
    "        lambda_use = lambda_ns_s_ns\n",
    "    elif method == \"kswgd\":\n",
    "        method_name = \"KSWGD (EDMD Koopman)\"\n",
    "        phi_use = phi_trunc_edmd\n",
    "        lambda_use = lambda_ns_s_ns_edmd\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}. Use 'dmps' or 'kswgd'.\")\n",
    "    \n",
    "    print(f\"Method: {method_name}\")\n",
    "    print(f\"Backend: {'GPU (CuPy)' if use_gpu else 'CPU (NumPy)'}\")\n",
    "    print(f\"Eigenfunctions shape: {phi_use.shape}\")\n",
    "    \n",
    "    x_hist = xp.zeros((num_particles, latent_dim, num_iters), dtype=xp.float64)\n",
    "    init_particles = rng.normal(0.0, 1.0, size=(num_particles, latent_dim))\n",
    "    x_hist[:, :, 0] = xp.asarray(init_particles)\n",
    "    \n",
    "    if use_gpu:\n",
    "        X_tar_dev = cp.asarray(X_tar)\n",
    "        p_tar_dev = cp.asarray(p_tar)\n",
    "        sq_tar_dev = cp.asarray(sq_tar)\n",
    "        D_vec_dev = cp.asarray(D_vec)\n",
    "        phi_dev = cp.asarray(phi_use)\n",
    "        lambda_dev = cp.asarray(lambda_use)\n",
    "    else:\n",
    "        X_tar_dev, p_tar_dev, sq_tar_dev, D_vec_dev = X_tar, p_tar, sq_tar, D_vec\n",
    "        phi_dev, lambda_dev = phi_use, lambda_use\n",
    "    \n",
    "    iterator = trange(num_iters - 1, desc=f\"{method.upper()} Transport\", unit=\"step\")\n",
    "    for t in iterator:\n",
    "        current = x_hist[:, :, t]\n",
    "        grad_matrix = grad_fn(current, X_tar_dev, p_tar_dev, sq_tar_dev, D_vec_dev, eps_kswgd)\n",
    "        cross_matrix = K_eval_fn(X_tar_dev, current, p_tar_dev, sq_tar_dev, D_vec_dev, eps_kswgd)\n",
    "        \n",
    "        tmp = (phi_dev.T @ cross_matrix) * lambda_dev[:, None]\n",
    "        push = phi_dev @ tmp\n",
    "        \n",
    "        for dim in range(latent_dim):\n",
    "            sum_term = grad_matrix[:, :, dim] @ push\n",
    "            x_hist[:, dim, t + 1] = x_hist[:, dim, t] - (step_size / num_particles) * xp.sum(sum_term, axis=1)\n",
    "        \n",
    "        if (t + 1) % 50 == 0:\n",
    "            step_norm = x_hist[:, :, t + 1] - x_hist[:, :, t]\n",
    "            mean_disp = float(xp.mean(xp.linalg.norm(step_norm, axis=1)))\n",
    "            iterator.set_postfix({\"mean_step\": f\"{mean_disp:.3e}\"})\n",
    "            if bool(xp.any(xp.isnan(x_hist[:, :, t + 1]))):\n",
    "                print(f\"\\nWarning: NaN detected at step {t+1}\")\n",
    "                return np.asarray(xp.asnumpy(x_hist[:, :, t]) if use_gpu else x_hist[:, :, t], dtype=np.float64)\n",
    "    \n",
    "    return np.asarray(xp.asnumpy(x_hist[:, :, -1]) if use_gpu else x_hist[:, :, -1], dtype=np.float64)\n",
    "\n",
    "\n",
    "# Backward compatibility alias\n",
    "def run_kswgd_sampler(num_particles=16, num_iters=200, step_size=0.05, rng_seed=42):\n",
    "    \"\"\"Legacy wrapper - uses DMPS by default for backward compatibility\"\"\"\n",
    "    return run_particle_sampler(num_particles, num_iters, step_size, rng_seed, method=\"dmps\")\n",
    "\n",
    "\n",
    "def decode_latents_to_images(flat_latents_std):\n",
    "    \"\"\"Decode standardized latent vectors to images (supports both PCA and non-PCA modes)\"\"\"\n",
    "    # 1. De-standardize\n",
    "    flat_latents = flat_latents_std * Z_std + Z_mean\n",
    "    \n",
    "    # 2. PCA Inverse Transform (only if PCA was used)\n",
    "    if USE_PCA and pca is not None:\n",
    "        latents_recovered = pca.inverse_transform(flat_latents)\n",
    "    else:\n",
    "        latents_recovered = flat_latents  # Already in 1024-dim space\n",
    "    \n",
    "    # 3. Reshape to (N, 4, 16, 16)\n",
    "    latents_tensor = torch.from_numpy(latents_recovered).float().view(-1, *full_latent_shape).to(device)\n",
    "    \n",
    "    # 4. VAE decode\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        decoded = vae.decode(latents_tensor / vae_scaling).sample\n",
    "        decoded_rgb = _from_vae_range(decoded)\n",
    "    \n",
    "    return decoded_rgb.cpu()\n",
    "\n",
    "print(\"Particle samplers and Decoder defined!\")\n",
    "print(f\"  Latent mode: {'PCA (' + str(REDUCED_DIM) + '-dim)' if USE_PCA else 'Full latent (1024-dim)'}\")\n",
    "print(f\"  Available methods:\")\n",
    "print(f\"    - 'dmps': Diffusion Map Particle Sampling ({above_tol} modes)\")\n",
    "print(f\"    - 'kswgd': EDMD Koopman KSWGD ({valid_idx_edmd.size} modes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcef6e8",
   "metadata": {},
   "source": [
    "### 7.1 Run Generation: DMPS vs KSWGD Comparison\n",
    "\n",
    "Now we run both methods and compare results:\n",
    "- **DMPS**: Diffusion Map Particle Sampling (faster, static kernel)\n",
    "- **KSWGD**: EDMD-based Koopman Spectral Wasserstein Gradient Descent (learns dynamics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8b93d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== Spectral Memory Cleanup ==============\n",
    "import gc\n",
    "import sys\n",
    "import torch\n",
    "import psutil\n",
    "\n",
    "print(\"Starting spectral memory cleanup after eigendecomposition...\")\n",
    "\n",
    "# 1. Record memory usage before cleanup\n",
    "print(\"\\n=== Memory Usage BEFORE Cleanup ===\")\n",
    "cpu_mem_before = psutil.virtual_memory().used / 1e9\n",
    "gpu_mem_before = []\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    gpu_allocated = torch.cuda.memory_allocated(i) / 1e9\n",
    "    gpu_reserved = torch.cuda.memory_reserved(i) / 1e9\n",
    "    gpu_mem_before.append((gpu_allocated, gpu_reserved))\n",
    "    print(f\"GPU {i}: Allocated: {gpu_allocated:.2f} GB, Reserved: {gpu_reserved:.2f} GB\")\n",
    "print(f\"CPU RAM: {cpu_mem_before:.2f} GB\")\n",
    "\n",
    "# 2. Calculate and print sizes of variables to be deleted\n",
    "print(f\"\\n=== Variables to be Released (max_samples = {max_samples}) ===\")\n",
    "memory_to_release = 0.0\n",
    "variables_info = {}\n",
    "\n",
    "# Variables that can be safely deleted after eigendecomposition\n",
    "deletable_vars = [\n",
    "    'dists',           # Distance matrix: (max_samples, max_samples) - huge!  \n",
    "    'data_kernel',     # Original kernel matrix: (max_samples, max_samples)\n",
    "    'rw_kernel',       # Random walk kernel: (max_samples, max_samples)  \n",
    "    'phi',             # Full eigenvector matrix (before truncation)\n",
    "    'lambda_ns',       # Full eigenvalue array (before truncation)\n",
    "    'Z_flat',          # Original flattened latent vectors\n",
    "    'all_latents',     # List of latent vectors\n",
    "    '_eig_result',     # Temporary eigendecomposition results\n",
    "]\n",
    "\n",
    "for var in deletable_vars:\n",
    "    if var in globals():\n",
    "        obj = globals()[var]\n",
    "        size_bytes = 0\n",
    "        if hasattr(obj, 'nbytes'):\n",
    "            size_bytes = obj.nbytes\n",
    "        elif hasattr(obj, '__sizeof__'):\n",
    "            size_bytes = obj.__sizeof__()\n",
    "        elif isinstance(obj, (list, dict)):\n",
    "            size_bytes = sys.getsizeof(obj)\n",
    "            if isinstance(obj, list) and len(obj) > 0:\n",
    "                # Estimate size for list of arrays\n",
    "                if hasattr(obj[0], 'nbytes'):\n",
    "                    size_bytes += sum(item.nbytes for item in obj if hasattr(item, 'nbytes'))\n",
    "        \n",
    "        size_gb = size_bytes / 1e9\n",
    "        memory_to_release += size_gb\n",
    "        variables_info[var] = size_gb\n",
    "        print(f\"  {var}: {size_gb:.3f} GB\")\n",
    "\n",
    "print(f\"Total estimated memory to release: {memory_to_release:.2f} GB\")\n",
    "\n",
    "# 3. Ensure spectral info independence before deletion\n",
    "print(f\"\\n=== Ensuring Spectral Data Independence ===\")\n",
    "\n",
    "# Ensure phi_trunc is independent from phi\n",
    "if 'phi' in globals() and 'phi_trunc' in globals():\n",
    "    if globals()['phi'] is globals()['phi_trunc']:\n",
    "        print(\"âš ï¸  phi and phi_trunc are the same object - creating independent copy\")\n",
    "        globals()['phi_trunc'] = globals()['phi_trunc'].copy()\n",
    "        print(\"âœ“ Made phi_trunc an independent copy.\")\n",
    "    else:\n",
    "        print(\"âœ“ phi_trunc is already independent from phi\")\n",
    "\n",
    "# 4. Clear Python tracebacks (which can hold references)\n",
    "sys.last_traceback = None\n",
    "sys.last_value = None\n",
    "sys.last_type = None\n",
    "\n",
    "# 5. Perform the actual deletion\n",
    "print(f\"\\n=== Deleting Large Matrices ===\")\n",
    "total_released = 0.0\n",
    "\n",
    "for var in deletable_vars:\n",
    "    if var in globals():\n",
    "        size_gb = variables_info.get(var, 0)\n",
    "        del globals()[var]\n",
    "        total_released += size_gb\n",
    "        print(f\"âœ“ Deleted {var} ({size_gb:.3f} GB)\")\n",
    "\n",
    "# 6. Clean up any remaining CuPy arrays\n",
    "if 'cp' in dir() and cp is not None:\n",
    "    cp_vars_found = []\n",
    "    for name in list(globals().keys()):\n",
    "        if isinstance(globals().get(name), type(cp.ndarray(1))) if cp is not None else False:\n",
    "            cp_vars_found.append(name)\n",
    "            del globals()[name]\n",
    "    if cp_vars_found:\n",
    "        print(f\"âœ“ Deleted CuPy arrays from globals: {', '.join(cp_vars_found)}\")\n",
    "\n",
    "# 7. Force memory release\n",
    "print(f\"\\n=== Forcing Memory Release ===\")\n",
    "# Python garbage collection\n",
    "for _ in range(3):\n",
    "    n_collected = gc.collect()\n",
    "    if n_collected > 0:\n",
    "        print(f\"âœ“ Python GC collected {n_collected} objects\")\n",
    "\n",
    "# CuPy memory pool cleanup\n",
    "if 'cp' in dir() and cp is not None:\n",
    "    try:\n",
    "        mempool = cp.get_default_memory_pool()\n",
    "        pinned_mempool = cp.get_default_pinned_memory_pool()\n",
    "        \n",
    "        for _ in range(3):\n",
    "            mempool.free_all_blocks()\n",
    "            pinned_mempool.free_all_blocks()\n",
    "        \n",
    "        print(f\"âœ“ CuPy memory pool cleared and blocks freed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— CuPy cleanup error: {e}\")\n",
    "\n",
    "# PyTorch cleanup\n",
    "torch.cuda.empty_cache()\n",
    "print(\"âœ“ PyTorch CUDA cache emptied\")\n",
    "\n",
    "# 8. Record memory usage after cleanup  \n",
    "print(f\"\\n=== Memory Usage AFTER Cleanup ===\")\n",
    "cpu_mem_after = psutil.virtual_memory().used / 1e9\n",
    "gpu_mem_after = []\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    gpu_allocated = torch.cuda.memory_allocated(i) / 1e9\n",
    "    gpu_reserved = torch.cuda.memory_reserved(i) / 1e9\n",
    "    gpu_mem_after.append((gpu_allocated, gpu_reserved))\n",
    "    total = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
    "    print(f\"GPU {i}: Allocated: {gpu_allocated:.2f} GB, Reserved: {gpu_reserved:.2f} GB, Free: {total - gpu_allocated:.2f} GB\")\n",
    "print(f\"CPU RAM: {cpu_mem_after:.2f} GB\")\n",
    "\n",
    "# 9. Calculate and report memory savings\n",
    "print(f\"\\n=== Memory Cleanup Summary ===\")\n",
    "cpu_released = cpu_mem_before - cpu_mem_after\n",
    "print(f\"CPU RAM released: {cpu_released:.2f} GB\")\n",
    "\n",
    "for i in range(len(gpu_mem_before)):\n",
    "    gpu_released_allocated = gpu_mem_before[i][0] - gpu_mem_after[i][0]  \n",
    "    gpu_released_reserved = gpu_mem_before[i][1] - gpu_mem_after[i][1]\n",
    "    print(f\"GPU {i} memory released: Allocated: {gpu_released_allocated:.2f} GB, Reserved: {gpu_released_reserved:.2f} GB\")\n",
    "\n",
    "print(f\"Estimated variables released: {total_released:.2f} GB\")\n",
    "\n",
    "# 10. Verify essential spectral data is preserved\n",
    "print(f\"\\n=== Essential Spectral Data Verification ===\")\n",
    "essential_vars = [\n",
    "    'phi_trunc',        # Truncated eigenvectors (n_samples, above_tol)\n",
    "    'lambda_ns_s_ns',   # Processed eigenvalue weights (above_tol,)\n",
    "    'X_tar',            # Standardized target samples (n_samples, latent_dim)\n",
    "    'p_tar',            # Target distribution weights (n_samples,)\n",
    "    'sq_tar',           # Squared sums of target samples (n_samples,) \n",
    "    'D_vec',            # Normalization vector (n_samples,)\n",
    "    'eps_kswgd',        # Kernel bandwidth parameter\n",
    "    'Z_mean',           # Standardization mean\n",
    "    'Z_std',            # Standardization std\n",
    "]\n",
    "\n",
    "all_essential_ok = True\n",
    "for var in essential_vars:\n",
    "    if var in globals():\n",
    "        obj = globals()[var]\n",
    "        if hasattr(obj, 'shape'):\n",
    "            print(f\"âœ“ {var}: shape {obj.shape}, dtype {obj.dtype}\")\n",
    "        else:\n",
    "            print(f\"âœ“ {var}: {type(obj).__name__} = {obj}\")\n",
    "    else:\n",
    "        print(f\"âœ— {var}: MISSING!\")\n",
    "        all_essential_ok = False\n",
    "\n",
    "if all_essential_ok:\n",
    "    print(f\"\\nâœ… All essential spectral data preserved! KSWGD sampling can proceed.\")\n",
    "else:\n",
    "    print(f\"\\nâŒ Some essential data is missing! Check before running KSWGD.\")\n",
    "\n",
    "print(f\"\\nâœ… Spectral memory cleanup complete!\")\n",
    "print(f\"   Total estimated release: {total_released:.2f} GB\")\n",
    "print(f\"   Kernel matrix ({max_samples}x{max_samples}) and full eigenvectors deleted\")\n",
    "print(f\"   Truncated spectral info (k_eig={k_eig}) preserved for KSWGD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373a7b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run KSWGD sampling to generate new latent vectors\n",
    "kswgd_config = {\n",
    "    \"num_particles\": n_generate,  # (n_generate) Same as LDM generation count for fair comparison\n",
    "    \"num_iters\": 500,             # Number of iterations\n",
    "    \"step_size\": 0.05,            # Step size (smaller for stability)\n",
    "    \"rng_seed\": 42\n",
    "}\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"KSWGD Generation Config:\")\n",
    "for k, v in kswgd_config.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nNote: Generating {kswgd_config['num_particles']} particles for reliable FID...\")\n",
    "print(\"This will take a while...\")\n",
    "\n",
    "# Run KSWGD (progress bar is built into run_kswgd_sampler via trange)\n",
    "start_time = time.time()\n",
    "Z_kswgd_std = run_kswgd_sampler(**kswgd_config)\n",
    "kswgd_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nâœ“ KSWGD Complete!\")\n",
    "print(f\"Generated samples shape: {Z_kswgd_std.shape}\")\n",
    "print(f\"Total time: {kswgd_time:.1f} seconds ({kswgd_time/60:.1f} minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2563ef8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode KSWGD generated latent vectors to images (in batches to avoid OOM)\n",
    "print(f\"Decoding {Z_kswgd_std.shape[0]} latent vectors to images...\")\n",
    "\n",
    "decode_batch_size = 128  # Increased for 80GB GPUs\n",
    "all_kswgd_images = []\n",
    "\n",
    "for i in tqdm(range(0, Z_kswgd_std.shape[0], decode_batch_size), desc=\"Decoding KSWGD\"):\n",
    "    batch_latents = Z_kswgd_std[i:i+decode_batch_size]\n",
    "    batch_images = decode_latents_to_images(batch_latents)\n",
    "    all_kswgd_images.append(batch_images.numpy())\n",
    "    \n",
    "    # Clear cache periodically\n",
    "    if (i // decode_batch_size + 1) % 20 == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "kswgd_images_np = np.concatenate(all_kswgd_images, axis=0)\n",
    "\n",
    "print(f\"\\nâœ“ Decoding complete!\")\n",
    "print(f\"Generated images shape: {kswgd_images_np.shape}\")\n",
    "print(f\"Pixel value range: [{kswgd_images_np.min():.3f}, {kswgd_images_np.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8c956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize KSWGD generated images (show first 16 of 10000)\n",
    "n_show = min(16, kswgd_images_np.shape[0])\n",
    "n_cols = 4\n",
    "n_rows = (n_show + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(3 * n_cols, 3 * n_rows))\n",
    "axes = np.asarray(axes).reshape(-1)\n",
    "\n",
    "for idx in range(n_show):\n",
    "    img = np.transpose(kswgd_images_np[idx], (1, 2, 0))  # (C,H,W) â†’ (H,W,C)\n",
    "    axes[idx].imshow(np.clip(img, 0.0, 1.0))\n",
    "    axes[idx].set_title(f\"KSWGD #{idx+1}\")\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "for idx in range(n_show, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "# Get image size from KSWGD generated images (shape is N, C, H, W)\n",
    "_kswgd_img_size = kswgd_images_np.shape[2]\n",
    "plt.suptitle(f\"KSWGD CelebA-HQ ({_kswgd_img_size}x{_kswgd_img_size})\", fontsize=22)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nGeneration Summary:\")\n",
    "print(f\"  - LDM: {len(ldm_generated_images)} images via UNet Denoising (200 steps)\")\n",
    "print(f\"  - KSWGD: {kswgd_images_np.shape[0]} images via KSWGD ({kswgd_config['num_iters']} steps)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a162c62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== Apply GFPGAN to KSWGD Generated Images ==============\n",
    "print(\"=\" * 60)\n",
    "print(\"Upscaling KSWGD Generated Images with GFPGAN\")\n",
    "print(\"(with preprocessing: Gaussian + Bilateral + Color Norm)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Convert KSWGD images to the right format for upscaling\n",
    "kswgd_images_for_upscale = []\n",
    "for i in range(kswgd_images_np.shape[0]):\n",
    "    img = np.transpose(kswgd_images_np[i], (1, 2, 0))  # (C,H,W) â†’ (H,W,C)\n",
    "    img = np.clip(img, 0, 1)\n",
    "    kswgd_images_for_upscale.append(img)\n",
    "\n",
    "print(f\"\\nEnhancing {len(kswgd_images_for_upscale)} KSWGD images with preprocessing + GFPGAN...\")\n",
    "kswgd_enhanced = upscale_images(kswgd_images_for_upscale, use_face_enhance=True, use_preprocess=True, desc=\"Preprocess+GFPGAN (KSWGD)\")\n",
    "\n",
    "# Visualization (show up to 16 in a grid)\n",
    "n_test_kswgd = len(kswgd_enhanced)\n",
    "n_show_kswgd = min(16, n_test_kswgd)\n",
    "n_cols = 4\n",
    "n_rows = (n_show_kswgd + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))\n",
    "axes = np.asarray(axes).reshape(-1)\n",
    "\n",
    "for i in range(n_show_kswgd):\n",
    "    axes[i].imshow(kswgd_enhanced[i])\n",
    "    # axes[i].set_title(f\"#{i+1}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "for i in range(n_show_kswgd, len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "# Get image size from enhanced images\n",
    "_kswgd_enhanced_size = kswgd_enhanced[0].shape[1] if isinstance(kswgd_enhanced[0], np.ndarray) else kswgd_enhanced[0].size[0]\n",
    "plt.suptitle(f\"KSWGD CelebA-HQ ({_kswgd_enhanced_size}x{_kswgd_enhanced_size})\\n\", fontsize=22, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('/workspace/kswgd/figures/kswgd_gfpgan_enhanced.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Store enhanced KSWGD images\n",
    "kswgd_upscaled = kswgd_enhanced\n",
    "\n",
    "print(f\"\\nâœ“ KSWGD Enhancement complete!\")\n",
    "print(f\"  Original KSWGD: 256x256\")\n",
    "print(f\"  Enhanced: {kswgd_upscaled[0].shape[1]}x{kswgd_upscaled[0].shape[0]}\")\n",
    "print(f\"  Preprocessing applied: Gaussian blur (3x3, Ïƒ=0.5) + Bilateral (d=5) + Color norm (Î±=0.3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c043c0c9",
   "metadata": {},
   "source": [
    "### 7.2 FID (FrÃ©chet Inception Distance) Evaluation\n",
    "\n",
    "Compute FID scores to quantitatively compare LDM and KSWGD generation quality.\n",
    "\n",
    "**FID** measures the distance between the distribution of generated images and real images in Inception-v3 feature space. **Lower is better.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff758ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== FID Utility Functions (Dual-GPU Support) ==============\n",
    "# Install pytorch-fid if needed\n",
    "import subprocess\n",
    "import sys\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "try:\n",
    "    from pytorch_fid import fid_score\n",
    "    from pytorch_fid.inception import InceptionV3\n",
    "except ImportError:\n",
    "    print(\"Installing pytorch-fid...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pytorch-fid\", \"-q\"])\n",
    "    from pytorch_fid import fid_score\n",
    "    from pytorch_fid.inception import InceptionV3\n",
    "\n",
    "from scipy import linalg\n",
    "from torchvision import transforms as T\n",
    "\n",
    "# Check number of GPUs for FID computation\n",
    "NUM_GPUS_FID = torch.cuda.device_count()\n",
    "USE_DUAL_GPU_FID = NUM_GPUS_FID >= 2\n",
    "print(f\"FID computation: Detected {NUM_GPUS_FID} GPU(s), using {'Dual-GPU' if USE_DUAL_GPU_FID else 'Single-GPU'} mode\")\n",
    "\n",
    "def get_inception_features_single_gpu(images, gpu_id, batch_size=256, desc=\"Extracting features\"):\n",
    "    \"\"\"Extract Inception-v3 features on a single GPU\"\"\"\n",
    "    device_local = torch.device(f\"cuda:{gpu_id}\")\n",
    "    \n",
    "    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n",
    "    inception = InceptionV3([block_idx]).to(device_local)\n",
    "    inception.eval()\n",
    "    \n",
    "    preprocess = T.Compose([\n",
    "        T.Resize((299, 299)),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    features_list = []\n",
    "    n_batches = (len(images) + batch_size - 1) // batch_size\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(images), batch_size), total=n_batches, desc=f\"{desc} (GPU {gpu_id})\"):\n",
    "            batch_samples = images[i:i+batch_size]\n",
    "            \n",
    "            tensors = []\n",
    "            for img in batch_samples:\n",
    "                if isinstance(img, Image.Image):\n",
    "                    t = T.ToTensor()(img)\n",
    "                elif isinstance(img, np.ndarray):\n",
    "                    t = torch.from_numpy(img).float()\n",
    "                    if t.ndim == 3:\n",
    "                        if t.shape[2] == 3 and t.shape[0] != 3:\n",
    "                            t = t.permute(2, 0, 1)\n",
    "                    if t.max() > 1.0: t /= 255.0\n",
    "                else:\n",
    "                    t = torch.as_tensor(img).float()\n",
    "                tensors.append(t)\n",
    "            \n",
    "            batch = torch.stack(tensors).to(device_local)\n",
    "            batch = preprocess(batch)\n",
    "            feat = inception(batch)[0]\n",
    "            feat = feat.squeeze(-1).squeeze(-1)\n",
    "            features_list.append(feat.cpu().numpy())\n",
    "    \n",
    "    del inception\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return np.concatenate(features_list, axis=0)\n",
    "\n",
    "def get_inception_features(images, batch_size=256, desc=\"Extracting features\"):\n",
    "    \"\"\"\n",
    "    Extract Inception-v3 features from images.\n",
    "    Automatically uses dual-GPU parallel processing if available.\n",
    "    \"\"\"\n",
    "    n_images = len(images)\n",
    "    \n",
    "    if USE_DUAL_GPU_FID and n_images >= 64:  # Only use dual-GPU for larger batches\n",
    "        print(f\"  Using Dual-GPU parallel feature extraction...\")\n",
    "        mid = n_images // 2\n",
    "        \n",
    "        images_gpu0 = images[:mid] if isinstance(images, list) else list(images[:mid])\n",
    "        images_gpu1 = images[mid:] if isinstance(images, list) else list(images[mid:])\n",
    "        \n",
    "        # Use ThreadPoolExecutor to run both GPUs in parallel\n",
    "        with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "            future_gpu0 = executor.submit(\n",
    "                get_inception_features_single_gpu, images_gpu0, 0, batch_size, desc\n",
    "            )\n",
    "            future_gpu1 = executor.submit(\n",
    "                get_inception_features_single_gpu, images_gpu1, 1, batch_size, desc\n",
    "            )\n",
    "            \n",
    "            features_gpu0 = future_gpu0.result()\n",
    "            features_gpu1 = future_gpu1.result()\n",
    "        \n",
    "        return np.concatenate([features_gpu0, features_gpu1], axis=0)\n",
    "    else:\n",
    "        # Single-GPU processing\n",
    "        return get_inception_features_single_gpu(images, 0, batch_size, desc)\n",
    "\n",
    "def calculate_fid(real_features, gen_features):\n",
    "    \"\"\"Calculate FID between two sets of features\"\"\"\n",
    "    mu_real = np.mean(real_features, axis=0)\n",
    "    mu_gen = np.mean(gen_features, axis=0)\n",
    "    sigma_real = np.cov(real_features, rowvar=False)\n",
    "    sigma_gen = np.cov(gen_features, rowvar=False)\n",
    "    \n",
    "    # Calculate FID\n",
    "    diff = mu_real - mu_gen\n",
    "    # Add a small value to diagonal for stability\n",
    "    offset = np.eye(sigma_real.shape[0]) * 1e-6\n",
    "    covmean, _ = linalg.sqrtm((sigma_real + offset) @ (sigma_gen + offset), disp=False)\n",
    "    \n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    \n",
    "    fid = diff @ diff + np.trace(sigma_real + sigma_gen - 2 * covmean)\n",
    "    return float(fid)\n",
    "\n",
    "print(\"âœ“ FID utility functions defined (with Dual-GPU support).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09982581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== FID Evaluation (Raw 256x256 Images) ==============\n",
    "print(\"=\" * 60)\n",
    "print(\"Computing FID Scores (on Raw 256x256 Images)...\")\n",
    "print(f\"  LDM raw:   {len(ldm_generated_images)} images\")\n",
    "print(f\"  KSWGD raw: {len(kswgd_images_np)} images\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Get real images from CelebA-HQ (resized to 256x256)\n",
    "n_real_samples = min(10000, len(celebahq_dataset))  # 10k real images for reliable FID\n",
    "real_images_256 = []\n",
    "for i in tqdm(range(n_real_samples), desc=\"Loading real images (256)\"):\n",
    "    img = celebahq_dataset[i][\"image\"].resize((256, 256))\n",
    "    real_images_256.append(np.array(img))\n",
    "\n",
    "print(f\"  Real images loaded: {len(real_images_256)}\")\n",
    "\n",
    "# 2. Extract features\n",
    "print(\"\\n[1/3] Extracting Inception features from real images (256)...\")\n",
    "real_features_256 = get_inception_features(real_images_256, desc=\"Real images (256)\")\n",
    "\n",
    "print(\"\\n[2/3] Extracting Inception features from LDM raw images (256)...\")\n",
    "ldm_features_256 = get_inception_features(ldm_generated_images, desc=\"LDM Raw (256)\")\n",
    "\n",
    "print(\"\\n[3/3] Extracting Inception features from KSWGD raw images (256)...\")\n",
    "kswgd_features_256 = get_inception_features(kswgd_images_np, desc=\"KSWGD Raw (256)\")\n",
    "\n",
    "# 3. Calculate FID\n",
    "print(\"\\nCalculating FID scores...\")\n",
    "fid_ldm_raw = calculate_fid(real_features_256, ldm_features_256)\n",
    "fid_kswgd_raw = calculate_fid(real_features_256, kswgd_features_256)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FID RESULTS (Raw 256x256, Lower is Better)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Real images:  {n_real_samples}\")\n",
    "print(f\"  LDM (Raw):   {len(ldm_generated_images)} â†’ FID = {fid_ldm_raw:.2f}\")\n",
    "print(f\"  KSWGD (Raw): {len(kswgd_images_np)} â†’ FID = {fid_kswgd_raw:.2f}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nNote: This FID measures the quality of the raw 256x256 generation before any enhancement.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5726b611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== FID Evaluation (Enhanced 1024x1024 Images) ==============\n",
    "print(\"=\" * 60)\n",
    "print(\"Computing FID Scores (on 1024x1024 Enhanced Images)...\")\n",
    "print(f\"  LDM enhanced:   {len(ldm_upscaled)} images\")\n",
    "print(f\"  KSWGD enhanced: {len(kswgd_upscaled)} images\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Get real images from CelebA-HQ (1024x1024)\n",
    "print(\"\\n[1/4] Loading real CelebA-HQ images (1024x1024)...\")\n",
    "# Use a smaller number for real images if memory is tight, but 1000-2000 is usually enough for comparison\n",
    "n_real_samples = min(10000, len(celebahq_dataset))  # 10k real images for reliable FID\n",
    "real_images_1024 = []\n",
    "for i in tqdm(range(n_real_samples), desc=\"Loading real images (1024)\"):\n",
    "    img = celebahq_dataset[i][\"image\"]\n",
    "    # No resize, keep original 1024x1024\n",
    "    img_tensor = T.ToTensor()(img) \n",
    "    real_images_1024.append(img_tensor.numpy())\n",
    "\n",
    "print(f\"  Real images loaded: {len(real_images_1024)}\")\n",
    "\n",
    "print(\"\\n[2/4] Extracting Inception features from real images (1024)...\")\n",
    "real_features_1024 = get_inception_features(real_images_1024, desc=\"Real images (1024)\")\n",
    "\n",
    "# 2. Get LDM enhanced images features\n",
    "print(\"\\n[3/4] Extracting Inception features from LDM enhanced images...\")\n",
    "ldm_features_1024 = get_inception_features(ldm_upscaled, desc=\"LDM Enhanced\")\n",
    "\n",
    "# 3. Get KSWGD enhanced images features  \n",
    "print(\"\\n[4/4] Extracting Inception features from KSWGD enhanced images...\")\n",
    "kswgd_features_1024 = get_inception_features(kswgd_upscaled, desc=\"KSWGD Enhanced\")\n",
    "\n",
    "# 4. Calculate FID scores\n",
    "print(\"\\nCalculating FID scores...\")\n",
    "fid_ldm_enhanced = calculate_fid(real_features_1024, ldm_features_1024)\n",
    "fid_kswgd_enhanced = calculate_fid(real_features_1024, kswgd_features_1024)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FID RESULTS (Enhanced 1024x1024, Lower is Better)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Real images:  {n_real_samples}\")\n",
    "print(f\"  LDM (Enhanced):   {len(ldm_upscaled)} â†’ FID = {fid_ldm_enhanced:.2f}\")\n",
    "print(f\"  KSWGD (Enhanced): {len(kswgd_upscaled)} â†’ FID = {fid_kswgd_enhanced:.2f}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nNote: This FID measures the quality of the full pipeline (Model + Preprocessing + GFPGAN).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c28e5a4",
   "metadata": {},
   "source": [
    "## 9. Clean Up GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e827693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up GPU memory\n",
    "import gc\n",
    "\n",
    "if 'ldm_pipe' in dir():\n",
    "    del ldm_pipe\n",
    "if 'vae' in dir():\n",
    "    del vae\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Current GPU memory usage: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"GPU memory cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kswgd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
